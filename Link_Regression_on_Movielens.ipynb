{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqYLHVWoIgSe"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKXgOkl6Iabv",
    "outputId": "b76b1c1a-ce4a-472b-abe6-c602f8efec2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nlp/tejomoy/anaconda3/envs/graph/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51OK1cQL9V9e"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjJrBa3J0btD",
    "outputId": "af076e3b-e797-4b1e-dedf-b361eb66a404"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_path = \"/raid/nlp/tejomoy/graphML/KP-GNN-Kishan/data/movielens/users.csv\"\n",
    "movies_path = \"/raid/nlp/tejomoy/graphML/KP-GNN-Kishan//data/movielens/movies.csv\"\n",
    "ratings_path = \"/raid/nlp/tejomoy/graphML/KP-GNN-Kishan//data/movielens/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haJz-BYBI2wi",
    "outputId": "719d51e8-19b2-404e-9723-3016f8aa359e"
   },
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(ratings_path)[[\"userId\", \"movieId\", \"rating\"]]\n",
    "users_df = pd.read_csv(user_path)[['user_id', 'Gender', 'Age', 'occupation', 'Pin Code']]\n",
    "\n",
    "\n",
    "ratings_df['rating_og'] = ratings_df['rating']\n",
    "ratings_df['rating'] = ratings_df['rating']\n",
    "\n",
    "movies_df = pd.read_csv(movies_path, index_col='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa402712220>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "rating_count = ratings_df.sort_values(['rating'], ascending=False)['rating'].value_counts()\n",
    "rating_count.sort_index().plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_features = users_df['Age'].str.get_dummies().values\n",
    "age_features = torch.from_numpy(age_features)#.to(torch.int64)\n",
    "\n",
    "gender_features = users_df['Gender'].str.get_dummies().values\n",
    "gender_features = torch.from_numpy(gender_features)#.to(torch.int64)\n",
    "\n",
    "occ_features = users_df['occupation'].str.get_dummies().values\n",
    "occ_features = torch.from_numpy(occ_features)#.to(torch.int64)\n",
    "\n",
    "address_features = users_df['Pin Code'].str.get_dummies().values\n",
    "address_features = torch.from_numpy(address_features)#.to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_peripheral_info = torch.cat([age_features, gender_features, occ_features, address_features], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497,
     "referenced_widgets": [
      "ed32d0d192404ba699e752f454549a2a",
      "6930e5b9888d4d79819bb6477ff2a8b4",
      "01e3d0f8640d49e3a9de2bb1824440e3",
      "365e6d10b1ca4841a45664163cf76da9",
      "9a35e140b5674849bfc1e6244f1a4a5e",
      "1021a4d43fde4a46baf2cb38913f9ab7",
      "75f13359b6744f3b92f70b1900de7e56",
      "a9b28bb3b91f460492f7a2a6c78b2f39",
      "ead3fe6696c54179bd74ea65f382fb41",
      "39220418c8a04408a906badd447c14ae",
      "bfc71c7b86834b29a57c03b8fca989ac",
      "cb50c9545e2f43c4af106b649e475f79",
      "5299dedb8eff4202ae51211b428fd6b8",
      "72d00cf9bbfd43a3b22a61c21fb6b2dc",
      "cf1dc50fb1774808b8691091d90069a1",
      "8447a5acf01045d78c8d512e0c7d1494",
      "df8c3d8a3fc44d7e9ce7a6a86f625aef",
      "8431f4111a9240758cdbda1c830576fa",
      "a0005be084eb4c3caca712d0aa0087c9",
      "c4cae87753e541e99efabb75d0745d16",
      "1f67efbd2c474dc0aa7383dfb65ceee6",
      "ffa49835e86e42248c93da624238651c",
      "3a34ff034cfc4cac95d19583c234aa6c",
      "18c9438aa1e8468b991de01bd1b77df2",
      "9c433ef967a3406daa193ce904e9d222",
      "6c3d50d6ccaf4cb6b35fef21b5feb90c",
      "509c11a30a984efd9340e3039c2ad7a0",
      "9cb08e56b4ca48758adc09030c871c94",
      "a32b0b6b3c834b068d769dab2558572f",
      "27236a64fb9b42c09d24f2ce350227f4",
      "5336152e581b41e5b98b18d723009ca3",
      "f659bd8028ab45e9b223b39668ad1d4f",
      "3b0b722f2aad4c4c97478e66a20bdc6e",
      "bd8c833513694f8fa66960a6ff3746c2",
      "2c02c05c036c42d3bdfd803816295b44",
      "ffc76606820e4fba8d11c50694737a57",
      "e440565f96d840629dde503854727bed",
      "63c20dbc9081488ba16996a9717ddc2b",
      "cf088c2fc6474d30b706992e7e1fcf42",
      "4dbef274b3ff464289bf8279ba2345bb",
      "9b6554a15f3145b1b4d13e9d3dc8385b",
      "18f3008fcfa24cc6a8dff5d85b6b6d85",
      "c53cafcacad84eaa836e52e817d2fdd8",
      "4770667b6b2d4ee097c0693407fa77a6",
      "806ba54b23944a6ea39c94ad33a409f5",
      "d1f505075cb14afabff64cf79fabb5b7",
      "0d2aa8937ed24c3b82c6d316ab4076e4",
      "c4c61a6123854429bfc27f3baf41eb11",
      "c7fd5905b58048e99fdc4a8a00d0cc8c",
      "5513af69ee814214ba0bc67f55ef5296",
      "f032af5a5cc24cd090c645369dc0c49c",
      "ace8d1d93404476783871a2873b46b37",
      "b05ca66fce674dca820eede95d3711c8",
      "c1fe5c0ecaa14980a194646f6f66cd0b",
      "28c1151d8ffe467ea40d8f9c9e0cd291",
      "90189e21f58e403ca32747603df1efdb",
      "548ce1469b414a659f862cc352014afd",
      "0eb13344a1724e7aaf9afda54f15b493",
      "386003509cec4f3b88bd7399a5984b2f",
      "56bff5bc4a544bb8b1fa58e6caea579e",
      "8243c17d3da141ceb37a0d1006eed8e0",
      "063b57709d1a4b7e9e4f4d2ecdaee3c1",
      "8f559778b18346adb3a259c734cdd724",
      "ddb5bd908e1d46d187c23e3cacfae75a",
      "1edb24d73c004692928b0b915c113be9",
      "46d374c237b04f82b0c64bdccae09071",
      "f4f65ca7dbb44e53abf4a8969eeaf5d4",
      "a7ceb44d4d254fba835e304c1b540268",
      "073ac27236834df2b1cf9c216abc79e5",
      "9845dfbf20be4cbab436e0847eb8d98f",
      "29c8e87ba7954e8e992650be6e5c7546",
      "69f29e15d31e441aa01c49d6890b4947",
      "c5e97a52f5554d47a293f5d01dbe8eeb",
      "71a401e0b3284652a96fe0e36f2b97b1",
      "82af6ec6f7324f02bfeccc4ecab72b9b",
      "b753e8d5d2a94ea9be46309729b96291",
      "129b98c69e934d4191957c0623e660fc",
      "53578221920149b7b0212a63b531e3ed",
      "b5e61177652e4e63b59188bd336df81a",
      "f84b23cb30aa4006b0cc8ef21d15878f",
      "35c6762b00824d8cada477984f34c9ec",
      "92bc9919deb84127b7d975e60c175520",
      "4d3401af5fec4a97aa305c49a0e21ec8",
      "7ddbc4308f2d4122b79eaa5a67c21b2c",
      "13602bc74b2c4844b9796f110d91e23e",
      "0b53fdf486b643ee9836adfb1e4890ed",
      "02bdda5953704642afc0ff3accff2c83",
      "207ad90aa1254ba8a5b05de42444da70",
      "cf4705662bad4992b3ec40d026df8427",
      "ee15028796094fbe82ac590c75ce1152",
      "4cca8f40500d4698b709f225b0e8ac21",
      "972248ce87f14339adc6239d99940e76",
      "87b66ea157fb425fa864cf6749b2a404",
      "1c2bcaff060d408cb8c0138dab6afb87",
      "189380c32b9e48159aad4ac1880604b8",
      "99b60ebc5adf42489183ea64e5cf2707",
      "f1408b78587f4fd4b6c043e118f293e3",
      "451c0e6ca80840598800340df57e036a",
      "831fe169202041e2b7a245c62f6d1e94",
      "8d017509927b40c0b8a9c6bf1e104306",
      "0a16d05cfdd34f6a83511335e14a97a8",
      "a60d771ede134585962cf170983e86c4",
      "a0ade0b19d494f7fb8f97dd52db8290c",
      "d75cf0296b7b4e72b5044e0319fc5d52",
      "4a715bd4c9474ff099928e14f7075f42",
      "6db786e7157743008b1d1024d71ed2cb",
      "c54efdd2a7864e128552552c299cbe79",
      "6dfb4e44f64c4a66abd4aed045dfafed",
      "ac0c050322354e1fb0be1bcb69129c89",
      "70cb2596438447a3a43a8d7e398fb642",
      "171f361c595946338f255f217fb33b11",
      "2fd0618070ad4ca6bb44220ceaa75b70",
      "19fad356297a4563bc6bd94850a23ef5",
      "2912bb4e3f4b4ed2a86d6016faf0f8c7",
      "e751597fa4904d4fa680581adb4f47dd",
      "26fdaa7b72f44bd69b677665537e4617",
      "a8829d3d46fd4508b55b5227c3efcb4b",
      "59e67a685bf54f8d83fbb2270de37940",
      "c085ff4c203c474ba5e324d0e2f06e42",
      "4b1437058b8240d39d144c899697d006",
      "949c1facc1264cba9ca17f07747a3eeb",
      "789771b912f24db69b119c58f4399c3a",
      "edb0030de05d46d685cd81f98b9510c6",
      "c5e60fd1188043008de959752a863636",
      "cbe6891eb4014e7f853ec76d4e56f1f8",
      "4a69524be52449129364390309db45ff",
      "49a8a040041244768d803bf98d98c666",
      "b8c7a57f106b470296b69d4238722600",
      "e862b242aa9a4ee7be6eeb6325f11d6d",
      "95919e20022d4f41816771418dc18a54",
      "989ec473f53041eeb36c0af9f7a42413",
      "d7bbe9f004794122adcacf4d5dc51698",
      "2c31b30d157944bf8b38b666836fa9b2",
      "d90fe4943e3a468a8f33e5ab9d0a1e23",
      "9a7b4718f2aa46e295d59a25e645d8ce",
      "cfec6212f00e43f9b0e69c00f18881f6",
      "72b5874ea4554f4ebf690421200b3856",
      "f7f27b06fd274ccaa45292ae892d6aff",
      "204c00dd2b634413821a77432a532c76",
      "c077b4f279ff488996e6b583691fdcdf",
      "366968eb63484a20beb5f1ff865b03a3",
      "908142a3070f4066bf34483734194cfa",
      "7c0e8b8413474c1a9b9bae2a0a011d84",
      "9f6e054cb20743fb86caa48b953ea343",
      "dcfc841b92ac4d969e7da44f7b5baaaa",
      "ad572bdf184a44a6b59257f2e76f1269",
      "7a364ff5a7b64a32a0d69ba5ca3131be",
      "ff370c53ac7246839419a26b0e748c9d",
      "f095354c57854bef86cf37752cb21a0b",
      "006cbd458d9e4d2da68ff964cb424e00",
      "c170a8414ab94b81be0463276372fd26",
      "77ca0fca1bd74cefa86987954c9a872f",
      "faf96d8953634c9db379b46521d2397c",
      "6d09e8a5a0cf436cbd76c2f518460e03",
      "00292370b95a4ca39c4261f7756674c6",
      "b821135bfeb64fc290ac622b73a2c81d",
      "050c709880634afb8cfbbf9c6e6d4c08",
      "347eeacd2c8548098dd0c84cc2a2b55d",
      "ec4242dd4fd54be1b49ff6ced32df6b1",
      "e256842b3677478b91db3797dfb6ec59",
      "f19c818af90b4d5ab4c01304b934e20c",
      "875bc0f0b47c4d5fa3628bba5cddfd1a",
      "8e2840d415724a4aa30f1f3b149dbb0a",
      "3c57bdd883e4407aa0b896de3a41d886",
      "92cb3b8ff3bc4de2ada06246ceb3c1a9"
     ]
    },
    "id": "fXF-BNIYJAMo",
    "outputId": "f8348b60-ff6a-47d4-db6f-373ca02dfaae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 122/122 [00:03<00:00, 37.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# One-hot encode the genres:\n",
    "genres = movies_df['genres'].str.get_dummies('|').values\n",
    "genres = torch.from_numpy(genres).to(torch.float)\n",
    "\n",
    "# Load the pre-trained sentence transformer model and encode the movie titles:\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "with torch.no_grad():\n",
    "    titles = model.encode(movies_df['title'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    titles = titles.cpu()\n",
    "\n",
    "# Concatenate the genres and title features:\n",
    "movie_features = torch.cat([genres, titles], dim=-1)\n",
    "\n",
    "# We don't have user features, which is why we use an identity matrix\n",
    "user_features = torch.eye(len(ratings_df['userId'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7YZJLJVJEbL",
    "outputId": "5c86ef21-e9ae-4236-f8ba-22f577ead8e8"
   },
   "outputs": [],
   "source": [
    "# Create a mapping from the userId to a unique consecutive value in the range [0, num_users]:\n",
    "unique_user_id = ratings_df['userId'].unique()\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'userId': unique_user_id,\n",
    "    'mappedUserId': pd.RangeIndex(len(unique_user_id))\n",
    "    })\n",
    "\n",
    "# Create a mapping from the movieId to a unique consecutive value in the range [0, num_movies]:\n",
    "unique_movie_id = ratings_df['movieId'].unique()\n",
    "unique_movie_id = pd.DataFrame(data={\n",
    "    'movieId': unique_movie_id,\n",
    "    'mappedMovieId': pd.RangeIndex(len(unique_movie_id))\n",
    "    })\n",
    "\n",
    "# Merge the mappings with the original data frame:\n",
    "ratings_df = ratings_df.merge(unique_user_id, on='userId')\n",
    "ratings_df = ratings_df.merge(unique_movie_id, on='movieId')\n",
    "\n",
    "edge_index = torch.stack([\n",
    "    torch.tensor(ratings_df['mappedUserId'].values),\n",
    "    torch.tensor(ratings_df['mappedMovieId'].values)]\n",
    "    , dim=0)\n",
    "\n",
    "assert edge_index.shape == (2, len(ratings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDQaX8OPJMvj",
    "outputId": "a4c9041d-db9f-4d2f-96ec-f89f89397daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "data['user'].x = user_features  # [num_users, num_features_users]\n",
    "data['movie'].x = movie_features  # [num_movies, num_features_movies]\n",
    "\n",
    "data['user'].pe_attr = user_peripheral_info  # [num_user, num_features_users]\n",
    "\n",
    "data['user', 'rates', 'movie'].edge_index = edge_index  # [2, num_ratings]\n",
    "\n",
    "rating = torch.from_numpy(ratings_df['rating'].values).to(torch.float)\n",
    "data['user', 'rates', 'movie'].edge_label = rating  # [num_ratings]\n",
    "\n",
    "data = T.ToUndirected()(data)\n",
    "\n",
    "# With the ToUndirected() we also got reversed labels for the edges.\n",
    "# We are going to remove them:\n",
    "del data['movie', 'rev_rates', 'user'].edge_label\n",
    "print(data['movie'].num_features)\n",
    "assert data['user'].num_nodes == len(unique_user_id)\n",
    "assert data['user', 'rates', 'movie'].num_edges == len(ratings_df)\n",
    "assert data['movie'].num_features == 402\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpcH0bniJaJw"
   },
   "source": [
    "## Dataset Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njo191z3JctD",
    "outputId": "6f0d4b01-30fd-4e39-9323-c31aad6ccba9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(HeteroData(\n",
       "   user={\n",
       "     x=[6040, 6040],\n",
       "     pe_attr=[6040, 3469],\n",
       "   },\n",
       "   movie={ x=[3883, 402] },\n",
       "   (user, rates, movie)={\n",
       "     edge_index=[2, 800169],\n",
       "     edge_attr=[800169],\n",
       "     x_dict={\n",
       "       user=[6040, 6040],\n",
       "       movie=[3883, 402],\n",
       "     },\n",
       "     user_features=[6040, 3469],\n",
       "     edge_label=[800169],\n",
       "     edge_label_index=[2, 800169],\n",
       "   },\n",
       "   (movie, rev_rates, user)={ edge_index=[2, 800169] }\n",
       " ),\n",
       " HeteroData(\n",
       "   user={\n",
       "     x=[6040, 6040],\n",
       "     pe_attr=[6040, 3469],\n",
       "   },\n",
       "   movie={ x=[3883, 402] },\n",
       "   (user, rates, movie)={\n",
       "     edge_index=[2, 800169],\n",
       "     edge_attr=[800169],\n",
       "     x_dict={\n",
       "       user=[6040, 6040],\n",
       "       movie=[3883, 402],\n",
       "     },\n",
       "     user_features=[6040, 3469],\n",
       "     edge_label=[100020],\n",
       "     edge_label_index=[2, 100020],\n",
       "   },\n",
       "   (movie, rev_rates, user)={ edge_index=[2, 800169] }\n",
       " ))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"user\", \"rates\", \"movie\"]['edge_attr'] = data[\"user\", \"rates\", \"movie\"]['edge_label']\n",
    "data[\"user\", \"rates\", \"movie\"]['x_dict'] = data.x_dict\n",
    "data[\"user\", \"rates\", \"movie\"]['user_features'] = data[\"user\"]['pe_attr']\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)\n",
    "train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def hit_rate_top_k(k, predicted_df, threshold=28):\n",
    "    uids = list(set(predicted_df['userId']))\n",
    "\n",
    "    intersection_count = []\n",
    "    for i in tqdm(range(len(uids))):\n",
    "\n",
    "        # extract top_k movies according to predicted and target rating \n",
    "        top_k_predicted = predicted_df[predicted_df['userId']==uids[i]].sort_values(['rating'], ascending=False)['movieId'][:k]\n",
    "        top_k_target = predicted_df[predicted_df['userId']==uids[i]].sort_values(['target'], ascending=False)['movieId'][:threshold]\n",
    "        \n",
    "        # calculate the intersection of predicted and target set\n",
    "        common_set = (set(top_k_predicted) & set(top_k_target))\n",
    "\n",
    "        # stores intersection of every user id\n",
    "        intersection_count.append(common_set)\n",
    "\n",
    "\n",
    "    # Average intersection\n",
    "    average_overlap = sum([len(x) for x in intersection_count])/len(intersection_count)\n",
    "    \n",
    "    # percentage of average overlap\n",
    "    top_k_hit = (average_overlap/k)*100\n",
    "\n",
    "    return top_k_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import layer_utils\n",
    "from models import model_utils\n",
    "from layers.input_encoder import LinearEncoder\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    layer = layer_utils.make_gnn_layer(args)\n",
    "    init_emb = LinearEncoder(args.input_size, args.hidden_size)\n",
    "    GNNModel = model_utils.make_GNN(args)\n",
    "    gnn = GNNModel(\n",
    "        num_layer=args.num_layer,\n",
    "        gnn_layer=layer,\n",
    "        JK=args.JK,\n",
    "        norm_type=args.norm_type,\n",
    "        init_emb=init_emb,\n",
    "        residual=args.residual,\n",
    "        virtual_node=args.virtual_node,\n",
    "        use_rd=args.use_rd,\n",
    "        num_hop1_edge=args.num_hop1_edge,\n",
    "        max_edge_count=args.max_edge_count,\n",
    "        max_hop_num=args.max_hop_num,\n",
    "        max_distance_count=args.max_distance_count,\n",
    "        wo_peripheral_edge=args.wo_peripheral_edge,\n",
    "        wo_peripheral_configuration=args.wo_peripheral_configuration,\n",
    "        drop_prob=args.drop_prob)\n",
    "\n",
    "    return gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args:\n",
    "        JK=\"last\"\n",
    "        K = 1\n",
    "        MODULO= 4\n",
    "        MOD_THRESH= 1\n",
    "        aggr= \"sum\"\n",
    "        batch_size= 128\n",
    "        combine= \"geometric\"\n",
    "        drop_prob= 0.0\n",
    "        eps= 0.0\n",
    "        gpu_ids= [0, 1]\n",
    "        hidden_size= 48\n",
    "        input_size= 48\n",
    "        kernel= \"spd\"\n",
    "        l2_wd= 3e-07\n",
    "        load_path= \"\"\n",
    "        lr= 0.001\n",
    "        max_distance_count= 1000\n",
    "        max_edge_count= 1000\n",
    "        max_edge_type= 1\n",
    "        max_hop_num= 5\n",
    "        max_pe_num= 1\n",
    "        model_name= \"KPGIN\" #\"KPGraphSAGE\", \"KPGINPlus\", \"KPGIN\"\n",
    "        norm_type= \"Batch\"\n",
    "        num_epochs= 40\n",
    "        num_hop1_edge= 1\n",
    "        num_hopk_edge= 1\n",
    "        num_layer= 3\n",
    "        num_workers= 0\n",
    "        output_size= 2\n",
    "        parallel= False\n",
    "        pooling_method= \"sum\"\n",
    "        reprocess= True\n",
    "        residual= False\n",
    "        seed= 224\n",
    "        split= 10\n",
    "        train_eps= False\n",
    "        use_rd= False\n",
    "        virtual_node= False\n",
    "        wo_edge_feature= False\n",
    "        wo_path_encoding= False\n",
    "        wo_peripheral_configuration= False\n",
    "        wo_peripheral_edge= False\n",
    "        epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps user features and movie features to a same dimensional embedding space\n",
    "class InitUserMovieEmb(torch.nn.Module):\n",
    "    def __init__(self, user_input_size, movie_input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.user_linear_encoder = torch.nn.Linear(user_input_size, output_size)\n",
    "        self.movie_linear_encoder = torch.nn.Linear(movie_input_size, output_size)\n",
    "    \n",
    "    def forward(self, user_x, movie_x):\n",
    "        user_emb = self.user_linear_encoder(user_x)\n",
    "        movie_emb = self.movie_linear_encoder(movie_x)\n",
    "        embs = torch.cat([user_emb, movie_emb], dim=0)\n",
    "        return embs, user_emb.size(0)\n",
    "\n",
    "# same as above but for peripheral graph\n",
    "class Peripheral_Emb(torch.nn.Module):\n",
    "    def __init__(self, user_input_size, movie_input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.user_linear_encoder = torch.nn.Linear(user_input_size, output_size)\n",
    "        self.movie_linear_encoder = torch.nn.Linear(movie_input_size, output_size)\n",
    "    \n",
    "    def forward(self, user_x, movie_x):\n",
    "        user_emb = self.user_linear_encoder(user_x.to(torch.float32))\n",
    "        movie_emb = self.movie_linear_encoder(movie_x)\n",
    "        embs = torch.cat([user_emb, movie_emb], dim=0)\n",
    "        return embs, user_emb.size(0)\n",
    "\n",
    "# given two node embeddings, determines the rating \n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels * 2, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, user_dict, movies_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([user_dict[row], movies_dict[col]], dim=-1)\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.init_peripheral = Peripheral_Emb(user_input_size=user_peripheral_info.size(1), \n",
    "                                              movie_input_size=movie_features.size(1), \n",
    "                                              output_size=args.hidden_size)\n",
    "        \n",
    "        self.init_user_movie_emb = InitUserMovieEmb(user_input_size=user_features.size(1),\n",
    "                                                    movie_input_size=movie_features.size(1), \n",
    "                                                    output_size=args.hidden_size)\n",
    "\n",
    "        self.encoder = get_model(args)\n",
    "        \n",
    "        self.decoder = EdgeDecoder(hidden_channels=args.hidden_size)\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"K: \",args.K)\n",
    "        print(\"Kernel: \", args.kernel)\n",
    "        print(\"Model Name: \", args.model_name)\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self, data, edge_label_index):\n",
    "        node_embs, split_index = self.init_user_movie_emb(data.x_dict['user'], data.x_dict['movie'])\n",
    "        peri_embs, _peri_split_index = self.init_peripheral(data['user_features'], data.x_dict['movie'])\n",
    "        data['x'] = node_embs\n",
    "        data['pe_attr'] = peri_embs\n",
    "        z_dict = self.encoder(data)\n",
    "        user_dict, movies_dict = torch.split(z_dict, [split_index, node_embs.size(0) - split_index], dim=0) # To split user and movie embeddings\n",
    "        return self.decoder(user_dict, movies_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "args.epochs = 500\n",
    "k_range = [1, 2, 3]\n",
    "kernel_set = [\"spd\", \"gd\"]\n",
    "variations = list(product(set(k_range),set(kernel_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "K:  1\n",
      "Kernel:  gd\n",
      "Model Name:  KPGIN\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 14.8744, Train: 3.1692, Val: 3.1714\n",
      "Epoch: 002, Loss: 12.4428, Train: 2.7895, Val: 2.7926\n",
      "Epoch: 003, Loss: 9.1875, Train: 2.5537, Val: 2.5546\n",
      "Epoch: 004, Loss: 5.4625, Train: 2.2138, Val: 2.2128\n",
      "Epoch: 005, Loss: 3.2286, Train: 1.7473, Val: 1.7431\n",
      "Epoch: 006, Loss: 3.3760, Train: 1.5694, Val: 1.5632\n",
      "Epoch: 007, Loss: 2.5965, Train: 1.5194, Val: 1.5136\n",
      "Epoch: 008, Loss: 2.0711, Train: 1.5076, Val: 1.5035\n",
      "Epoch: 009, Loss: 1.7724, Train: 1.4965, Val: 1.4942\n",
      "Epoch: 010, Loss: 1.7367, Train: 1.4867, Val: 1.4816\n",
      "Epoch: 011, Loss: 1.5297, Train: 1.4989, Val: 1.4936\n",
      "Epoch: 012, Loss: 1.5209, Train: 1.5087, Val: 1.5037\n",
      "Epoch: 013, Loss: 1.4164, Train: 1.5235, Val: 1.5187\n",
      "Epoch: 014, Loss: 1.3999, Train: 1.5291, Val: 1.5245\n",
      "Epoch: 015, Loss: 1.3961, Train: 1.5258, Val: 1.5213\n",
      "Epoch: 016, Loss: 1.3781, Train: 1.5151, Val: 1.5105\n",
      "Epoch: 017, Loss: 1.3478, Train: 1.5004, Val: 1.4958\n",
      "Epoch: 018, Loss: 1.3220, Train: 1.4852, Val: 1.4806\n",
      "Epoch: 019, Loss: 1.3070, Train: 1.4681, Val: 1.4642\n",
      "Epoch: 020, Loss: 1.2919, Train: 1.4463, Val: 1.4436\n",
      "Epoch: 021, Loss: 1.2819, Train: 1.4230, Val: 1.4207\n",
      "Epoch: 022, Loss: 1.2764, Train: 1.3887, Val: 1.3855\n",
      "Epoch: 023, Loss: 1.2722, Train: 1.3979, Val: 1.3950\n",
      "Epoch: 024, Loss: 1.2619, Train: 1.4074, Val: 1.4048\n",
      "Epoch: 025, Loss: 1.2607, Train: 1.4167, Val: 1.4142\n",
      "Epoch: 026, Loss: 1.2513, Train: 1.4265, Val: 1.4239\n",
      "Epoch: 027, Loss: 1.2437, Train: 1.4331, Val: 1.4306\n",
      "Epoch: 028, Loss: 1.2329, Train: 1.4295, Val: 1.4269\n",
      "Epoch: 029, Loss: 1.2332, Train: 1.4256, Val: 1.4230\n",
      "Epoch: 030, Loss: 1.2345, Train: 1.4204, Val: 1.4178\n",
      "Epoch: 031, Loss: 1.2282, Train: 1.4117, Val: 1.4089\n",
      "Epoch: 032, Loss: 1.2176, Train: 1.4062, Val: 1.4032\n",
      "Epoch: 033, Loss: 1.2126, Train: 1.3950, Val: 1.3920\n",
      "Epoch: 034, Loss: 1.2048, Train: 1.3888, Val: 1.3856\n",
      "Epoch: 035, Loss: 1.2040, Train: 1.3823, Val: 1.3792\n",
      "Epoch: 036, Loss: 1.2032, Train: 1.3750, Val: 1.3719\n",
      "Epoch: 037, Loss: 1.1972, Train: 1.3671, Val: 1.3640\n",
      "Epoch: 038, Loss: 1.1958, Train: 1.3506, Val: 1.3472\n",
      "Epoch: 039, Loss: 1.1918, Train: 1.3366, Val: 1.3331\n",
      "Epoch: 040, Loss: 1.1912, Train: 1.3297, Val: 1.3261\n",
      "Epoch: 041, Loss: 1.1883, Train: 1.3217, Val: 1.3179\n",
      "Epoch: 042, Loss: 1.1862, Train: 1.3115, Val: 1.3076\n",
      "Epoch: 043, Loss: 1.1834, Train: 1.3048, Val: 1.3010\n",
      "Epoch: 044, Loss: 1.1817, Train: 1.2993, Val: 1.2955\n",
      "Epoch: 045, Loss: 1.1807, Train: 1.2887, Val: 1.2850\n",
      "Epoch: 046, Loss: 1.1791, Train: 1.2725, Val: 1.2690\n",
      "Epoch: 047, Loss: 1.1773, Train: 1.2602, Val: 1.2566\n",
      "Epoch: 048, Loss: 1.1759, Train: 1.2534, Val: 1.2498\n",
      "Epoch: 049, Loss: 1.1753, Train: 1.2391, Val: 1.2355\n",
      "Epoch: 050, Loss: 1.1736, Train: 1.2274, Val: 1.2239\n",
      "Epoch: 051, Loss: 1.1723, Train: 1.2228, Val: 1.2193\n",
      "Epoch: 052, Loss: 1.1707, Train: 1.2117, Val: 1.2082\n",
      "Epoch: 053, Loss: 1.1696, Train: 1.1922, Val: 1.1889\n",
      "Epoch: 054, Loss: 1.1687, Train: 1.1755, Val: 1.1727\n",
      "Epoch: 055, Loss: 1.1673, Train: 1.1607, Val: 1.1585\n",
      "Epoch: 056, Loss: 1.1665, Train: 1.1274, Val: 1.1262\n",
      "Epoch: 057, Loss: 1.1657, Train: 1.1164, Val: 1.1157\n",
      "Epoch: 058, Loss: 1.1648, Train: 1.1116, Val: 1.1113\n",
      "Epoch: 059, Loss: 1.1639, Train: 1.0981, Val: 1.0980\n",
      "Epoch: 060, Loss: 1.1628, Train: 1.0956, Val: 1.0956\n",
      "Epoch: 061, Loss: 1.1622, Train: 1.1003, Val: 1.1005\n",
      "Epoch: 062, Loss: 1.1617, Train: 1.0872, Val: 1.0876\n",
      "Epoch: 063, Loss: 1.1609, Train: 1.0879, Val: 1.0883\n",
      "Epoch: 064, Loss: 1.1601, Train: 1.0944, Val: 1.0948\n",
      "Epoch: 065, Loss: 1.1600, Train: 1.0820, Val: 1.0822\n",
      "Epoch: 066, Loss: 1.1594, Train: 1.0869, Val: 1.0872\n",
      "Epoch: 067, Loss: 1.1583, Train: 1.0983, Val: 1.0987\n",
      "Epoch: 068, Loss: 1.1586, Train: 1.0787, Val: 1.0786\n",
      "Epoch: 069, Loss: 1.1590, Train: 1.0832, Val: 1.0833\n",
      "Epoch: 070, Loss: 1.1567, Train: 1.1100, Val: 1.1103\n",
      "Epoch: 071, Loss: 1.1600, Train: 1.1091, Val: 1.1082\n",
      "Epoch: 072, Loss: 1.1636, Train: 1.1006, Val: 1.0998\n",
      "Epoch: 073, Loss: 1.1617, Train: 1.0993, Val: 1.0996\n",
      "Epoch: 074, Loss: 1.1586, Train: 1.1186, Val: 1.1192\n",
      "Epoch: 075, Loss: 1.1585, Train: 1.0907, Val: 1.0897\n",
      "Epoch: 076, Loss: 1.1630, Train: 1.0928, Val: 1.0916\n",
      "Epoch: 077, Loss: 1.1582, Train: 1.0870, Val: 1.0870\n",
      "Epoch: 078, Loss: 1.1607, Train: 1.0898, Val: 1.0900\n",
      "Epoch: 079, Loss: 1.1540, Train: 1.0774, Val: 1.0773\n",
      "Epoch: 080, Loss: 1.1596, Train: 1.0771, Val: 1.0769\n",
      "Epoch: 081, Loss: 1.1524, Train: 1.0754, Val: 1.0747\n",
      "Epoch: 082, Loss: 1.1566, Train: 1.0812, Val: 1.0803\n",
      "Epoch: 083, Loss: 1.1527, Train: 1.0767, Val: 1.0761\n",
      "Epoch: 084, Loss: 1.1549, Train: 1.0811, Val: 1.0808\n",
      "Epoch: 085, Loss: 1.1521, Train: 1.0807, Val: 1.0797\n",
      "Epoch: 086, Loss: 1.1537, Train: 1.1036, Val: 1.1020\n",
      "Epoch: 087, Loss: 1.1514, Train: 1.0798, Val: 1.0788\n",
      "Epoch: 088, Loss: 1.1511, Train: 1.0930, Val: 1.0927\n",
      "Epoch: 089, Loss: 1.1525, Train: 1.1325, Val: 1.1305\n",
      "Epoch: 090, Loss: 1.1509, Train: 1.1408, Val: 1.1389\n",
      "Epoch: 091, Loss: 1.1518, Train: 1.0936, Val: 1.0927\n",
      "Epoch: 092, Loss: 1.1490, Train: 1.1254, Val: 1.1244\n",
      "Epoch: 093, Loss: 1.1511, Train: 1.3062, Val: 1.3011\n",
      "Epoch: 094, Loss: 1.1500, Train: 1.3821, Val: 1.3767\n",
      "Epoch: 095, Loss: 1.1508, Train: 1.2386, Val: 1.2338\n",
      "Epoch: 096, Loss: 1.1481, Train: 1.2148, Val: 1.2108\n",
      "Epoch: 097, Loss: 1.1488, Train: 1.3605, Val: 1.3550\n",
      "Epoch: 098, Loss: 1.1481, Train: 1.4913, Val: 1.4857\n",
      "Epoch: 099, Loss: 1.1482, Train: 1.0862, Val: 1.0860\n",
      "Epoch: 100, Loss: 1.1472, Train: 1.0728, Val: 1.0724\n",
      "Epoch: 101, Loss: 1.1473, Train: 1.0793, Val: 1.0785\n",
      "Epoch: 102, Loss: 1.1466, Train: 1.0758, Val: 1.0753\n",
      "Epoch: 103, Loss: 1.1469, Train: 1.0731, Val: 1.0729\n",
      "Epoch: 104, Loss: 1.1453, Train: 1.0724, Val: 1.0718\n",
      "Epoch: 105, Loss: 1.1461, Train: 1.0705, Val: 1.0700\n",
      "Epoch: 106, Loss: 1.1445, Train: 1.0723, Val: 1.0718\n",
      "Epoch: 107, Loss: 1.1453, Train: 1.0768, Val: 1.0758\n",
      "Epoch: 108, Loss: 1.1440, Train: 1.0752, Val: 1.0744\n",
      "Epoch: 109, Loss: 1.1440, Train: 1.0890, Val: 1.0890\n",
      "Epoch: 110, Loss: 1.1450, Train: 1.1476, Val: 1.1459\n",
      "Epoch: 111, Loss: 1.1483, Train: 1.0753, Val: 1.0745\n",
      "Epoch: 112, Loss: 1.1441, Train: 1.1425, Val: 1.1435\n",
      "Epoch: 113, Loss: 1.1528, Train: 1.1883, Val: 1.1871\n",
      "Epoch: 114, Loss: 1.1503, Train: 1.2368, Val: 1.2355\n",
      "Epoch: 115, Loss: 1.1538, Train: 1.0737, Val: 1.0734\n",
      "Epoch: 116, Loss: 1.1444, Train: 1.1698, Val: 1.1711\n",
      "Epoch: 117, Loss: 1.1545, Train: 1.0776, Val: 1.0769\n",
      "Epoch: 118, Loss: 1.1445, Train: 1.1613, Val: 1.1599\n",
      "Epoch: 119, Loss: 1.1496, Train: 1.0890, Val: 1.0880\n",
      "Epoch: 120, Loss: 1.1462, Train: 1.0962, Val: 1.0966\n",
      "Epoch: 121, Loss: 1.1449, Train: 1.1273, Val: 1.1280\n",
      "Epoch: 122, Loss: 1.1469, Train: 1.1023, Val: 1.1010\n",
      "Epoch: 123, Loss: 1.1430, Train: 1.1520, Val: 1.1506\n",
      "Epoch: 124, Loss: 1.1452, Train: 1.1022, Val: 1.1011\n",
      "Epoch: 125, Loss: 1.1434, Train: 1.0779, Val: 1.0778\n",
      "Epoch: 126, Loss: 1.1422, Train: 1.0734, Val: 1.0728\n",
      "Epoch: 127, Loss: 1.1447, Train: 1.0738, Val: 1.0730\n",
      "Epoch: 128, Loss: 1.1416, Train: 1.0788, Val: 1.0780\n",
      "Epoch: 129, Loss: 1.1443, Train: 1.1655, Val: 1.1640\n",
      "Epoch: 130, Loss: 1.1595, Train: 1.1454, Val: 1.1438\n",
      "Epoch: 131, Loss: 1.1510, Train: 1.1136, Val: 1.1140\n",
      "Epoch: 132, Loss: 1.1572, Train: 1.1121, Val: 1.1127\n",
      "Epoch: 133, Loss: 1.1476, Train: 1.1009, Val: 1.1000\n",
      "Epoch: 134, Loss: 1.1565, Train: 1.1627, Val: 1.1614\n",
      "Epoch: 135, Loss: 1.1480, Train: 1.0887, Val: 1.0878\n",
      "Epoch: 136, Loss: 1.1533, Train: 1.1041, Val: 1.1039\n",
      "Epoch: 137, Loss: 1.1465, Train: 1.1146, Val: 1.1145\n",
      "Epoch: 138, Loss: 1.1521, Train: 1.0771, Val: 1.0762\n",
      "Epoch: 139, Loss: 1.1427, Train: 1.1242, Val: 1.1226\n",
      "Epoch: 140, Loss: 1.1481, Train: 1.1108, Val: 1.1094\n",
      "Epoch: 141, Loss: 1.1460, Train: 1.0700, Val: 1.0697\n",
      "Epoch: 142, Loss: 1.1439, Train: 1.1069, Val: 1.1073\n",
      "Epoch: 143, Loss: 1.1461, Train: 1.0751, Val: 1.0746\n",
      "Epoch: 144, Loss: 1.1420, Train: 1.1145, Val: 1.1130\n",
      "Epoch: 145, Loss: 1.1441, Train: 1.1112, Val: 1.1098\n",
      "Epoch: 146, Loss: 1.1419, Train: 1.0784, Val: 1.0775\n",
      "Epoch: 147, Loss: 1.1430, Train: 1.2365, Val: 1.2314\n",
      "Epoch: 148, Loss: 1.1400, Train: 1.1654, Val: 1.1631\n",
      "Epoch: 149, Loss: 1.1403, Train: 1.1009, Val: 1.0995\n",
      "Epoch: 150, Loss: 1.1399, Train: 1.0900, Val: 1.0888\n",
      "Epoch: 151, Loss: 1.1406, Train: 1.0897, Val: 1.0889\n",
      "Epoch: 152, Loss: 1.1389, Train: 1.2640, Val: 1.2581\n",
      "Epoch: 153, Loss: 1.1425, Train: 1.2979, Val: 1.2916\n",
      "Epoch: 154, Loss: 1.1382, Train: 1.4691, Val: 1.4640\n",
      "Epoch: 155, Loss: 1.1426, Train: 1.3522, Val: 1.3459\n",
      "Epoch: 156, Loss: 1.1411, Train: 1.1081, Val: 1.1069\n",
      "Epoch: 157, Loss: 1.1405, Train: 1.0748, Val: 1.0746\n",
      "Epoch: 158, Loss: 1.1406, Train: 1.0737, Val: 1.0732\n",
      "Epoch: 159, Loss: 1.1382, Train: 1.1028, Val: 1.1015\n",
      "Epoch: 160, Loss: 1.1396, Train: 1.2119, Val: 1.2075\n",
      "Epoch: 161, Loss: 1.1374, Train: 1.4347, Val: 1.4293\n",
      "Epoch: 162, Loss: 1.1386, Train: 1.5011, Val: 1.4950\n",
      "Epoch: 163, Loss: 1.1378, Train: 1.1927, Val: 1.1908\n",
      "Epoch: 164, Loss: 1.1385, Train: 1.0956, Val: 1.0953\n",
      "Epoch: 165, Loss: 1.1363, Train: 1.0776, Val: 1.0773\n",
      "Epoch: 166, Loss: 1.1369, Train: 1.1100, Val: 1.1105\n",
      "Epoch: 167, Loss: 1.1356, Train: 1.1358, Val: 1.1368\n",
      "Epoch: 168, Loss: 1.1352, Train: 1.0914, Val: 1.0916\n",
      "Epoch: 169, Loss: 1.1355, Train: 1.5599, Val: 1.5555\n",
      "Epoch: 170, Loss: 1.1428, Train: 1.5807, Val: 1.5770\n",
      "Epoch: 171, Loss: 1.1381, Train: 1.5826, Val: 1.5787\n",
      "Epoch: 172, Loss: 1.1364, Train: 1.5591, Val: 1.5545\n",
      "Epoch: 173, Loss: 1.1377, Train: 1.5724, Val: 1.5681\n",
      "Epoch: 174, Loss: 1.1325, Train: 1.5964, Val: 1.5927\n",
      "Epoch: 175, Loss: 1.1369, Train: 1.5731, Val: 1.5687\n",
      "Epoch: 176, Loss: 1.1345, Train: 1.5909, Val: 1.5869\n",
      "Epoch: 177, Loss: 1.1314, Train: 1.5743, Val: 1.5701\n",
      "Epoch: 178, Loss: 1.1399, Train: 1.5439, Val: 1.5398\n",
      "Epoch: 179, Loss: 1.1872, Train: 1.5083, Val: 1.5034\n",
      "Epoch: 180, Loss: 1.1875, Train: 1.5646, Val: 1.5626\n",
      "Epoch: 181, Loss: 1.3524, Train: 1.4096, Val: 1.4065\n",
      "Epoch: 182, Loss: 1.2844, Train: 1.5984, Val: 1.5955\n",
      "Epoch: 183, Loss: 1.2475, Train: 1.5831, Val: 1.5810\n",
      "Epoch: 184, Loss: 1.2609, Train: 1.6050, Val: 1.6029\n",
      "Epoch: 185, Loss: 1.2525, Train: 1.6972, Val: 1.6950\n",
      "Epoch: 186, Loss: 1.2342, Train: 1.6840, Val: 1.6817\n",
      "Epoch: 187, Loss: 1.2556, Train: 1.5105, Val: 1.5083\n",
      "Epoch: 188, Loss: 1.2224, Train: 1.8060, Val: 1.8038\n",
      "Epoch: 189, Loss: 1.2557, Train: 1.7525, Val: 1.7501\n",
      "Epoch: 190, Loss: 1.2321, Train: 1.7833, Val: 1.7809\n",
      "Epoch: 191, Loss: 1.2401, Train: 1.6666, Val: 1.6634\n",
      "Epoch: 192, Loss: 1.2379, Train: 1.5723, Val: 1.5685\n",
      "Epoch: 193, Loss: 1.2268, Train: 1.1606, Val: 1.1603\n",
      "Epoch: 194, Loss: 1.2374, Train: 1.4988, Val: 1.5005\n",
      "Epoch: 195, Loss: 1.2132, Train: 1.5109, Val: 1.5128\n",
      "Epoch: 196, Loss: 1.2242, Train: 1.3932, Val: 1.3956\n",
      "Epoch: 197, Loss: 1.2143, Train: 1.1843, Val: 1.1859\n",
      "Epoch: 198, Loss: 1.2178, Train: 1.1240, Val: 1.1252\n",
      "Epoch: 199, Loss: 1.2137, Train: 1.1024, Val: 1.1033\n",
      "Epoch: 200, Loss: 1.2105, Train: 1.5633, Val: 1.5612\n",
      "Epoch: 201, Loss: 1.2060, Train: 1.1303, Val: 1.1313\n",
      "Epoch: 202, Loss: 1.2018, Train: 1.1275, Val: 1.1291\n",
      "Epoch: 203, Loss: 1.2025, Train: 1.1055, Val: 1.1068\n",
      "Epoch: 204, Loss: 1.1971, Train: 1.0954, Val: 1.0965\n",
      "Epoch: 205, Loss: 1.1961, Train: 1.0977, Val: 1.0986\n",
      "Epoch: 206, Loss: 1.1923, Train: 1.0980, Val: 1.0988\n",
      "Epoch: 207, Loss: 1.1900, Train: 1.0986, Val: 1.0993\n",
      "Epoch: 208, Loss: 1.1870, Train: 1.0978, Val: 1.0985\n",
      "Epoch: 209, Loss: 1.1836, Train: 1.0976, Val: 1.0986\n",
      "Epoch: 210, Loss: 1.1811, Train: 1.1038, Val: 1.1050\n",
      "Epoch: 211, Loss: 1.1791, Train: 1.1078, Val: 1.1090\n",
      "Epoch: 212, Loss: 1.1765, Train: 1.1083, Val: 1.1095\n",
      "Epoch: 213, Loss: 1.1743, Train: 1.1061, Val: 1.1072\n",
      "Epoch: 214, Loss: 1.1729, Train: 1.0997, Val: 1.1007\n",
      "Epoch: 215, Loss: 1.1707, Train: 1.0943, Val: 1.0952\n",
      "Epoch: 216, Loss: 1.1692, Train: 1.0914, Val: 1.0921\n",
      "Epoch: 217, Loss: 1.1675, Train: 1.0914, Val: 1.0920\n",
      "Epoch: 218, Loss: 1.1661, Train: 1.0898, Val: 1.0902\n",
      "Epoch: 219, Loss: 1.1647, Train: 1.0863, Val: 1.0865\n",
      "Epoch: 220, Loss: 1.1631, Train: 1.0865, Val: 1.0865\n",
      "Epoch: 221, Loss: 1.1615, Train: 1.0868, Val: 1.0867\n",
      "Epoch: 222, Loss: 1.1595, Train: 1.0867, Val: 1.0865\n",
      "Epoch: 223, Loss: 1.1579, Train: 1.0953, Val: 1.0949\n",
      "Epoch: 224, Loss: 1.1556, Train: 1.1104, Val: 1.1100\n",
      "Epoch: 225, Loss: 1.1542, Train: 1.1152, Val: 1.1148\n",
      "Epoch: 226, Loss: 1.1523, Train: 1.1498, Val: 1.1493\n",
      "Epoch: 227, Loss: 1.1514, Train: 1.1938, Val: 1.1927\n",
      "Epoch: 228, Loss: 1.1652, Train: 1.4844, Val: 1.4840\n",
      "Epoch: 229, Loss: 1.1583, Train: 1.6080, Val: 1.6073\n",
      "Epoch: 230, Loss: 1.1666, Train: 1.3271, Val: 1.3266\n",
      "Epoch: 231, Loss: 1.1905, Train: 1.1283, Val: 1.1280\n",
      "Epoch: 232, Loss: 1.1721, Train: 1.0822, Val: 1.0828\n",
      "Epoch: 233, Loss: 1.1887, Train: 1.2091, Val: 1.2106\n",
      "Epoch: 234, Loss: 1.1675, Train: 1.3338, Val: 1.3359\n",
      "Epoch: 235, Loss: 1.1685, Train: 1.4125, Val: 1.4146\n",
      "Epoch: 236, Loss: 1.1568, Train: 1.4151, Val: 1.4172\n",
      "Epoch: 237, Loss: 1.1725, Train: 1.6414, Val: 1.6392\n",
      "Epoch: 238, Loss: 1.1539, Train: 1.3352, Val: 1.3371\n",
      "Epoch: 239, Loss: 1.1538, Train: 1.3384, Val: 1.3404\n",
      "Epoch: 240, Loss: 1.1566, Train: 1.1418, Val: 1.1426\n",
      "Epoch: 241, Loss: 1.1530, Train: 1.4319, Val: 1.4339\n",
      "Epoch: 242, Loss: 1.1517, Train: 1.5580, Val: 1.5600\n",
      "Epoch: 243, Loss: 1.1512, Train: 1.5309, Val: 1.5331\n",
      "Epoch: 244, Loss: 1.1464, Train: 1.2405, Val: 1.2415\n",
      "Epoch: 245, Loss: 1.1602, Train: 1.6409, Val: 1.6433\n",
      "Epoch: 246, Loss: 1.1539, Train: 1.6740, Val: 1.6764\n",
      "Epoch: 247, Loss: 1.1609, Train: 1.6347, Val: 1.6370\n",
      "Epoch: 248, Loss: 1.1637, Train: 1.5570, Val: 1.5591\n",
      "Epoch: 249, Loss: 1.1553, Train: 1.5448, Val: 1.5470\n",
      "Epoch: 250, Loss: 1.1483, Train: 1.5817, Val: 1.5839\n",
      "Epoch: 251, Loss: 1.1452, Train: 1.6104, Val: 1.6126\n",
      "Epoch: 252, Loss: 1.1492, Train: 1.6109, Val: 1.6131\n",
      "Epoch: 253, Loss: 1.1476, Train: 1.5469, Val: 1.5489\n",
      "Epoch: 254, Loss: 1.1416, Train: 1.3689, Val: 1.3705\n",
      "Epoch: 255, Loss: 1.1410, Train: 1.2202, Val: 1.2214\n",
      "Epoch: 256, Loss: 1.1408, Train: 1.1507, Val: 1.1516\n",
      "Epoch: 257, Loss: 1.1410, Train: 1.1315, Val: 1.1322\n",
      "Epoch: 258, Loss: 1.1372, Train: 1.1376, Val: 1.1384\n",
      "Epoch: 259, Loss: 1.1343, Train: 1.1401, Val: 1.1410\n",
      "Epoch: 260, Loss: 1.1316, Train: 1.1320, Val: 1.1328\n",
      "Epoch: 261, Loss: 1.1353, Train: 1.1539, Val: 1.1549\n",
      "Epoch: 262, Loss: 1.1316, Train: 1.1642, Val: 1.1652\n",
      "Epoch: 263, Loss: 1.1267, Train: 1.1736, Val: 1.1747\n",
      "Epoch: 264, Loss: 1.1267, Train: 1.2023, Val: 1.2037\n",
      "Epoch: 265, Loss: 1.1221, Train: 1.2165, Val: 1.2180\n",
      "Epoch: 266, Loss: 1.1252, Train: 1.1854, Val: 1.1866\n",
      "Epoch: 267, Loss: 1.1201, Train: 1.1515, Val: 1.1523\n",
      "Epoch: 268, Loss: 1.1210, Train: 1.1506, Val: 1.1514\n",
      "Epoch: 269, Loss: 1.1168, Train: 1.1700, Val: 1.1710\n",
      "Epoch: 270, Loss: 1.1172, Train: 1.1721, Val: 1.1733\n",
      "Epoch: 271, Loss: 1.1148, Train: 1.1564, Val: 1.1574\n",
      "Epoch: 272, Loss: 1.1143, Train: 1.1353, Val: 1.1361\n",
      "Epoch: 273, Loss: 1.1115, Train: 1.1255, Val: 1.1264\n",
      "Epoch: 274, Loss: 1.1119, Train: 1.1265, Val: 1.1273\n",
      "Epoch: 275, Loss: 1.1090, Train: 1.1309, Val: 1.1317\n",
      "Epoch: 276, Loss: 1.1087, Train: 1.1193, Val: 1.1202\n",
      "Epoch: 277, Loss: 1.1068, Train: 1.0927, Val: 1.0936\n",
      "Epoch: 278, Loss: 1.1053, Train: 1.0743, Val: 1.0751\n",
      "Epoch: 279, Loss: 1.1045, Train: 1.0689, Val: 1.0697\n",
      "Epoch: 280, Loss: 1.1030, Train: 1.0713, Val: 1.0723\n",
      "Epoch: 281, Loss: 1.1021, Train: 1.0731, Val: 1.0741\n",
      "Epoch: 282, Loss: 1.1009, Train: 1.0620, Val: 1.0628\n",
      "Epoch: 283, Loss: 1.0999, Train: 1.0525, Val: 1.0533\n",
      "Epoch: 284, Loss: 1.0988, Train: 1.0510, Val: 1.0518\n",
      "Epoch: 285, Loss: 1.0982, Train: 1.0527, Val: 1.0536\n",
      "Epoch: 286, Loss: 1.0969, Train: 1.0564, Val: 1.0573\n",
      "Epoch: 287, Loss: 1.0963, Train: 1.0528, Val: 1.0537\n",
      "Epoch: 288, Loss: 1.0956, Train: 1.0492, Val: 1.0499\n",
      "Epoch: 289, Loss: 1.0944, Train: 1.0485, Val: 1.0493\n",
      "Epoch: 290, Loss: 1.0939, Train: 1.0482, Val: 1.0490\n",
      "Epoch: 291, Loss: 1.0927, Train: 1.0479, Val: 1.0489\n",
      "Epoch: 292, Loss: 1.0917, Train: 1.0488, Val: 1.0497\n",
      "Epoch: 293, Loss: 1.0908, Train: 1.0472, Val: 1.0480\n",
      "Epoch: 294, Loss: 1.0894, Train: 1.0477, Val: 1.0484\n",
      "Epoch: 295, Loss: 1.0881, Train: 1.0531, Val: 1.0535\n",
      "Epoch: 296, Loss: 1.0871, Train: 1.0591, Val: 1.0590\n",
      "Epoch: 297, Loss: 1.0861, Train: 1.0607, Val: 1.0604\n",
      "Epoch: 298, Loss: 1.0849, Train: 1.0586, Val: 1.0585\n",
      "Epoch: 299, Loss: 1.0841, Train: 1.0549, Val: 1.0551\n",
      "Epoch: 300, Loss: 1.0830, Train: 1.0563, Val: 1.0567\n",
      "Epoch: 301, Loss: 1.0821, Train: 1.0527, Val: 1.0531\n",
      "Epoch: 302, Loss: 1.0811, Train: 1.0480, Val: 1.0488\n",
      "Epoch: 303, Loss: 1.0806, Train: 1.0461, Val: 1.0468\n",
      "Epoch: 304, Loss: 1.0808, Train: 1.0482, Val: 1.0489\n",
      "Epoch: 305, Loss: 1.0793, Train: 1.0454, Val: 1.0461\n",
      "Epoch: 306, Loss: 1.0780, Train: 1.0456, Val: 1.0464\n",
      "Epoch: 307, Loss: 1.0768, Train: 1.0547, Val: 1.0548\n",
      "Epoch: 308, Loss: 1.0757, Train: 1.0478, Val: 1.0483\n",
      "Epoch: 309, Loss: 1.0748, Train: 1.0520, Val: 1.0524\n",
      "Epoch: 310, Loss: 1.0738, Train: 1.0413, Val: 1.0426\n",
      "Epoch: 311, Loss: 1.0728, Train: 1.0440, Val: 1.0455\n",
      "Epoch: 312, Loss: 1.0719, Train: 1.0484, Val: 1.0491\n",
      "Epoch: 313, Loss: 1.0717, Train: 1.0489, Val: 1.0499\n",
      "Epoch: 314, Loss: 1.0723, Train: 1.0545, Val: 1.0551\n",
      "Epoch: 315, Loss: 1.0757, Train: 1.0402, Val: 1.0417\n",
      "Epoch: 316, Loss: 1.0699, Train: 1.0501, Val: 1.0519\n",
      "Epoch: 317, Loss: 1.0692, Train: 1.0500, Val: 1.0520\n",
      "Epoch: 318, Loss: 1.0758, Train: 1.0412, Val: 1.0427\n",
      "Epoch: 319, Loss: 1.0712, Train: 1.0401, Val: 1.0418\n",
      "Epoch: 320, Loss: 1.0646, Train: 1.0496, Val: 1.0515\n",
      "Epoch: 321, Loss: 1.0727, Train: 1.4976, Val: 1.4931\n",
      "Epoch: 322, Loss: 1.1037, Train: 1.5220, Val: 1.5183\n",
      "Epoch: 323, Loss: 1.1256, Train: 1.5105, Val: 1.5061\n",
      "Epoch: 324, Loss: 1.1125, Train: 1.3959, Val: 1.3930\n",
      "Epoch: 325, Loss: 1.0974, Train: 1.1458, Val: 1.1447\n",
      "Epoch: 326, Loss: 1.1044, Train: 1.2846, Val: 1.2837\n",
      "Epoch: 327, Loss: 1.1017, Train: 1.1897, Val: 1.1903\n",
      "Epoch: 328, Loss: 1.0924, Train: 1.1143, Val: 1.1144\n",
      "Epoch: 329, Loss: 1.0963, Train: 1.2098, Val: 1.2085\n",
      "Epoch: 330, Loss: 1.1028, Train: 1.4015, Val: 1.3997\n",
      "Epoch: 331, Loss: 1.1012, Train: 1.4618, Val: 1.4588\n",
      "Epoch: 332, Loss: 1.1018, Train: 1.4399, Val: 1.4368\n",
      "Epoch: 333, Loss: 1.0931, Train: 1.4186, Val: 1.4154\n",
      "Epoch: 334, Loss: 1.0934, Train: 1.4172, Val: 1.4139\n",
      "Epoch: 335, Loss: 1.0918, Train: 1.4030, Val: 1.4002\n",
      "Epoch: 336, Loss: 1.0885, Train: 1.3851, Val: 1.3828\n",
      "Epoch: 337, Loss: 1.0869, Train: 1.3913, Val: 1.3890\n",
      "Epoch: 338, Loss: 1.0844, Train: 1.3681, Val: 1.3658\n",
      "Epoch: 339, Loss: 1.0797, Train: 1.3464, Val: 1.3450\n",
      "Epoch: 340, Loss: 1.0774, Train: 1.2844, Val: 1.2824\n",
      "Epoch: 341, Loss: 1.0773, Train: 1.1824, Val: 1.1821\n",
      "Epoch: 342, Loss: 1.0745, Train: 1.0868, Val: 1.0860\n",
      "Epoch: 343, Loss: 1.0711, Train: 1.0542, Val: 1.0556\n",
      "Epoch: 344, Loss: 1.0688, Train: 1.0643, Val: 1.0654\n",
      "Epoch: 345, Loss: 1.0650, Train: 1.0558, Val: 1.0571\n",
      "Epoch: 346, Loss: 1.0634, Train: 1.0478, Val: 1.0493\n",
      "Epoch: 347, Loss: 1.0597, Train: 1.0495, Val: 1.0509\n",
      "Epoch: 348, Loss: 1.0607, Train: 1.0515, Val: 1.0526\n",
      "Epoch: 349, Loss: 1.0571, Train: 1.0504, Val: 1.0514\n",
      "Epoch: 350, Loss: 1.0555, Train: 1.0409, Val: 1.0425\n",
      "Epoch: 351, Loss: 1.0533, Train: 1.0447, Val: 1.0464\n",
      "Epoch: 352, Loss: 1.0514, Train: 1.0483, Val: 1.0496\n",
      "Epoch: 353, Loss: 1.0493, Train: 1.0520, Val: 1.0534\n",
      "Epoch: 354, Loss: 1.0481, Train: 1.0495, Val: 1.0510\n",
      "Epoch: 355, Loss: 1.0474, Train: 1.0469, Val: 1.0484\n",
      "Epoch: 356, Loss: 1.0440, Train: 1.0890, Val: 1.0882\n",
      "Epoch: 357, Loss: 1.0426, Train: 1.2201, Val: 1.2187\n",
      "Epoch: 358, Loss: 1.0406, Train: 1.2877, Val: 1.2860\n",
      "Epoch: 359, Loss: 1.0393, Train: 1.3172, Val: 1.3162\n",
      "Epoch: 360, Loss: 1.0380, Train: 1.2928, Val: 1.2909\n",
      "Epoch: 361, Loss: 1.0361, Train: 1.2550, Val: 1.2527\n",
      "Epoch: 362, Loss: 1.0350, Train: 1.2020, Val: 1.2008\n",
      "Epoch: 363, Loss: 1.0337, Train: 1.1485, Val: 1.1481\n",
      "Epoch: 364, Loss: 1.0323, Train: 1.1206, Val: 1.1207\n",
      "Epoch: 365, Loss: 1.0309, Train: 1.0870, Val: 1.0865\n",
      "Epoch: 366, Loss: 1.0295, Train: 1.0861, Val: 1.0860\n",
      "Epoch: 367, Loss: 1.0289, Train: 1.0631, Val: 1.0642\n",
      "Epoch: 368, Loss: 1.0272, Train: 1.0594, Val: 1.0609\n",
      "Epoch: 369, Loss: 1.0258, Train: 1.0586, Val: 1.0602\n",
      "Epoch: 370, Loss: 1.0256, Train: 1.0595, Val: 1.0617\n",
      "Epoch: 371, Loss: 1.0267, Train: 1.0555, Val: 1.0576\n",
      "Epoch: 372, Loss: 1.0246, Train: 1.0508, Val: 1.0531\n",
      "Epoch: 373, Loss: 1.0239, Train: 1.0640, Val: 1.0655\n",
      "Epoch: 374, Loss: 1.0275, Train: 1.0352, Val: 1.0373\n",
      "Epoch: 375, Loss: 1.0348, Train: 1.0682, Val: 1.0683\n",
      "Epoch: 376, Loss: 1.0428, Train: 1.0707, Val: 1.0708\n",
      "Epoch: 377, Loss: 1.0259, Train: 1.0494, Val: 1.0507\n",
      "Epoch: 378, Loss: 1.0344, Train: 1.0444, Val: 1.0455\n",
      "Epoch: 379, Loss: 1.0250, Train: 1.0726, Val: 1.0736\n",
      "Epoch: 380, Loss: 1.0308, Train: 1.0471, Val: 1.0488\n",
      "Epoch: 381, Loss: 1.0220, Train: 1.0459, Val: 1.0481\n",
      "Epoch: 382, Loss: 1.0259, Train: 1.0363, Val: 1.0376\n",
      "Epoch: 383, Loss: 1.0188, Train: 1.0484, Val: 1.0492\n",
      "Epoch: 384, Loss: 1.0213, Train: 1.0522, Val: 1.0537\n",
      "Epoch: 385, Loss: 1.0158, Train: 1.0270, Val: 1.0298\n",
      "Epoch: 386, Loss: 1.0180, Train: 1.0297, Val: 1.0326\n",
      "Epoch: 387, Loss: 1.0140, Train: 1.0484, Val: 1.0500\n",
      "Epoch: 388, Loss: 1.0152, Train: 1.0501, Val: 1.0512\n",
      "Epoch: 389, Loss: 1.0124, Train: 1.0860, Val: 1.0862\n",
      "Epoch: 390, Loss: 1.0130, Train: 1.0540, Val: 1.0553\n",
      "Epoch: 391, Loss: 1.0098, Train: 1.0536, Val: 1.0551\n",
      "Epoch: 392, Loss: 1.0110, Train: 1.0868, Val: 1.0865\n",
      "Epoch: 393, Loss: 1.0085, Train: 1.0855, Val: 1.0855\n",
      "Epoch: 394, Loss: 1.0098, Train: 1.1025, Val: 1.1026\n",
      "Epoch: 395, Loss: 1.0061, Train: 1.1014, Val: 1.1019\n",
      "Epoch: 396, Loss: 1.0067, Train: 1.1057, Val: 1.1060\n",
      "Epoch: 397, Loss: 1.0040, Train: 1.0880, Val: 1.0885\n",
      "Epoch: 398, Loss: 1.0036, Train: 1.1032, Val: 1.1033\n",
      "Epoch: 399, Loss: 1.0035, Train: 1.0503, Val: 1.0527\n",
      "Epoch: 400, Loss: 1.0038, Train: 1.0439, Val: 1.0471\n",
      "Epoch: 401, Loss: 1.0068, Train: 1.0531, Val: 1.0565\n",
      "Epoch: 402, Loss: 1.0122, Train: 1.0567, Val: 1.0595\n",
      "Epoch: 403, Loss: 1.0100, Train: 1.0584, Val: 1.0608\n",
      "Epoch: 404, Loss: 1.0054, Train: 1.0555, Val: 1.0590\n",
      "Epoch: 405, Loss: 0.9985, Train: 1.0636, Val: 1.0671\n",
      "Epoch: 406, Loss: 1.0024, Train: 1.1046, Val: 1.1089\n",
      "Epoch: 407, Loss: 1.0101, Train: 1.0761, Val: 1.0785\n",
      "Epoch: 408, Loss: 1.0069, Train: 1.0324, Val: 1.0350\n",
      "Epoch: 409, Loss: 1.0107, Train: 1.0984, Val: 1.1007\n",
      "Epoch: 410, Loss: 1.0343, Train: 1.1275, Val: 1.1293\n",
      "Epoch: 411, Loss: 1.0286, Train: 1.1467, Val: 1.1481\n",
      "Epoch: 412, Loss: 1.0376, Train: 1.1308, Val: 1.1317\n",
      "Epoch: 413, Loss: 1.0117, Train: 1.0669, Val: 1.0681\n",
      "Epoch: 414, Loss: 1.0241, Train: 1.0387, Val: 1.0416\n",
      "Epoch: 415, Loss: 1.0106, Train: 1.0369, Val: 1.0405\n",
      "Epoch: 416, Loss: 1.0095, Train: 1.0202, Val: 1.0244\n",
      "Epoch: 417, Loss: 1.0411, Train: 1.0759, Val: 1.0770\n",
      "Epoch: 418, Loss: 1.1694, Train: 1.1421, Val: 1.1407\n",
      "Epoch: 419, Loss: 1.0860, Train: 1.2013, Val: 1.1998\n",
      "Epoch: 420, Loss: 1.0553, Train: 1.1245, Val: 1.1247\n",
      "Epoch: 421, Loss: 1.0750, Train: 1.2149, Val: 1.2155\n",
      "Epoch: 422, Loss: 1.0550, Train: 1.2280, Val: 1.2291\n",
      "Epoch: 423, Loss: 1.0593, Train: 1.1661, Val: 1.1663\n",
      "Epoch: 424, Loss: 1.0601, Train: 1.3663, Val: 1.3652\n",
      "Epoch: 425, Loss: 1.0422, Train: 1.3293, Val: 1.3289\n",
      "Epoch: 426, Loss: 1.0508, Train: 1.2550, Val: 1.2558\n",
      "Epoch: 427, Loss: 1.0316, Train: 1.2608, Val: 1.2618\n",
      "Epoch: 428, Loss: 1.0430, Train: 1.2564, Val: 1.2578\n",
      "Epoch: 429, Loss: 1.0356, Train: 1.1383, Val: 1.1400\n",
      "Epoch: 430, Loss: 1.0205, Train: 1.0666, Val: 1.0687\n",
      "Epoch: 431, Loss: 1.0305, Train: 1.0725, Val: 1.0748\n",
      "Epoch: 432, Loss: 1.0219, Train: 1.0963, Val: 1.0986\n",
      "Epoch: 433, Loss: 1.0163, Train: 1.0991, Val: 1.1011\n",
      "Epoch: 434, Loss: 1.0153, Train: 1.1234, Val: 1.1239\n",
      "Epoch: 435, Loss: 1.0152, Train: 1.1392, Val: 1.1391\n",
      "Epoch: 436, Loss: 1.0138, Train: 1.1044, Val: 1.1048\n",
      "Epoch: 437, Loss: 1.0069, Train: 1.1134, Val: 1.1132\n",
      "Epoch: 438, Loss: 1.0098, Train: 1.0593, Val: 1.0622\n",
      "Epoch: 439, Loss: 1.0065, Train: 1.0321, Val: 1.0360\n",
      "Epoch: 440, Loss: 1.0033, Train: 1.0184, Val: 1.0225\n",
      "Epoch: 441, Loss: 1.0041, Train: 1.0188, Val: 1.0228\n",
      "Epoch: 442, Loss: 1.0018, Train: 1.0238, Val: 1.0275\n",
      "Epoch: 443, Loss: 0.9993, Train: 1.0217, Val: 1.0254\n",
      "Epoch: 444, Loss: 0.9975, Train: 1.0197, Val: 1.0234\n",
      "Epoch: 445, Loss: 0.9970, Train: 1.0179, Val: 1.0220\n",
      "Epoch: 446, Loss: 0.9974, Train: 1.0218, Val: 1.0259\n",
      "Epoch: 447, Loss: 0.9941, Train: 1.0340, Val: 1.0375\n",
      "Epoch: 448, Loss: 0.9935, Train: 1.0584, Val: 1.0601\n",
      "Epoch: 449, Loss: 0.9912, Train: 1.1232, Val: 1.1227\n",
      "Epoch: 450, Loss: 0.9908, Train: 1.1676, Val: 1.1666\n",
      "Epoch: 451, Loss: 0.9905, Train: 1.1631, Val: 1.1627\n",
      "Epoch: 452, Loss: 0.9889, Train: 1.1080, Val: 1.1091\n",
      "Epoch: 453, Loss: 0.9888, Train: 1.0603, Val: 1.0617\n",
      "Epoch: 454, Loss: 0.9882, Train: 1.0250, Val: 1.0287\n",
      "Epoch: 455, Loss: 0.9868, Train: 1.0159, Val: 1.0203\n",
      "Epoch: 456, Loss: 0.9866, Train: 1.0113, Val: 1.0158\n",
      "Epoch: 457, Loss: 0.9923, Train: 1.0700, Val: 1.0720\n",
      "Epoch: 458, Loss: 1.0115, Train: 1.2711, Val: 1.2699\n",
      "Epoch: 459, Loss: 1.0062, Train: 1.2732, Val: 1.2733\n",
      "Epoch: 460, Loss: 1.0015, Train: 1.2769, Val: 1.2771\n",
      "Epoch: 461, Loss: 1.0004, Train: 1.2518, Val: 1.2508\n",
      "Epoch: 462, Loss: 0.9972, Train: 1.2211, Val: 1.2195\n",
      "Epoch: 463, Loss: 0.9968, Train: 1.2218, Val: 1.2199\n",
      "Epoch: 464, Loss: 0.9977, Train: 1.1441, Val: 1.1440\n",
      "Epoch: 465, Loss: 0.9907, Train: 1.1054, Val: 1.1054\n",
      "Epoch: 466, Loss: 0.9969, Train: 1.0424, Val: 1.0446\n",
      "Epoch: 467, Loss: 0.9886, Train: 1.0539, Val: 1.0561\n",
      "Epoch: 468, Loss: 0.9919, Train: 1.0316, Val: 1.0356\n",
      "Epoch: 469, Loss: 0.9897, Train: 1.0083, Val: 1.0132\n",
      "Epoch: 470, Loss: 0.9863, Train: 1.0039, Val: 1.0087\n",
      "Epoch: 471, Loss: 0.9882, Train: 1.0020, Val: 1.0067\n",
      "Epoch: 472, Loss: 0.9852, Train: 1.0065, Val: 1.0108\n",
      "Epoch: 473, Loss: 0.9850, Train: 1.0096, Val: 1.0135\n",
      "Epoch: 474, Loss: 0.9849, Train: 1.0020, Val: 1.0059\n",
      "Epoch: 475, Loss: 0.9849, Train: 1.0057, Val: 1.0099\n",
      "Epoch: 476, Loss: 0.9842, Train: 1.0101, Val: 1.0144\n",
      "Epoch: 477, Loss: 0.9823, Train: 1.0102, Val: 1.0141\n",
      "Epoch: 478, Loss: 0.9837, Train: 1.0113, Val: 1.0151\n",
      "Epoch: 479, Loss: 0.9809, Train: 1.0077, Val: 1.0118\n",
      "Epoch: 480, Loss: 0.9813, Train: 1.0018, Val: 1.0061\n",
      "Epoch: 481, Loss: 0.9806, Train: 1.0075, Val: 1.0119\n",
      "Epoch: 482, Loss: 0.9788, Train: 1.0036, Val: 1.0084\n",
      "Epoch: 483, Loss: 0.9792, Train: 1.0012, Val: 1.0062\n",
      "Epoch: 484, Loss: 0.9782, Train: 1.0002, Val: 1.0051\n",
      "Epoch: 485, Loss: 0.9778, Train: 1.0010, Val: 1.0057\n",
      "Epoch: 486, Loss: 0.9768, Train: 1.0015, Val: 1.0065\n",
      "Epoch: 487, Loss: 0.9767, Train: 1.0017, Val: 1.0068\n",
      "Epoch: 488, Loss: 0.9761, Train: 0.9973, Val: 1.0024\n",
      "Epoch: 489, Loss: 0.9757, Train: 0.9984, Val: 1.0032\n",
      "Epoch: 490, Loss: 0.9752, Train: 1.0003, Val: 1.0048\n",
      "Epoch: 491, Loss: 0.9744, Train: 1.0017, Val: 1.0065\n",
      "Epoch: 492, Loss: 0.9742, Train: 1.0017, Val: 1.0065\n",
      "Epoch: 493, Loss: 0.9736, Train: 1.0001, Val: 1.0047\n",
      "Epoch: 494, Loss: 0.9736, Train: 0.9986, Val: 1.0033\n",
      "Epoch: 495, Loss: 0.9730, Train: 1.0011, Val: 1.0060\n",
      "Epoch: 496, Loss: 0.9729, Train: 1.0076, Val: 1.0121\n",
      "Epoch: 497, Loss: 0.9727, Train: 1.0129, Val: 1.0176\n",
      "Epoch: 498, Loss: 0.9734, Train: 1.0189, Val: 1.0235\n",
      "Epoch: 499, Loss: 0.9753, Train: 1.0171, Val: 1.0220\n",
      "Epoch: 500, Loss: 0.9747, Train: 1.0173, Val: 1.0216\n",
      "Test RMSE: 1.0064\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.524073       3.583673\n",
      "std      1727.484387     741.673176       0.503981       1.116938\n",
      "min         0.000000       0.000000       1.468800       1.000000\n",
      "25%      1500.000000     259.000000       3.214822       3.000000\n",
      "50%      3066.000000     693.000000       3.600408       4.000000\n",
      "75%      4472.000000    1292.000000       3.874269       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1092.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.85740647542357\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  1\n",
      "Kernel:  spd\n",
      "Model Name:  KPGIN\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.6017, Train: 2.8613, Val: 2.8652\n",
      "Epoch: 002, Loss: 10.8572, Train: 2.6235, Val: 2.6248\n",
      "Epoch: 003, Loss: 6.0395, Train: 2.3017, Val: 2.3029\n",
      "Epoch: 004, Loss: 3.5523, Train: 1.8787, Val: 1.8760\n",
      "Epoch: 005, Loss: 4.5161, Train: 1.7640, Val: 1.7598\n",
      "Epoch: 006, Loss: 2.0283, Train: 1.7653, Val: 1.7626\n",
      "Epoch: 007, Loss: 1.6429, Train: 1.7719, Val: 1.7713\n",
      "Epoch: 008, Loss: 1.6964, Train: 1.7180, Val: 1.7171\n",
      "Epoch: 009, Loss: 1.3882, Train: 1.6615, Val: 1.6597\n",
      "Epoch: 010, Loss: 1.3457, Train: 1.6124, Val: 1.6097\n",
      "Epoch: 011, Loss: 1.4118, Train: 1.5862, Val: 1.5835\n",
      "Epoch: 012, Loss: 1.4099, Train: 1.5699, Val: 1.5682\n",
      "Epoch: 013, Loss: 1.3473, Train: 1.5460, Val: 1.5431\n",
      "Epoch: 014, Loss: 1.3294, Train: 1.5106, Val: 1.5067\n",
      "Epoch: 015, Loss: 1.3361, Train: 1.4616, Val: 1.4587\n",
      "Epoch: 016, Loss: 1.3039, Train: 1.4098, Val: 1.4079\n",
      "Epoch: 017, Loss: 1.2960, Train: 1.3706, Val: 1.3693\n",
      "Epoch: 018, Loss: 1.2928, Train: 1.3513, Val: 1.3506\n",
      "Epoch: 019, Loss: 1.2707, Train: 1.3348, Val: 1.3335\n",
      "Epoch: 020, Loss: 1.2700, Train: 1.3160, Val: 1.3127\n",
      "Epoch: 021, Loss: 1.2520, Train: 1.3277, Val: 1.3285\n",
      "Epoch: 022, Loss: 1.2479, Train: 1.2702, Val: 1.2705\n",
      "Epoch: 023, Loss: 1.2323, Train: 1.2746, Val: 1.2752\n",
      "Epoch: 024, Loss: 1.2348, Train: 1.2702, Val: 1.2711\n",
      "Epoch: 025, Loss: 1.2238, Train: 1.2629, Val: 1.2640\n",
      "Epoch: 026, Loss: 1.2236, Train: 1.2580, Val: 1.2593\n",
      "Epoch: 027, Loss: 1.2219, Train: 1.2560, Val: 1.2574\n",
      "Epoch: 028, Loss: 1.2183, Train: 1.2539, Val: 1.2554\n",
      "Epoch: 029, Loss: 1.2128, Train: 1.2970, Val: 1.2937\n",
      "Epoch: 030, Loss: 1.2095, Train: 1.2384, Val: 1.2397\n",
      "Epoch: 031, Loss: 1.2087, Train: 1.2241, Val: 1.2253\n",
      "Epoch: 032, Loss: 1.2070, Train: 1.2144, Val: 1.2156\n",
      "Epoch: 033, Loss: 1.2068, Train: 1.2157, Val: 1.2169\n",
      "Epoch: 034, Loss: 1.2047, Train: 1.2180, Val: 1.2194\n",
      "Epoch: 035, Loss: 1.2045, Train: 1.2098, Val: 1.2112\n",
      "Epoch: 036, Loss: 1.2023, Train: 1.1980, Val: 1.1993\n",
      "Epoch: 037, Loss: 1.2010, Train: 1.1917, Val: 1.1930\n",
      "Epoch: 038, Loss: 1.1996, Train: 1.1887, Val: 1.1901\n",
      "Epoch: 039, Loss: 1.1991, Train: 1.1811, Val: 1.1824\n",
      "Epoch: 040, Loss: 1.1987, Train: 1.1702, Val: 1.1715\n",
      "Epoch: 041, Loss: 1.1979, Train: 1.1632, Val: 1.1644\n",
      "Epoch: 042, Loss: 1.1970, Train: 1.1620, Val: 1.1633\n",
      "Epoch: 043, Loss: 1.1954, Train: 1.1604, Val: 1.1616\n",
      "Epoch: 044, Loss: 1.1944, Train: 1.1521, Val: 1.1533\n",
      "Epoch: 045, Loss: 1.1931, Train: 1.1431, Val: 1.1441\n",
      "Epoch: 046, Loss: 1.1923, Train: 1.1379, Val: 1.1389\n",
      "Epoch: 047, Loss: 1.1911, Train: 1.1323, Val: 1.1332\n",
      "Epoch: 048, Loss: 1.1898, Train: 1.1258, Val: 1.1268\n",
      "Epoch: 049, Loss: 1.1884, Train: 1.1207, Val: 1.1216\n",
      "Epoch: 050, Loss: 1.1876, Train: 1.1174, Val: 1.1182\n",
      "Epoch: 051, Loss: 1.1868, Train: 1.1149, Val: 1.1157\n",
      "Epoch: 052, Loss: 1.1856, Train: 1.1117, Val: 1.1125\n",
      "Epoch: 053, Loss: 1.1845, Train: 1.1080, Val: 1.1087\n",
      "Epoch: 054, Loss: 1.1835, Train: 1.1067, Val: 1.1073\n",
      "Epoch: 055, Loss: 1.1824, Train: 1.1073, Val: 1.1081\n",
      "Epoch: 056, Loss: 1.1815, Train: 1.1055, Val: 1.1062\n",
      "Epoch: 057, Loss: 1.1805, Train: 1.1025, Val: 1.1031\n",
      "Epoch: 058, Loss: 1.1793, Train: 1.1012, Val: 1.1018\n",
      "Epoch: 059, Loss: 1.1783, Train: 1.0992, Val: 1.0997\n",
      "Epoch: 060, Loss: 1.1773, Train: 1.0976, Val: 1.0981\n",
      "Epoch: 061, Loss: 1.1763, Train: 1.0978, Val: 1.0984\n",
      "Epoch: 062, Loss: 1.1754, Train: 1.0956, Val: 1.0961\n",
      "Epoch: 063, Loss: 1.1745, Train: 1.0948, Val: 1.0952\n",
      "Epoch: 064, Loss: 1.1731, Train: 1.0939, Val: 1.0943\n",
      "Epoch: 065, Loss: 1.1720, Train: 1.0934, Val: 1.0938\n",
      "Epoch: 066, Loss: 1.1710, Train: 1.0928, Val: 1.0932\n",
      "Epoch: 067, Loss: 1.1702, Train: 1.0918, Val: 1.0922\n",
      "Epoch: 068, Loss: 1.1691, Train: 1.0912, Val: 1.0915\n",
      "Epoch: 069, Loss: 1.1681, Train: 1.0904, Val: 1.0906\n",
      "Epoch: 070, Loss: 1.1671, Train: 1.0906, Val: 1.0909\n",
      "Epoch: 071, Loss: 1.1663, Train: 1.0892, Val: 1.0895\n",
      "Epoch: 072, Loss: 1.1662, Train: 1.0922, Val: 1.0927\n",
      "Epoch: 073, Loss: 1.1647, Train: 1.0997, Val: 1.1001\n",
      "Epoch: 074, Loss: 1.1824, Train: 1.1165, Val: 1.1174\n",
      "Epoch: 075, Loss: 1.1745, Train: 1.1212, Val: 1.1221\n",
      "Epoch: 076, Loss: 1.1638, Train: 1.1219, Val: 1.1228\n",
      "Epoch: 077, Loss: 1.1692, Train: 1.1290, Val: 1.1299\n",
      "Epoch: 078, Loss: 1.1643, Train: 1.1211, Val: 1.1220\n",
      "Epoch: 079, Loss: 1.1630, Train: 1.0909, Val: 1.0913\n",
      "Epoch: 080, Loss: 1.1643, Train: 1.0861, Val: 1.0862\n",
      "Epoch: 081, Loss: 1.1625, Train: 1.0996, Val: 1.1001\n",
      "Epoch: 082, Loss: 1.1582, Train: 1.1372, Val: 1.1382\n",
      "Epoch: 083, Loss: 1.1624, Train: 1.1032, Val: 1.1038\n",
      "Epoch: 084, Loss: 1.1574, Train: 1.0998, Val: 1.1005\n",
      "Epoch: 085, Loss: 1.1564, Train: 1.1276, Val: 1.1286\n",
      "Epoch: 086, Loss: 1.1569, Train: 1.1318, Val: 1.1328\n",
      "Epoch: 087, Loss: 1.1541, Train: 1.1067, Val: 1.1074\n",
      "Epoch: 088, Loss: 1.1544, Train: 1.1146, Val: 1.1154\n",
      "Epoch: 089, Loss: 1.1532, Train: 1.1352, Val: 1.1361\n",
      "Epoch: 090, Loss: 1.1523, Train: 1.0909, Val: 1.0914\n",
      "Epoch: 091, Loss: 1.1534, Train: 1.1050, Val: 1.1058\n",
      "Epoch: 092, Loss: 1.1513, Train: 1.1366, Val: 1.1376\n",
      "Epoch: 093, Loss: 1.1517, Train: 1.0804, Val: 1.0802\n",
      "Epoch: 094, Loss: 1.1523, Train: 1.1092, Val: 1.1087\n",
      "Epoch: 095, Loss: 1.1535, Train: 1.1282, Val: 1.1277\n",
      "Epoch: 096, Loss: 1.1530, Train: 1.1698, Val: 1.1694\n",
      "Epoch: 097, Loss: 1.1497, Train: 1.1108, Val: 1.1102\n",
      "Epoch: 098, Loss: 1.1487, Train: 1.0854, Val: 1.0850\n",
      "Epoch: 099, Loss: 1.1461, Train: 1.1306, Val: 1.1310\n",
      "Epoch: 100, Loss: 1.1453, Train: 1.1320, Val: 1.1329\n",
      "Epoch: 101, Loss: 1.1466, Train: 1.1373, Val: 1.1383\n",
      "Epoch: 102, Loss: 1.1446, Train: 1.0813, Val: 1.0815\n",
      "Epoch: 103, Loss: 1.1434, Train: 1.1056, Val: 1.1050\n",
      "Epoch: 104, Loss: 1.1425, Train: 1.1237, Val: 1.1230\n",
      "Epoch: 105, Loss: 1.1413, Train: 1.0876, Val: 1.0874\n",
      "Epoch: 106, Loss: 1.1430, Train: 1.1161, Val: 1.1153\n",
      "Epoch: 107, Loss: 1.1431, Train: 1.0859, Val: 1.0855\n",
      "Epoch: 108, Loss: 1.1387, Train: 1.0813, Val: 1.0815\n",
      "Epoch: 109, Loss: 1.1403, Train: 1.0840, Val: 1.0837\n",
      "Epoch: 110, Loss: 1.1399, Train: 1.0816, Val: 1.0813\n",
      "Epoch: 111, Loss: 1.1373, Train: 1.0761, Val: 1.0759\n",
      "Epoch: 112, Loss: 1.1364, Train: 1.1092, Val: 1.1086\n",
      "Epoch: 113, Loss: 1.1348, Train: 1.1369, Val: 1.1360\n",
      "Epoch: 114, Loss: 1.1350, Train: 1.1072, Val: 1.1064\n",
      "Epoch: 115, Loss: 1.1338, Train: 1.0963, Val: 1.0956\n",
      "Epoch: 116, Loss: 1.1323, Train: 1.0775, Val: 1.0777\n",
      "Epoch: 117, Loss: 1.1317, Train: 1.0780, Val: 1.0783\n",
      "Epoch: 118, Loss: 1.1298, Train: 1.0687, Val: 1.0688\n",
      "Epoch: 119, Loss: 1.1298, Train: 1.0711, Val: 1.0715\n",
      "Epoch: 120, Loss: 1.1296, Train: 1.0862, Val: 1.0854\n",
      "Epoch: 121, Loss: 1.1323, Train: 1.1456, Val: 1.1461\n",
      "Epoch: 122, Loss: 1.1346, Train: 1.1240, Val: 1.1241\n",
      "Epoch: 123, Loss: 1.1325, Train: 1.1547, Val: 1.1560\n",
      "Epoch: 124, Loss: 1.1272, Train: 1.2166, Val: 1.2186\n",
      "Epoch: 125, Loss: 1.1356, Train: 1.1039, Val: 1.1042\n",
      "Epoch: 126, Loss: 1.1532, Train: 1.3153, Val: 1.3137\n",
      "Epoch: 127, Loss: 1.1413, Train: 1.1989, Val: 1.1978\n",
      "Epoch: 128, Loss: 1.1577, Train: 1.5708, Val: 1.5691\n",
      "Epoch: 129, Loss: 1.1654, Train: 1.1982, Val: 1.1971\n",
      "Epoch: 130, Loss: 1.1413, Train: 1.1172, Val: 1.1179\n",
      "Epoch: 131, Loss: 1.1396, Train: 1.1383, Val: 1.1388\n",
      "Epoch: 132, Loss: 1.1370, Train: 1.1466, Val: 1.1469\n",
      "Epoch: 133, Loss: 1.1400, Train: 1.1544, Val: 1.1547\n",
      "Epoch: 134, Loss: 1.1356, Train: 1.1472, Val: 1.1476\n",
      "Epoch: 135, Loss: 1.1375, Train: 1.1025, Val: 1.1022\n",
      "Epoch: 136, Loss: 1.1295, Train: 1.0907, Val: 1.0902\n",
      "Epoch: 137, Loss: 1.1314, Train: 1.1005, Val: 1.1006\n",
      "Epoch: 138, Loss: 1.1310, Train: 1.0909, Val: 1.0907\n",
      "Epoch: 139, Loss: 1.1280, Train: 1.0905, Val: 1.0907\n",
      "Epoch: 140, Loss: 1.1283, Train: 1.1273, Val: 1.1274\n",
      "Epoch: 141, Loss: 1.1268, Train: 1.1492, Val: 1.1492\n",
      "Epoch: 142, Loss: 1.1233, Train: 1.1214, Val: 1.1214\n",
      "Epoch: 143, Loss: 1.1232, Train: 1.1073, Val: 1.1078\n",
      "Epoch: 144, Loss: 1.1236, Train: 1.0880, Val: 1.0881\n",
      "Epoch: 145, Loss: 1.1222, Train: 1.0903, Val: 1.0905\n",
      "Epoch: 146, Loss: 1.1208, Train: 1.0955, Val: 1.0959\n",
      "Epoch: 147, Loss: 1.1194, Train: 1.1423, Val: 1.1421\n",
      "Epoch: 148, Loss: 1.1186, Train: 1.1118, Val: 1.1117\n",
      "Epoch: 149, Loss: 1.1173, Train: 1.0872, Val: 1.0875\n",
      "Epoch: 150, Loss: 1.1166, Train: 1.0759, Val: 1.0761\n",
      "Epoch: 151, Loss: 1.1152, Train: 1.0744, Val: 1.0746\n",
      "Epoch: 152, Loss: 1.1158, Train: 1.1000, Val: 1.1007\n",
      "Epoch: 153, Loss: 1.1154, Train: 1.0801, Val: 1.0803\n",
      "Epoch: 154, Loss: 1.1161, Train: 1.0652, Val: 1.0653\n",
      "Epoch: 155, Loss: 1.1171, Train: 1.0635, Val: 1.0634\n",
      "Epoch: 156, Loss: 1.1134, Train: 1.0701, Val: 1.0705\n",
      "Epoch: 157, Loss: 1.1099, Train: 1.0682, Val: 1.0685\n",
      "Epoch: 158, Loss: 1.1086, Train: 1.0550, Val: 1.0555\n",
      "Epoch: 159, Loss: 1.1096, Train: 1.0816, Val: 1.0818\n",
      "Epoch: 160, Loss: 1.1103, Train: 1.0815, Val: 1.0809\n",
      "Epoch: 161, Loss: 1.1127, Train: 1.0829, Val: 1.0823\n",
      "Epoch: 162, Loss: 1.1091, Train: 1.0611, Val: 1.0617\n",
      "Epoch: 163, Loss: 1.1069, Train: 1.0783, Val: 1.0779\n",
      "Epoch: 164, Loss: 1.1108, Train: 1.0749, Val: 1.0750\n",
      "Epoch: 165, Loss: 1.1130, Train: 1.0571, Val: 1.0578\n",
      "Epoch: 166, Loss: 1.1050, Train: 1.0774, Val: 1.0773\n",
      "Epoch: 167, Loss: 1.1106, Train: 1.0728, Val: 1.0729\n",
      "Epoch: 168, Loss: 1.1123, Train: 1.0534, Val: 1.0539\n",
      "Epoch: 169, Loss: 1.1037, Train: 1.1008, Val: 1.1004\n",
      "Epoch: 170, Loss: 1.1134, Train: 1.0681, Val: 1.0682\n",
      "Epoch: 171, Loss: 1.1054, Train: 1.1064, Val: 1.1059\n",
      "Epoch: 172, Loss: 1.1103, Train: 1.0705, Val: 1.0717\n",
      "Epoch: 173, Loss: 1.1059, Train: 1.0549, Val: 1.0555\n",
      "Epoch: 174, Loss: 1.1024, Train: 1.0762, Val: 1.0758\n",
      "Epoch: 175, Loss: 1.1064, Train: 1.0744, Val: 1.0755\n",
      "Epoch: 176, Loss: 1.0988, Train: 1.0946, Val: 1.0948\n",
      "Epoch: 177, Loss: 1.1085, Train: 1.0857, Val: 1.0858\n",
      "Epoch: 178, Loss: 1.1075, Train: 1.0721, Val: 1.0719\n",
      "Epoch: 179, Loss: 1.1012, Train: 1.0735, Val: 1.0744\n",
      "Epoch: 180, Loss: 1.1049, Train: 1.0981, Val: 1.0993\n",
      "Epoch: 181, Loss: 1.0984, Train: 1.1268, Val: 1.1263\n",
      "Epoch: 182, Loss: 1.1104, Train: 1.0793, Val: 1.0786\n",
      "Epoch: 183, Loss: 1.1012, Train: 1.1191, Val: 1.1204\n",
      "Epoch: 184, Loss: 1.1012, Train: 1.0815, Val: 1.0814\n",
      "Epoch: 185, Loss: 1.1073, Train: 1.0616, Val: 1.0621\n",
      "Epoch: 186, Loss: 1.0944, Train: 1.0570, Val: 1.0580\n",
      "Epoch: 187, Loss: 1.1028, Train: 1.0877, Val: 1.0868\n",
      "Epoch: 188, Loss: 1.0973, Train: 1.0642, Val: 1.0651\n",
      "Epoch: 189, Loss: 1.1021, Train: 1.0532, Val: 1.0540\n",
      "Epoch: 190, Loss: 1.0959, Train: 1.0615, Val: 1.0608\n",
      "Epoch: 191, Loss: 1.0925, Train: 1.0761, Val: 1.0762\n",
      "Epoch: 192, Loss: 1.0990, Train: 1.0474, Val: 1.0486\n",
      "Epoch: 193, Loss: 1.0934, Train: 1.0638, Val: 1.0650\n",
      "Epoch: 194, Loss: 1.0945, Train: 1.0684, Val: 1.0684\n",
      "Epoch: 195, Loss: 1.0921, Train: 1.0757, Val: 1.0746\n",
      "Epoch: 196, Loss: 1.0912, Train: 1.1302, Val: 1.1317\n",
      "Epoch: 197, Loss: 1.0968, Train: 1.1454, Val: 1.1450\n",
      "Epoch: 198, Loss: 1.0922, Train: 1.1886, Val: 1.1877\n",
      "Epoch: 199, Loss: 1.0974, Train: 1.1156, Val: 1.1149\n",
      "Epoch: 200, Loss: 1.0918, Train: 1.0961, Val: 1.0959\n",
      "Epoch: 201, Loss: 1.0903, Train: 1.1333, Val: 1.1323\n",
      "Epoch: 202, Loss: 1.0885, Train: 1.1061, Val: 1.1046\n",
      "Epoch: 203, Loss: 1.0904, Train: 1.1001, Val: 1.0991\n",
      "Epoch: 204, Loss: 1.0859, Train: 1.0916, Val: 1.0901\n",
      "Epoch: 205, Loss: 1.0834, Train: 1.0434, Val: 1.0438\n",
      "Epoch: 206, Loss: 1.0859, Train: 1.0565, Val: 1.0575\n",
      "Epoch: 207, Loss: 1.0832, Train: 1.0971, Val: 1.0954\n",
      "Epoch: 208, Loss: 1.0861, Train: 1.0562, Val: 1.0568\n",
      "Epoch: 209, Loss: 1.0919, Train: 1.1478, Val: 1.1454\n",
      "Epoch: 210, Loss: 1.1015, Train: 1.0551, Val: 1.0557\n",
      "Epoch: 211, Loss: 1.0858, Train: 1.0729, Val: 1.0740\n",
      "Epoch: 212, Loss: 1.0883, Train: 1.1349, Val: 1.1335\n",
      "Epoch: 213, Loss: 1.0928, Train: 1.1186, Val: 1.1170\n",
      "Epoch: 214, Loss: 1.0873, Train: 1.0449, Val: 1.0455\n",
      "Epoch: 215, Loss: 1.0884, Train: 1.1539, Val: 1.1523\n",
      "Epoch: 216, Loss: 1.0875, Train: 1.0864, Val: 1.0852\n",
      "Epoch: 217, Loss: 1.0919, Train: 1.0692, Val: 1.0688\n",
      "Epoch: 218, Loss: 1.0785, Train: 1.1744, Val: 1.1734\n",
      "Epoch: 219, Loss: 1.0966, Train: 1.0579, Val: 1.0574\n",
      "Epoch: 220, Loss: 1.0817, Train: 1.0871, Val: 1.0863\n",
      "Epoch: 221, Loss: 1.0849, Train: 1.1905, Val: 1.1901\n",
      "Epoch: 222, Loss: 1.0807, Train: 1.1531, Val: 1.1527\n",
      "Epoch: 223, Loss: 1.0778, Train: 1.0633, Val: 1.0630\n",
      "Epoch: 224, Loss: 1.0791, Train: 1.0730, Val: 1.0724\n",
      "Epoch: 225, Loss: 1.0737, Train: 1.1070, Val: 1.1078\n",
      "Epoch: 226, Loss: 1.0786, Train: 1.0557, Val: 1.0565\n",
      "Epoch: 227, Loss: 1.0724, Train: 1.0846, Val: 1.0837\n",
      "Epoch: 228, Loss: 1.0751, Train: 1.0791, Val: 1.0788\n",
      "Epoch: 229, Loss: 1.0692, Train: 1.0619, Val: 1.0630\n",
      "Epoch: 230, Loss: 1.0735, Train: 1.0889, Val: 1.0874\n",
      "Epoch: 231, Loss: 1.0757, Train: 1.0379, Val: 1.0388\n",
      "Epoch: 232, Loss: 1.0675, Train: 1.0601, Val: 1.0612\n",
      "Epoch: 233, Loss: 1.0753, Train: 1.0789, Val: 1.0788\n",
      "Epoch: 234, Loss: 1.0689, Train: 1.0519, Val: 1.0533\n",
      "Epoch: 235, Loss: 1.0688, Train: 1.1045, Val: 1.1064\n",
      "Epoch: 236, Loss: 1.0714, Train: 1.0481, Val: 1.0489\n",
      "Epoch: 237, Loss: 1.0652, Train: 1.0533, Val: 1.0537\n",
      "Epoch: 238, Loss: 1.0705, Train: 1.0408, Val: 1.0421\n",
      "Epoch: 239, Loss: 1.0756, Train: 1.0801, Val: 1.0819\n",
      "Epoch: 240, Loss: 1.0729, Train: 1.0760, Val: 1.0765\n",
      "Epoch: 241, Loss: 1.0720, Train: 1.0376, Val: 1.0391\n",
      "Epoch: 242, Loss: 1.0615, Train: 1.1265, Val: 1.1276\n",
      "Epoch: 243, Loss: 1.0654, Train: 1.0415, Val: 1.0433\n",
      "Epoch: 244, Loss: 1.0688, Train: 1.1507, Val: 1.1511\n",
      "Epoch: 245, Loss: 1.0639, Train: 1.1171, Val: 1.1189\n",
      "Epoch: 246, Loss: 1.0663, Train: 1.1259, Val: 1.1254\n",
      "Epoch: 247, Loss: 1.0708, Train: 1.0893, Val: 1.0904\n",
      "Epoch: 248, Loss: 1.0662, Train: 1.1245, Val: 1.1250\n",
      "Epoch: 249, Loss: 1.0793, Train: 1.1434, Val: 1.1446\n",
      "Epoch: 250, Loss: 1.0851, Train: 1.0919, Val: 1.0916\n",
      "Epoch: 251, Loss: 1.0765, Train: 1.1477, Val: 1.1473\n",
      "Epoch: 252, Loss: 1.0753, Train: 1.0807, Val: 1.0813\n",
      "Epoch: 253, Loss: 1.0781, Train: 1.2153, Val: 1.2142\n",
      "Epoch: 254, Loss: 1.0859, Train: 1.1588, Val: 1.1585\n",
      "Epoch: 255, Loss: 1.0684, Train: 1.0792, Val: 1.0811\n",
      "Epoch: 256, Loss: 1.0782, Train: 1.1257, Val: 1.1246\n",
      "Epoch: 257, Loss: 1.0789, Train: 1.0891, Val: 1.0890\n",
      "Epoch: 258, Loss: 1.0697, Train: 1.0835, Val: 1.0845\n",
      "Epoch: 259, Loss: 1.0766, Train: 1.1131, Val: 1.1134\n",
      "Epoch: 260, Loss: 1.0634, Train: 1.0692, Val: 1.0698\n",
      "Epoch: 261, Loss: 1.0709, Train: 1.0950, Val: 1.0960\n",
      "Epoch: 262, Loss: 1.0630, Train: 1.1366, Val: 1.1372\n",
      "Epoch: 263, Loss: 1.0675, Train: 1.0472, Val: 1.0489\n",
      "Epoch: 264, Loss: 1.0597, Train: 1.0853, Val: 1.0858\n",
      "Epoch: 265, Loss: 1.0608, Train: 1.1301, Val: 1.1309\n",
      "Epoch: 266, Loss: 1.0576, Train: 1.0958, Val: 1.0972\n",
      "Epoch: 267, Loss: 1.0615, Train: 1.1314, Val: 1.1312\n",
      "Epoch: 268, Loss: 1.0580, Train: 1.0877, Val: 1.0884\n",
      "Epoch: 269, Loss: 1.0542, Train: 1.1221, Val: 1.1231\n",
      "Epoch: 270, Loss: 1.0538, Train: 1.1143, Val: 1.1151\n",
      "Epoch: 271, Loss: 1.0560, Train: 1.0474, Val: 1.0496\n",
      "Epoch: 272, Loss: 1.0558, Train: 1.0851, Val: 1.0866\n",
      "Epoch: 273, Loss: 1.0484, Train: 1.1370, Val: 1.1377\n",
      "Epoch: 274, Loss: 1.0530, Train: 1.0478, Val: 1.0496\n",
      "Epoch: 275, Loss: 1.0558, Train: 1.1025, Val: 1.1031\n",
      "Epoch: 276, Loss: 1.0524, Train: 1.0981, Val: 1.0998\n",
      "Epoch: 277, Loss: 1.0518, Train: 1.1563, Val: 1.1548\n",
      "Epoch: 278, Loss: 1.0613, Train: 1.0868, Val: 1.0879\n",
      "Epoch: 279, Loss: 1.0803, Train: 1.0588, Val: 1.0593\n",
      "Epoch: 280, Loss: 1.0757, Train: 1.0364, Val: 1.0381\n",
      "Epoch: 281, Loss: 1.0635, Train: 1.0786, Val: 1.0786\n",
      "Epoch: 282, Loss: 1.0628, Train: 1.1254, Val: 1.1258\n",
      "Epoch: 283, Loss: 1.0646, Train: 1.0590, Val: 1.0605\n",
      "Epoch: 284, Loss: 1.0565, Train: 1.1201, Val: 1.1182\n",
      "Epoch: 285, Loss: 1.0705, Train: 1.0575, Val: 1.0592\n",
      "Epoch: 286, Loss: 1.0531, Train: 1.0814, Val: 1.0828\n",
      "Epoch: 287, Loss: 1.0603, Train: 1.0627, Val: 1.0628\n",
      "Epoch: 288, Loss: 1.0609, Train: 1.0373, Val: 1.0386\n",
      "Epoch: 289, Loss: 1.0507, Train: 1.0328, Val: 1.0352\n",
      "Epoch: 290, Loss: 1.0548, Train: 1.0717, Val: 1.0716\n",
      "Epoch: 291, Loss: 1.0543, Train: 1.0718, Val: 1.0722\n",
      "Epoch: 292, Loss: 1.0567, Train: 1.0400, Val: 1.0413\n",
      "Epoch: 293, Loss: 1.0542, Train: 1.0313, Val: 1.0330\n",
      "Epoch: 294, Loss: 1.0456, Train: 1.0359, Val: 1.0379\n",
      "Epoch: 295, Loss: 1.0503, Train: 1.0501, Val: 1.0517\n",
      "Epoch: 296, Loss: 1.0464, Train: 1.0319, Val: 1.0339\n",
      "Epoch: 297, Loss: 1.0499, Train: 1.0577, Val: 1.0590\n",
      "Epoch: 298, Loss: 1.0444, Train: 1.0368, Val: 1.0388\n",
      "Epoch: 299, Loss: 1.0438, Train: 1.0353, Val: 1.0375\n",
      "Epoch: 300, Loss: 1.0486, Train: 1.1125, Val: 1.1138\n",
      "Epoch: 301, Loss: 1.0474, Train: 1.0418, Val: 1.0454\n",
      "Epoch: 302, Loss: 1.0507, Train: 1.0398, Val: 1.0421\n",
      "Epoch: 303, Loss: 1.0461, Train: 1.0399, Val: 1.0423\n",
      "Epoch: 304, Loss: 1.0431, Train: 1.0264, Val: 1.0296\n",
      "Epoch: 305, Loss: 1.0439, Train: 1.0762, Val: 1.0788\n",
      "Epoch: 306, Loss: 1.0466, Train: 1.0363, Val: 1.0388\n",
      "Epoch: 307, Loss: 1.0400, Train: 1.0486, Val: 1.0505\n",
      "Epoch: 308, Loss: 1.0441, Train: 1.0750, Val: 1.0772\n",
      "Epoch: 309, Loss: 1.0395, Train: 1.0661, Val: 1.0684\n",
      "Epoch: 310, Loss: 1.0452, Train: 1.1221, Val: 1.1222\n",
      "Epoch: 311, Loss: 1.0414, Train: 1.2573, Val: 1.2538\n",
      "Epoch: 312, Loss: 1.0405, Train: 1.0507, Val: 1.0536\n",
      "Epoch: 313, Loss: 1.0406, Train: 1.0372, Val: 1.0409\n",
      "Epoch: 314, Loss: 1.0348, Train: 1.0412, Val: 1.0433\n",
      "Epoch: 315, Loss: 1.0380, Train: 1.0337, Val: 1.0365\n",
      "Epoch: 316, Loss: 1.0425, Train: 1.0570, Val: 1.0575\n",
      "Epoch: 317, Loss: 1.0427, Train: 1.0726, Val: 1.0752\n",
      "Epoch: 318, Loss: 1.0674, Train: 1.1198, Val: 1.1178\n",
      "Epoch: 319, Loss: 1.0751, Train: 1.5537, Val: 1.5495\n",
      "Epoch: 320, Loss: 1.0653, Train: 1.1849, Val: 1.1831\n",
      "Epoch: 321, Loss: 1.0736, Train: 1.1222, Val: 1.1215\n",
      "Epoch: 322, Loss: 1.1132, Train: 1.2411, Val: 1.2368\n",
      "Epoch: 323, Loss: 1.0747, Train: 1.3437, Val: 1.3404\n",
      "Epoch: 324, Loss: 1.0778, Train: 1.2708, Val: 1.2665\n",
      "Epoch: 325, Loss: 1.0718, Train: 1.2005, Val: 1.1969\n",
      "Epoch: 326, Loss: 1.0658, Train: 1.1948, Val: 1.1909\n",
      "Epoch: 327, Loss: 1.0715, Train: 1.1347, Val: 1.1328\n",
      "Epoch: 328, Loss: 1.0693, Train: 1.1601, Val: 1.1572\n",
      "Epoch: 329, Loss: 1.0529, Train: 1.1659, Val: 1.1628\n",
      "Epoch: 330, Loss: 1.0744, Train: 1.2019, Val: 1.2019\n",
      "Epoch: 331, Loss: 1.1196, Train: 1.2308, Val: 1.2294\n",
      "Epoch: 332, Loss: 1.0752, Train: 1.3033, Val: 1.3021\n",
      "Epoch: 333, Loss: 1.0900, Train: 1.6074, Val: 1.6042\n",
      "Epoch: 334, Loss: 1.1051, Train: 1.2742, Val: 1.2732\n",
      "Epoch: 335, Loss: 1.1389, Train: 1.2789, Val: 1.2773\n",
      "Epoch: 336, Loss: 1.0969, Train: 1.2157, Val: 1.2138\n",
      "Epoch: 337, Loss: 1.0913, Train: 1.2009, Val: 1.1991\n",
      "Epoch: 338, Loss: 1.0891, Train: 1.2597, Val: 1.2587\n",
      "Epoch: 339, Loss: 1.0669, Train: 1.2449, Val: 1.2442\n",
      "Epoch: 340, Loss: 1.0827, Train: 1.1669, Val: 1.1666\n",
      "Epoch: 341, Loss: 1.0719, Train: 1.1091, Val: 1.1088\n",
      "Epoch: 342, Loss: 1.0735, Train: 1.0962, Val: 1.0967\n",
      "Epoch: 343, Loss: 1.0731, Train: 1.1047, Val: 1.1052\n",
      "Epoch: 344, Loss: 1.0602, Train: 1.1120, Val: 1.1121\n",
      "Epoch: 345, Loss: 1.0669, Train: 1.0812, Val: 1.0818\n",
      "Epoch: 346, Loss: 1.0606, Train: 1.0754, Val: 1.0760\n",
      "Epoch: 347, Loss: 1.0542, Train: 1.0928, Val: 1.0932\n",
      "Epoch: 348, Loss: 1.0593, Train: 1.0790, Val: 1.0797\n",
      "Epoch: 349, Loss: 1.0495, Train: 1.0860, Val: 1.0871\n",
      "Epoch: 350, Loss: 1.0531, Train: 1.0872, Val: 1.0884\n",
      "Epoch: 351, Loss: 1.0498, Train: 1.0859, Val: 1.0873\n",
      "Epoch: 352, Loss: 1.0473, Train: 1.0830, Val: 1.0847\n",
      "Epoch: 353, Loss: 1.0467, Train: 1.0533, Val: 1.0560\n",
      "Epoch: 354, Loss: 1.0418, Train: 1.0489, Val: 1.0516\n",
      "Epoch: 355, Loss: 1.0439, Train: 1.0540, Val: 1.0563\n",
      "Epoch: 356, Loss: 1.0393, Train: 1.0917, Val: 1.0930\n",
      "Epoch: 357, Loss: 1.0453, Train: 1.0715, Val: 1.0737\n",
      "Epoch: 358, Loss: 1.0451, Train: 1.0345, Val: 1.0363\n",
      "Epoch: 359, Loss: 1.0413, Train: 1.0609, Val: 1.0627\n",
      "Epoch: 360, Loss: 1.0379, Train: 1.0633, Val: 1.0655\n",
      "Epoch: 361, Loss: 1.0387, Train: 1.0722, Val: 1.0744\n",
      "Epoch: 362, Loss: 1.0350, Train: 1.0416, Val: 1.0445\n",
      "Epoch: 363, Loss: 1.0351, Train: 1.0377, Val: 1.0403\n",
      "Epoch: 364, Loss: 1.0327, Train: 1.0985, Val: 1.1000\n",
      "Epoch: 365, Loss: 1.0365, Train: 1.0349, Val: 1.0377\n",
      "Epoch: 366, Loss: 1.0330, Train: 1.0425, Val: 1.0446\n",
      "Epoch: 367, Loss: 1.0337, Train: 1.0836, Val: 1.0857\n",
      "Epoch: 368, Loss: 1.0305, Train: 1.0726, Val: 1.0750\n",
      "Epoch: 369, Loss: 1.0361, Train: 1.0666, Val: 1.0689\n",
      "Epoch: 370, Loss: 1.0328, Train: 1.0314, Val: 1.0345\n",
      "Epoch: 371, Loss: 1.0282, Train: 1.0878, Val: 1.0897\n",
      "Epoch: 372, Loss: 1.0279, Train: 1.0441, Val: 1.0469\n",
      "Epoch: 373, Loss: 1.0252, Train: 1.0552, Val: 1.0579\n",
      "Epoch: 374, Loss: 1.0257, Train: 1.0840, Val: 1.0859\n",
      "Epoch: 375, Loss: 1.0232, Train: 1.0430, Val: 1.0457\n",
      "Epoch: 376, Loss: 1.0233, Train: 1.0707, Val: 1.0735\n",
      "Epoch: 377, Loss: 1.0255, Train: 1.0349, Val: 1.0380\n",
      "Epoch: 378, Loss: 1.0266, Train: 1.1254, Val: 1.1271\n",
      "Epoch: 379, Loss: 1.0338, Train: 1.0428, Val: 1.0450\n",
      "Epoch: 380, Loss: 1.0381, Train: 1.0847, Val: 1.0862\n",
      "Epoch: 381, Loss: 1.0301, Train: 1.1125, Val: 1.1146\n",
      "Epoch: 382, Loss: 1.0272, Train: 1.0567, Val: 1.0587\n",
      "Epoch: 383, Loss: 1.0218, Train: 1.1045, Val: 1.1047\n",
      "Epoch: 384, Loss: 1.0292, Train: 1.0687, Val: 1.0713\n",
      "Epoch: 385, Loss: 1.0331, Train: 1.1245, Val: 1.1256\n",
      "Epoch: 386, Loss: 1.0282, Train: 1.0722, Val: 1.0730\n",
      "Epoch: 387, Loss: 1.0215, Train: 1.0943, Val: 1.0950\n",
      "Epoch: 388, Loss: 1.0192, Train: 1.0972, Val: 1.0993\n",
      "Epoch: 389, Loss: 1.0243, Train: 1.0645, Val: 1.0665\n",
      "Epoch: 390, Loss: 1.0243, Train: 1.1305, Val: 1.1295\n",
      "Epoch: 391, Loss: 1.0230, Train: 1.0680, Val: 1.0697\n",
      "Epoch: 392, Loss: 1.0199, Train: 1.0963, Val: 1.0979\n",
      "Epoch: 393, Loss: 1.0166, Train: 1.1038, Val: 1.1045\n",
      "Epoch: 394, Loss: 1.0203, Train: 1.0341, Val: 1.0367\n",
      "Epoch: 395, Loss: 1.0204, Train: 1.1349, Val: 1.1356\n",
      "Epoch: 396, Loss: 1.0246, Train: 1.0431, Val: 1.0455\n",
      "Epoch: 397, Loss: 1.0184, Train: 1.0701, Val: 1.0716\n",
      "Epoch: 398, Loss: 1.0158, Train: 1.1183, Val: 1.1194\n",
      "Epoch: 399, Loss: 1.0201, Train: 1.0438, Val: 1.0469\n",
      "Epoch: 400, Loss: 1.0183, Train: 1.0968, Val: 1.0974\n",
      "Epoch: 401, Loss: 1.0170, Train: 1.0732, Val: 1.0744\n",
      "Epoch: 402, Loss: 1.0138, Train: 1.0444, Val: 1.0470\n",
      "Epoch: 403, Loss: 1.0171, Train: 1.0962, Val: 1.0972\n",
      "Epoch: 404, Loss: 1.0154, Train: 1.0268, Val: 1.0294\n",
      "Epoch: 405, Loss: 1.0144, Train: 1.0590, Val: 1.0617\n",
      "Epoch: 406, Loss: 1.0119, Train: 1.0634, Val: 1.0655\n",
      "Epoch: 407, Loss: 1.0125, Train: 1.0418, Val: 1.0443\n",
      "Epoch: 408, Loss: 1.0156, Train: 1.0601, Val: 1.0627\n",
      "Epoch: 409, Loss: 1.0165, Train: 1.0376, Val: 1.0407\n",
      "Epoch: 410, Loss: 1.0169, Train: 1.0296, Val: 1.0324\n",
      "Epoch: 411, Loss: 1.0164, Train: 1.0570, Val: 1.0594\n",
      "Epoch: 412, Loss: 1.0144, Train: 1.0123, Val: 1.0162\n",
      "Epoch: 413, Loss: 1.0199, Train: 1.1106, Val: 1.1117\n",
      "Epoch: 414, Loss: 1.0297, Train: 1.0110, Val: 1.0148\n",
      "Epoch: 415, Loss: 1.0181, Train: 1.0462, Val: 1.0488\n",
      "Epoch: 416, Loss: 1.0149, Train: 1.0519, Val: 1.0539\n",
      "Epoch: 417, Loss: 1.0159, Train: 1.0166, Val: 1.0201\n",
      "Epoch: 418, Loss: 1.0137, Train: 1.0562, Val: 1.0587\n",
      "Epoch: 419, Loss: 1.0105, Train: 1.0315, Val: 1.0340\n",
      "Epoch: 420, Loss: 1.0102, Train: 1.0251, Val: 1.0282\n",
      "Epoch: 421, Loss: 1.0124, Train: 1.0439, Val: 1.0466\n",
      "Epoch: 422, Loss: 1.0139, Train: 1.0162, Val: 1.0198\n",
      "Epoch: 423, Loss: 1.0160, Train: 1.0273, Val: 1.0306\n",
      "Epoch: 424, Loss: 1.0133, Train: 1.0305, Val: 1.0341\n",
      "Epoch: 425, Loss: 1.0111, Train: 1.0191, Val: 1.0236\n",
      "Epoch: 426, Loss: 1.0142, Train: 1.0636, Val: 1.0660\n",
      "Epoch: 427, Loss: 1.0201, Train: 1.0140, Val: 1.0179\n",
      "Epoch: 428, Loss: 1.0143, Train: 1.0247, Val: 1.0281\n",
      "Epoch: 429, Loss: 1.0111, Train: 1.0219, Val: 1.0256\n",
      "Epoch: 430, Loss: 1.0142, Train: 1.0151, Val: 1.0187\n",
      "Epoch: 431, Loss: 1.0152, Train: 1.0158, Val: 1.0192\n",
      "Epoch: 432, Loss: 1.0137, Train: 1.0182, Val: 1.0215\n",
      "Epoch: 433, Loss: 1.0141, Train: 1.0141, Val: 1.0175\n",
      "Epoch: 434, Loss: 1.0121, Train: 1.0306, Val: 1.0330\n",
      "Epoch: 435, Loss: 1.0096, Train: 1.0208, Val: 1.0245\n",
      "Epoch: 436, Loss: 1.0128, Train: 1.0402, Val: 1.0428\n",
      "Epoch: 437, Loss: 1.0230, Train: 1.0121, Val: 1.0163\n",
      "Epoch: 438, Loss: 1.0108, Train: 1.0149, Val: 1.0185\n",
      "Epoch: 439, Loss: 1.0109, Train: 1.0372, Val: 1.0396\n",
      "Epoch: 440, Loss: 1.0159, Train: 1.0158, Val: 1.0196\n",
      "Epoch: 441, Loss: 1.0127, Train: 1.0237, Val: 1.0271\n",
      "Epoch: 442, Loss: 1.0096, Train: 1.0167, Val: 1.0204\n",
      "Epoch: 443, Loss: 1.0101, Train: 1.0276, Val: 1.0312\n",
      "Epoch: 444, Loss: 1.0092, Train: 1.0481, Val: 1.0508\n",
      "Epoch: 445, Loss: 1.0217, Train: 1.0240, Val: 1.0281\n",
      "Epoch: 446, Loss: 1.0309, Train: 1.0482, Val: 1.0503\n",
      "Epoch: 447, Loss: 1.0900, Train: 1.2671, Val: 1.2682\n",
      "Epoch: 448, Loss: 1.2706, Train: 1.2610, Val: 1.2616\n",
      "Epoch: 449, Loss: 1.2427, Train: 1.3052, Val: 1.3041\n",
      "Epoch: 450, Loss: 1.1595, Train: 1.6713, Val: 1.6667\n",
      "Epoch: 451, Loss: 1.1564, Train: 1.6842, Val: 1.6795\n",
      "Epoch: 452, Loss: 1.1611, Train: 1.6822, Val: 1.6778\n",
      "Epoch: 453, Loss: 1.1667, Train: 1.6150, Val: 1.6098\n",
      "Epoch: 454, Loss: 1.1490, Train: 1.5481, Val: 1.5422\n",
      "Epoch: 455, Loss: 1.1559, Train: 1.1474, Val: 1.1477\n",
      "Epoch: 456, Loss: 1.1497, Train: 1.1382, Val: 1.1382\n",
      "Epoch: 457, Loss: 1.1305, Train: 1.1427, Val: 1.1421\n",
      "Epoch: 458, Loss: 1.1367, Train: 1.1457, Val: 1.1451\n",
      "Epoch: 459, Loss: 1.1223, Train: 1.1306, Val: 1.1300\n",
      "Epoch: 460, Loss: 1.1191, Train: 1.1206, Val: 1.1202\n",
      "Epoch: 461, Loss: 1.1151, Train: 1.1173, Val: 1.1168\n",
      "Epoch: 462, Loss: 1.1037, Train: 1.1206, Val: 1.1201\n",
      "Epoch: 463, Loss: 1.1048, Train: 1.0896, Val: 1.0898\n",
      "Epoch: 464, Loss: 1.0967, Train: 1.1037, Val: 1.1036\n",
      "Epoch: 465, Loss: 1.0931, Train: 1.1193, Val: 1.1191\n",
      "Epoch: 466, Loss: 1.0906, Train: 1.0835, Val: 1.0837\n",
      "Epoch: 467, Loss: 1.0809, Train: 1.0596, Val: 1.0603\n",
      "Epoch: 468, Loss: 1.0791, Train: 1.0601, Val: 1.0608\n",
      "Epoch: 469, Loss: 1.0733, Train: 1.0631, Val: 1.0637\n",
      "Epoch: 470, Loss: 1.0670, Train: 1.0546, Val: 1.0556\n",
      "Epoch: 471, Loss: 1.0618, Train: 1.0521, Val: 1.0531\n",
      "Epoch: 472, Loss: 1.0615, Train: 1.0566, Val: 1.0576\n",
      "Epoch: 473, Loss: 1.0523, Train: 1.0600, Val: 1.0607\n",
      "Epoch: 474, Loss: 1.0515, Train: 1.0475, Val: 1.0483\n",
      "Epoch: 475, Loss: 1.0474, Train: 1.0468, Val: 1.0476\n",
      "Epoch: 476, Loss: 1.0443, Train: 1.0522, Val: 1.0530\n",
      "Epoch: 477, Loss: 1.0413, Train: 1.0411, Val: 1.0426\n",
      "Epoch: 478, Loss: 1.0392, Train: 1.0355, Val: 1.0378\n",
      "Epoch: 479, Loss: 1.0370, Train: 1.0369, Val: 1.0391\n",
      "Epoch: 480, Loss: 1.0368, Train: 1.0445, Val: 1.0460\n",
      "Epoch: 481, Loss: 1.0382, Train: 1.0625, Val: 1.0628\n",
      "Epoch: 482, Loss: 1.0485, Train: 1.0853, Val: 1.0857\n",
      "Epoch: 483, Loss: 1.0525, Train: 1.0992, Val: 1.1000\n",
      "Epoch: 484, Loss: 1.0474, Train: 1.0666, Val: 1.0683\n",
      "Epoch: 485, Loss: 1.0441, Train: 1.0432, Val: 1.0452\n",
      "Epoch: 486, Loss: 1.0443, Train: 1.0317, Val: 1.0335\n",
      "Epoch: 487, Loss: 1.0454, Train: 1.0459, Val: 1.0472\n",
      "Epoch: 488, Loss: 1.0340, Train: 1.0557, Val: 1.0571\n",
      "Epoch: 489, Loss: 1.0368, Train: 1.0505, Val: 1.0521\n",
      "Epoch: 490, Loss: 1.0355, Train: 1.0248, Val: 1.0271\n",
      "Epoch: 491, Loss: 1.0266, Train: 1.0265, Val: 1.0291\n",
      "Epoch: 492, Loss: 1.0287, Train: 1.0175, Val: 1.0203\n",
      "Epoch: 493, Loss: 1.0248, Train: 1.0271, Val: 1.0298\n",
      "Epoch: 494, Loss: 1.0264, Train: 1.0159, Val: 1.0187\n",
      "Epoch: 495, Loss: 1.0205, Train: 1.0422, Val: 1.0441\n",
      "Epoch: 496, Loss: 1.0200, Train: 1.0290, Val: 1.0315\n",
      "Epoch: 497, Loss: 1.0169, Train: 1.0136, Val: 1.0165\n",
      "Epoch: 498, Loss: 1.0173, Train: 1.0112, Val: 1.0144\n",
      "Epoch: 499, Loss: 1.0134, Train: 1.0187, Val: 1.0220\n",
      "Epoch: 500, Loss: 1.0120, Train: 1.0140, Val: 1.0174\n",
      "Test RMSE: 1.0209\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.553251       3.583673\n",
      "std      1727.484387     741.673176       0.447088       1.116938\n",
      "min         0.000000       0.000000       1.647398       1.000000\n",
      "25%      1500.000000     259.000000       3.256177       3.000000\n",
      "50%      3066.000000     693.000000       3.616825       4.000000\n",
      "75%      4472.000000    1292.000000       3.878940       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1090.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.89095789297097\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  2\n",
      "Kernel:  gd\n",
      "Model Name:  KPGIN\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 15.5108, Train: 3.5872, Val: 3.5908\n",
      "Epoch: 002, Loss: 13.7605, Train: 3.0232, Val: 3.0267\n",
      "Epoch: 003, Loss: 11.3765, Train: 2.7303, Val: 2.7330\n",
      "Epoch: 004, Loss: 8.4566, Train: 2.4199, Val: 2.4201\n",
      "Epoch: 005, Loss: 5.3075, Train: 1.9709, Val: 1.9690\n",
      "Epoch: 006, Loss: 3.0513, Train: 1.5723, Val: 1.5665\n",
      "Epoch: 007, Loss: 3.4055, Train: 1.5109, Val: 1.5042\n",
      "Epoch: 008, Loss: 3.4932, Train: 1.5023, Val: 1.4973\n",
      "Epoch: 009, Loss: 2.0738, Train: 1.5835, Val: 1.5814\n",
      "Epoch: 010, Loss: 1.9925, Train: 1.6512, Val: 1.6471\n",
      "Epoch: 011, Loss: 2.0432, Train: 1.6387, Val: 1.6356\n",
      "Epoch: 012, Loss: 1.9826, Train: 1.5743, Val: 1.5724\n",
      "Epoch: 013, Loss: 1.7073, Train: 1.5003, Val: 1.4973\n",
      "Epoch: 014, Loss: 1.5136, Train: 1.4488, Val: 1.4447\n",
      "Epoch: 015, Loss: 1.5990, Train: 1.4280, Val: 1.4238\n",
      "Epoch: 016, Loss: 1.7029, Train: 1.4138, Val: 1.4105\n",
      "Epoch: 017, Loss: 1.4757, Train: 1.4118, Val: 1.4093\n",
      "Epoch: 018, Loss: 1.4228, Train: 1.4100, Val: 1.4070\n",
      "Epoch: 019, Loss: 1.4456, Train: 1.3868, Val: 1.3833\n",
      "Epoch: 020, Loss: 1.3936, Train: 1.3406, Val: 1.3366\n",
      "Epoch: 021, Loss: 1.3177, Train: 1.2941, Val: 1.2899\n",
      "Epoch: 022, Loss: 1.2960, Train: 1.2600, Val: 1.2560\n",
      "Epoch: 023, Loss: 1.3223, Train: 1.2256, Val: 1.2223\n",
      "Epoch: 024, Loss: 1.3033, Train: 1.1929, Val: 1.1913\n",
      "Epoch: 025, Loss: 1.2586, Train: 1.1832, Val: 1.1827\n",
      "Epoch: 026, Loss: 1.2563, Train: 1.1880, Val: 1.1883\n",
      "Epoch: 027, Loss: 1.2796, Train: 1.1786, Val: 1.1793\n",
      "Epoch: 028, Loss: 1.2750, Train: 1.1511, Val: 1.1521\n",
      "Epoch: 029, Loss: 1.2526, Train: 1.1277, Val: 1.1285\n",
      "Epoch: 030, Loss: 1.2549, Train: 1.1204, Val: 1.1211\n",
      "Epoch: 031, Loss: 1.2745, Train: 1.1221, Val: 1.1228\n",
      "Epoch: 032, Loss: 1.2708, Train: 1.1315, Val: 1.1324\n",
      "Epoch: 033, Loss: 1.2557, Train: 1.1478, Val: 1.1489\n",
      "Epoch: 034, Loss: 1.2608, Train: 1.1566, Val: 1.1577\n",
      "Epoch: 035, Loss: 1.2720, Train: 1.1477, Val: 1.1487\n",
      "Epoch: 036, Loss: 1.2660, Train: 1.1311, Val: 1.1319\n",
      "Epoch: 037, Loss: 1.2556, Train: 1.1216, Val: 1.1221\n",
      "Epoch: 038, Loss: 1.2596, Train: 1.1196, Val: 1.1200\n",
      "Epoch: 039, Loss: 1.2648, Train: 1.1196, Val: 1.1200\n",
      "Epoch: 040, Loss: 1.2571, Train: 1.1232, Val: 1.1238\n",
      "Epoch: 041, Loss: 1.2511, Train: 1.1290, Val: 1.1297\n",
      "Epoch: 042, Loss: 1.2550, Train: 1.1295, Val: 1.1302\n",
      "Epoch: 043, Loss: 1.2561, Train: 1.1234, Val: 1.1240\n",
      "Epoch: 044, Loss: 1.2499, Train: 1.1179, Val: 1.1183\n",
      "Epoch: 045, Loss: 1.2477, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 046, Loss: 1.2513, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 047, Loss: 1.2506, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 048, Loss: 1.2465, Train: 1.1194, Val: 1.1199\n",
      "Epoch: 049, Loss: 1.2472, Train: 1.1210, Val: 1.1216\n",
      "Epoch: 050, Loss: 1.2495, Train: 1.1192, Val: 1.1197\n",
      "Epoch: 051, Loss: 1.2476, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 052, Loss: 1.2456, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 053, Loss: 1.2472, Train: 1.1162, Val: 1.1165\n",
      "Epoch: 054, Loss: 1.2476, Train: 1.1161, Val: 1.1163\n",
      "Epoch: 055, Loss: 1.2453, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 056, Loss: 1.2474, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 057, Loss: 1.2450, Train: 1.1163, Val: 1.1166\n",
      "Epoch: 058, Loss: 1.2448, Train: 1.1160, Val: 1.1163\n",
      "Epoch: 059, Loss: 1.2448, Train: 1.1158, Val: 1.1161\n",
      "Epoch: 060, Loss: 1.2446, Train: 1.1159, Val: 1.1162\n",
      "Epoch: 061, Loss: 1.2443, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 062, Loss: 1.2440, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 063, Loss: 1.2438, Train: 1.1158, Val: 1.1161\n",
      "Epoch: 064, Loss: 1.2433, Train: 1.1153, Val: 1.1156\n",
      "Epoch: 065, Loss: 1.2428, Train: 1.1149, Val: 1.1152\n",
      "Epoch: 066, Loss: 1.2423, Train: 1.1147, Val: 1.1150\n",
      "Epoch: 067, Loss: 1.2416, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 068, Loss: 1.2395, Train: 1.1145, Val: 1.1148\n",
      "Epoch: 069, Loss: 1.2672, Train: 1.1169, Val: 1.1170\n",
      "Epoch: 070, Loss: 1.2421, Train: 1.1341, Val: 1.1339\n",
      "Epoch: 071, Loss: 1.2503, Train: 1.1405, Val: 1.1402\n",
      "Epoch: 072, Loss: 1.2617, Train: 1.1234, Val: 1.1233\n",
      "Epoch: 073, Loss: 1.2427, Train: 1.1126, Val: 1.1128\n",
      "Epoch: 074, Loss: 1.2381, Train: 1.1143, Val: 1.1146\n",
      "Epoch: 075, Loss: 1.3284, Train: 1.1146, Val: 1.1146\n",
      "Epoch: 076, Loss: 1.2306, Train: 1.1282, Val: 1.1279\n",
      "Epoch: 077, Loss: 1.2585, Train: 1.1231, Val: 1.1228\n",
      "Epoch: 078, Loss: 1.2510, Train: 1.1102, Val: 1.1103\n",
      "Epoch: 079, Loss: 1.2255, Train: 1.1144, Val: 1.1148\n",
      "Epoch: 080, Loss: 1.2369, Train: 1.1153, Val: 1.1158\n",
      "Epoch: 081, Loss: 1.2425, Train: 1.1070, Val: 1.1072\n",
      "Epoch: 082, Loss: 1.2186, Train: 1.1092, Val: 1.1092\n",
      "Epoch: 083, Loss: 1.2319, Train: 1.1078, Val: 1.1077\n",
      "Epoch: 084, Loss: 1.2310, Train: 1.1025, Val: 1.1027\n",
      "Epoch: 085, Loss: 1.2100, Train: 1.1085, Val: 1.1090\n",
      "Epoch: 086, Loss: 1.2122, Train: 1.1102, Val: 1.1107\n",
      "Epoch: 087, Loss: 1.2117, Train: 1.1018, Val: 1.1021\n",
      "Epoch: 088, Loss: 1.1990, Train: 1.0962, Val: 1.0964\n",
      "Epoch: 089, Loss: 1.1998, Train: 1.0949, Val: 1.0950\n",
      "Epoch: 090, Loss: 1.1998, Train: 1.0940, Val: 1.0941\n",
      "Epoch: 091, Loss: 1.2043, Train: 1.0939, Val: 1.0939\n",
      "Epoch: 092, Loss: 1.1947, Train: 1.0983, Val: 1.0983\n",
      "Epoch: 093, Loss: 1.1957, Train: 1.0992, Val: 1.0992\n",
      "Epoch: 094, Loss: 1.1971, Train: 1.0933, Val: 1.0935\n",
      "Epoch: 095, Loss: 1.1903, Train: 1.0907, Val: 1.0911\n",
      "Epoch: 096, Loss: 1.1909, Train: 1.0908, Val: 1.0913\n",
      "Epoch: 097, Loss: 1.1925, Train: 1.0898, Val: 1.0903\n",
      "Epoch: 098, Loss: 1.1876, Train: 1.0910, Val: 1.0914\n",
      "Epoch: 099, Loss: 1.1874, Train: 1.0917, Val: 1.0922\n",
      "Epoch: 100, Loss: 1.1886, Train: 1.0891, Val: 1.0897\n",
      "Epoch: 101, Loss: 1.1849, Train: 1.0883, Val: 1.0891\n",
      "Epoch: 102, Loss: 1.1844, Train: 1.0888, Val: 1.0895\n",
      "Epoch: 103, Loss: 1.1852, Train: 1.0875, Val: 1.0882\n",
      "Epoch: 104, Loss: 1.1823, Train: 1.0872, Val: 1.0879\n",
      "Epoch: 105, Loss: 1.1818, Train: 1.0874, Val: 1.0881\n",
      "Epoch: 106, Loss: 1.1823, Train: 1.0863, Val: 1.0869\n",
      "Epoch: 107, Loss: 1.1799, Train: 1.0862, Val: 1.0870\n",
      "Epoch: 108, Loss: 1.1795, Train: 1.0864, Val: 1.0871\n",
      "Epoch: 109, Loss: 1.1795, Train: 1.0853, Val: 1.0859\n",
      "Epoch: 110, Loss: 1.1775, Train: 1.0850, Val: 1.0856\n",
      "Epoch: 111, Loss: 1.1772, Train: 1.0848, Val: 1.0854\n",
      "Epoch: 112, Loss: 1.1769, Train: 1.0842, Val: 1.0848\n",
      "Epoch: 113, Loss: 1.1753, Train: 1.0843, Val: 1.0849\n",
      "Epoch: 114, Loss: 1.1752, Train: 1.0841, Val: 1.0847\n",
      "Epoch: 115, Loss: 1.1747, Train: 1.0833, Val: 1.0839\n",
      "Epoch: 116, Loss: 1.1734, Train: 1.0833, Val: 1.0838\n",
      "Epoch: 117, Loss: 1.1732, Train: 1.0830, Val: 1.0835\n",
      "Epoch: 118, Loss: 1.1725, Train: 1.0825, Val: 1.0830\n",
      "Epoch: 119, Loss: 1.1715, Train: 1.0824, Val: 1.0830\n",
      "Epoch: 120, Loss: 1.1713, Train: 1.0820, Val: 1.0825\n",
      "Epoch: 121, Loss: 1.1705, Train: 1.0819, Val: 1.0824\n",
      "Epoch: 122, Loss: 1.1698, Train: 1.0819, Val: 1.0824\n",
      "Epoch: 123, Loss: 1.1695, Train: 1.0814, Val: 1.0819\n",
      "Epoch: 124, Loss: 1.1687, Train: 1.0810, Val: 1.0815\n",
      "Epoch: 125, Loss: 1.1683, Train: 1.0808, Val: 1.0813\n",
      "Epoch: 126, Loss: 1.1678, Train: 1.0807, Val: 1.0811\n",
      "Epoch: 127, Loss: 1.1671, Train: 1.0808, Val: 1.0812\n",
      "Epoch: 128, Loss: 1.1668, Train: 1.0805, Val: 1.0808\n",
      "Epoch: 129, Loss: 1.1662, Train: 1.0800, Val: 1.0804\n",
      "Epoch: 130, Loss: 1.1657, Train: 1.0798, Val: 1.0802\n",
      "Epoch: 131, Loss: 1.1654, Train: 1.0798, Val: 1.0801\n",
      "Epoch: 132, Loss: 1.1648, Train: 1.0799, Val: 1.0802\n",
      "Epoch: 133, Loss: 1.1645, Train: 1.0796, Val: 1.0799\n",
      "Epoch: 134, Loss: 1.1639, Train: 1.0792, Val: 1.0795\n",
      "Epoch: 135, Loss: 1.1636, Train: 1.0791, Val: 1.0794\n",
      "Epoch: 136, Loss: 1.1631, Train: 1.0791, Val: 1.0793\n",
      "Epoch: 137, Loss: 1.1627, Train: 1.0789, Val: 1.0791\n",
      "Epoch: 138, Loss: 1.1623, Train: 1.0786, Val: 1.0789\n",
      "Epoch: 139, Loss: 1.1619, Train: 1.0785, Val: 1.0788\n",
      "Epoch: 140, Loss: 1.1614, Train: 1.0785, Val: 1.0787\n",
      "Epoch: 141, Loss: 1.1611, Train: 1.0782, Val: 1.0785\n",
      "Epoch: 142, Loss: 1.1607, Train: 1.0781, Val: 1.0783\n",
      "Epoch: 143, Loss: 1.1606, Train: 1.0777, Val: 1.0780\n",
      "Epoch: 144, Loss: 1.1601, Train: 1.0775, Val: 1.0778\n",
      "Epoch: 145, Loss: 1.1598, Train: 1.0776, Val: 1.0779\n",
      "Epoch: 146, Loss: 1.1594, Train: 1.0775, Val: 1.0778\n",
      "Epoch: 147, Loss: 1.1590, Train: 1.0770, Val: 1.0773\n",
      "Epoch: 148, Loss: 1.1589, Train: 1.0768, Val: 1.0772\n",
      "Epoch: 149, Loss: 1.1589, Train: 1.0766, Val: 1.0770\n",
      "Epoch: 150, Loss: 1.1587, Train: 1.0766, Val: 1.0770\n",
      "Epoch: 151, Loss: 1.1581, Train: 1.0763, Val: 1.0767\n",
      "Epoch: 152, Loss: 1.1577, Train: 1.0763, Val: 1.0767\n",
      "Epoch: 153, Loss: 1.1576, Train: 1.0760, Val: 1.0764\n",
      "Epoch: 154, Loss: 1.1571, Train: 1.0759, Val: 1.0764\n",
      "Epoch: 155, Loss: 1.1569, Train: 1.0757, Val: 1.0762\n",
      "Epoch: 156, Loss: 1.1565, Train: 1.0755, Val: 1.0760\n",
      "Epoch: 157, Loss: 1.1563, Train: 1.0754, Val: 1.0759\n",
      "Epoch: 158, Loss: 1.1558, Train: 1.0754, Val: 1.0759\n",
      "Epoch: 159, Loss: 1.1557, Train: 1.0752, Val: 1.0756\n",
      "Epoch: 160, Loss: 1.1553, Train: 1.0752, Val: 1.0756\n",
      "Epoch: 161, Loss: 1.1551, Train: 1.0749, Val: 1.0754\n",
      "Epoch: 162, Loss: 1.1547, Train: 1.0749, Val: 1.0753\n",
      "Epoch: 163, Loss: 1.1546, Train: 1.0748, Val: 1.0752\n",
      "Epoch: 164, Loss: 1.1542, Train: 1.0747, Val: 1.0752\n",
      "Epoch: 165, Loss: 1.1539, Train: 1.0746, Val: 1.0751\n",
      "Epoch: 166, Loss: 1.1538, Train: 1.0746, Val: 1.0750\n",
      "Epoch: 167, Loss: 1.1535, Train: 1.0744, Val: 1.0749\n",
      "Epoch: 168, Loss: 1.1532, Train: 1.0743, Val: 1.0748\n",
      "Epoch: 169, Loss: 1.1531, Train: 1.0743, Val: 1.0747\n",
      "Epoch: 170, Loss: 1.1528, Train: 1.0741, Val: 1.0745\n",
      "Epoch: 171, Loss: 1.1524, Train: 1.0739, Val: 1.0744\n",
      "Epoch: 172, Loss: 1.1523, Train: 1.0740, Val: 1.0745\n",
      "Epoch: 173, Loss: 1.1521, Train: 1.0737, Val: 1.0742\n",
      "Epoch: 174, Loss: 1.1518, Train: 1.0737, Val: 1.0741\n",
      "Epoch: 175, Loss: 1.1514, Train: 1.0738, Val: 1.0742\n",
      "Epoch: 176, Loss: 1.1513, Train: 1.0734, Val: 1.0739\n",
      "Epoch: 177, Loss: 1.1511, Train: 1.0736, Val: 1.0740\n",
      "Epoch: 178, Loss: 1.1510, Train: 1.0732, Val: 1.0737\n",
      "Epoch: 179, Loss: 1.1506, Train: 1.0733, Val: 1.0738\n",
      "Epoch: 180, Loss: 1.1502, Train: 1.0733, Val: 1.0738\n",
      "Epoch: 181, Loss: 1.1500, Train: 1.0729, Val: 1.0734\n",
      "Epoch: 182, Loss: 1.1501, Train: 1.0733, Val: 1.0738\n",
      "Epoch: 183, Loss: 1.1504, Train: 1.0726, Val: 1.0732\n",
      "Epoch: 184, Loss: 1.1493, Train: 1.0725, Val: 1.0730\n",
      "Epoch: 185, Loss: 1.1494, Train: 1.0740, Val: 1.0744\n",
      "Epoch: 186, Loss: 1.1511, Train: 1.0722, Val: 1.0728\n",
      "Epoch: 187, Loss: 1.1487, Train: 1.0724, Val: 1.0730\n",
      "Epoch: 188, Loss: 1.1493, Train: 1.0738, Val: 1.0743\n",
      "Epoch: 189, Loss: 1.1506, Train: 1.0718, Val: 1.0724\n",
      "Epoch: 190, Loss: 1.1479, Train: 1.0730, Val: 1.0737\n",
      "Epoch: 191, Loss: 1.1505, Train: 1.0728, Val: 1.0733\n",
      "Epoch: 192, Loss: 1.1502, Train: 1.0716, Val: 1.0722\n",
      "Epoch: 193, Loss: 1.1484, Train: 1.0739, Val: 1.0747\n",
      "Epoch: 194, Loss: 1.1500, Train: 1.0711, Val: 1.0717\n",
      "Epoch: 195, Loss: 1.1468, Train: 1.0725, Val: 1.0730\n",
      "Epoch: 196, Loss: 1.1492, Train: 1.0711, Val: 1.0717\n",
      "Epoch: 197, Loss: 1.1466, Train: 1.0719, Val: 1.0726\n",
      "Epoch: 198, Loss: 1.1473, Train: 1.0713, Val: 1.0719\n",
      "Epoch: 199, Loss: 1.1469, Train: 1.0709, Val: 1.0714\n",
      "Epoch: 200, Loss: 1.1457, Train: 1.0716, Val: 1.0723\n",
      "Epoch: 201, Loss: 1.1470, Train: 1.0709, Val: 1.0714\n",
      "Epoch: 202, Loss: 1.1450, Train: 1.0708, Val: 1.0714\n",
      "Epoch: 203, Loss: 1.1447, Train: 1.0710, Val: 1.0717\n",
      "Epoch: 204, Loss: 1.1454, Train: 1.0705, Val: 1.0711\n",
      "Epoch: 205, Loss: 1.1443, Train: 1.0704, Val: 1.0710\n",
      "Epoch: 206, Loss: 1.1437, Train: 1.0706, Val: 1.0713\n",
      "Epoch: 207, Loss: 1.1449, Train: 1.0706, Val: 1.0712\n",
      "Epoch: 208, Loss: 1.1448, Train: 1.0701, Val: 1.0708\n",
      "Epoch: 209, Loss: 1.1432, Train: 1.0715, Val: 1.0723\n",
      "Epoch: 210, Loss: 1.1436, Train: 1.0697, Val: 1.0703\n",
      "Epoch: 211, Loss: 1.1422, Train: 1.0699, Val: 1.0705\n",
      "Epoch: 212, Loss: 1.1425, Train: 1.0709, Val: 1.0718\n",
      "Epoch: 213, Loss: 1.1419, Train: 1.0694, Val: 1.0702\n",
      "Epoch: 214, Loss: 1.1402, Train: 1.0693, Val: 1.0700\n",
      "Epoch: 215, Loss: 1.1399, Train: 1.0688, Val: 1.0695\n",
      "Epoch: 216, Loss: 1.1697, Train: 1.0867, Val: 1.0869\n",
      "Epoch: 217, Loss: 1.1808, Train: 1.0798, Val: 1.0804\n",
      "Epoch: 218, Loss: 1.1768, Train: 1.0881, Val: 1.0891\n",
      "Epoch: 219, Loss: 1.1850, Train: 1.0891, Val: 1.0902\n",
      "Epoch: 220, Loss: 1.1707, Train: 1.0751, Val: 1.0761\n",
      "Epoch: 221, Loss: 1.1475, Train: 1.0727, Val: 1.0734\n",
      "Epoch: 222, Loss: 1.1478, Train: 1.0893, Val: 1.0897\n",
      "Epoch: 223, Loss: 1.1987, Train: 1.0755, Val: 1.0764\n",
      "Epoch: 224, Loss: 1.2020, Train: 1.0734, Val: 1.0739\n",
      "Epoch: 225, Loss: 1.1481, Train: 1.0988, Val: 1.0989\n",
      "Epoch: 226, Loss: 1.1868, Train: 1.0709, Val: 1.0713\n",
      "Epoch: 227, Loss: 1.1557, Train: 1.0708, Val: 1.0712\n",
      "Epoch: 228, Loss: 1.1632, Train: 1.0899, Val: 1.0899\n",
      "Epoch: 229, Loss: 1.1473, Train: 1.1121, Val: 1.1118\n",
      "Epoch: 230, Loss: 1.1615, Train: 1.0841, Val: 1.0842\n",
      "Epoch: 231, Loss: 1.1428, Train: 1.0722, Val: 1.0726\n",
      "Epoch: 232, Loss: 1.1475, Train: 1.0734, Val: 1.0738\n",
      "Epoch: 233, Loss: 1.1409, Train: 1.0787, Val: 1.0790\n",
      "Epoch: 234, Loss: 1.1449, Train: 1.0725, Val: 1.0730\n",
      "Epoch: 235, Loss: 1.1409, Train: 1.0683, Val: 1.0691\n",
      "Epoch: 236, Loss: 1.1397, Train: 1.0674, Val: 1.0682\n",
      "Epoch: 237, Loss: 1.1375, Train: 1.0675, Val: 1.0681\n",
      "Epoch: 238, Loss: 1.1351, Train: 1.0675, Val: 1.0680\n",
      "Epoch: 239, Loss: 1.1344, Train: 1.0646, Val: 1.0653\n",
      "Epoch: 240, Loss: 1.1284, Train: 1.0653, Val: 1.0662\n",
      "Epoch: 241, Loss: 1.1343, Train: 1.0842, Val: 1.0855\n",
      "Epoch: 242, Loss: 1.1374, Train: 1.0748, Val: 1.0759\n",
      "Epoch: 243, Loss: 1.1319, Train: 1.0692, Val: 1.0702\n",
      "Epoch: 244, Loss: 1.1385, Train: 1.0712, Val: 1.0723\n",
      "Epoch: 245, Loss: 1.1350, Train: 1.0833, Val: 1.0847\n",
      "Epoch: 246, Loss: 1.1313, Train: 1.0871, Val: 1.0886\n",
      "Epoch: 247, Loss: 1.1294, Train: 1.0728, Val: 1.0739\n",
      "Epoch: 248, Loss: 1.1274, Train: 1.0676, Val: 1.0685\n",
      "Epoch: 249, Loss: 1.1307, Train: 1.0718, Val: 1.0729\n",
      "Epoch: 250, Loss: 1.1265, Train: 1.0728, Val: 1.0740\n",
      "Epoch: 251, Loss: 1.1230, Train: 1.0675, Val: 1.0686\n",
      "Epoch: 252, Loss: 1.1225, Train: 1.0632, Val: 1.0643\n",
      "Epoch: 253, Loss: 1.1217, Train: 1.0622, Val: 1.0635\n",
      "Epoch: 254, Loss: 1.1183, Train: 1.0646, Val: 1.0662\n",
      "Epoch: 255, Loss: 1.1159, Train: 1.0613, Val: 1.0630\n",
      "Epoch: 256, Loss: 1.1140, Train: 1.0552, Val: 1.0566\n",
      "Epoch: 257, Loss: 1.1128, Train: 1.0539, Val: 1.0552\n",
      "Epoch: 258, Loss: 1.1102, Train: 1.0546, Val: 1.0559\n",
      "Epoch: 259, Loss: 1.1071, Train: 1.0550, Val: 1.0563\n",
      "Epoch: 260, Loss: 1.1065, Train: 1.0534, Val: 1.0546\n",
      "Epoch: 261, Loss: 1.1027, Train: 1.0524, Val: 1.0536\n",
      "Epoch: 262, Loss: 1.1006, Train: 1.0525, Val: 1.0537\n",
      "Epoch: 263, Loss: 1.1006, Train: 1.0512, Val: 1.0526\n",
      "Epoch: 264, Loss: 1.1005, Train: 1.0515, Val: 1.0527\n",
      "Epoch: 265, Loss: 1.1037, Train: 1.0508, Val: 1.0522\n",
      "Epoch: 266, Loss: 1.0995, Train: 1.0556, Val: 1.0575\n",
      "Epoch: 267, Loss: 1.0992, Train: 1.0494, Val: 1.0511\n",
      "Epoch: 268, Loss: 1.0927, Train: 1.0467, Val: 1.0481\n",
      "Epoch: 269, Loss: 1.0971, Train: 1.0456, Val: 1.0472\n",
      "Epoch: 270, Loss: 1.0926, Train: 1.0500, Val: 1.0522\n",
      "Epoch: 271, Loss: 1.0905, Train: 1.0493, Val: 1.0515\n",
      "Epoch: 272, Loss: 1.0908, Train: 1.0444, Val: 1.0463\n",
      "Epoch: 273, Loss: 1.0892, Train: 1.0434, Val: 1.0453\n",
      "Epoch: 274, Loss: 1.0885, Train: 1.0446, Val: 1.0468\n",
      "Epoch: 275, Loss: 1.0855, Train: 1.0455, Val: 1.0479\n",
      "Epoch: 276, Loss: 1.0863, Train: 1.0420, Val: 1.0440\n",
      "Epoch: 277, Loss: 1.0837, Train: 1.0418, Val: 1.0438\n",
      "Epoch: 278, Loss: 1.0841, Train: 1.0428, Val: 1.0451\n",
      "Epoch: 279, Loss: 1.0815, Train: 1.0452, Val: 1.0478\n",
      "Epoch: 280, Loss: 1.0816, Train: 1.0407, Val: 1.0431\n",
      "Epoch: 281, Loss: 1.0799, Train: 1.0389, Val: 1.0412\n",
      "Epoch: 282, Loss: 1.0799, Train: 1.0398, Val: 1.0422\n",
      "Epoch: 283, Loss: 1.0774, Train: 1.0411, Val: 1.0436\n",
      "Epoch: 284, Loss: 1.0777, Train: 1.0385, Val: 1.0408\n",
      "Epoch: 285, Loss: 1.0760, Train: 1.0373, Val: 1.0395\n",
      "Epoch: 286, Loss: 1.0755, Train: 1.0372, Val: 1.0396\n",
      "Epoch: 287, Loss: 1.0744, Train: 1.0372, Val: 1.0396\n",
      "Epoch: 288, Loss: 1.0734, Train: 1.0363, Val: 1.0384\n",
      "Epoch: 289, Loss: 1.0724, Train: 1.0360, Val: 1.0381\n",
      "Epoch: 290, Loss: 1.0717, Train: 1.0355, Val: 1.0378\n",
      "Epoch: 291, Loss: 1.0704, Train: 1.0348, Val: 1.0370\n",
      "Epoch: 292, Loss: 1.0695, Train: 1.0347, Val: 1.0367\n",
      "Epoch: 293, Loss: 1.0688, Train: 1.0341, Val: 1.0361\n",
      "Epoch: 294, Loss: 1.0675, Train: 1.0338, Val: 1.0360\n",
      "Epoch: 295, Loss: 1.0671, Train: 1.0335, Val: 1.0354\n",
      "Epoch: 296, Loss: 1.0659, Train: 1.0335, Val: 1.0354\n",
      "Epoch: 297, Loss: 1.0653, Train: 1.0328, Val: 1.0348\n",
      "Epoch: 298, Loss: 1.0645, Train: 1.0327, Val: 1.0346\n",
      "Epoch: 299, Loss: 1.0635, Train: 1.0324, Val: 1.0343\n",
      "Epoch: 300, Loss: 1.0628, Train: 1.0319, Val: 1.0338\n",
      "Epoch: 301, Loss: 1.0620, Train: 1.0322, Val: 1.0340\n",
      "Epoch: 302, Loss: 1.0612, Train: 1.0316, Val: 1.0334\n",
      "Epoch: 303, Loss: 1.0603, Train: 1.0311, Val: 1.0329\n",
      "Epoch: 304, Loss: 1.0596, Train: 1.0311, Val: 1.0328\n",
      "Epoch: 305, Loss: 1.0590, Train: 1.0307, Val: 1.0325\n",
      "Epoch: 306, Loss: 1.0582, Train: 1.0315, Val: 1.0331\n",
      "Epoch: 307, Loss: 1.0575, Train: 1.0308, Val: 1.0324\n",
      "Epoch: 308, Loss: 1.0567, Train: 1.0317, Val: 1.0333\n",
      "Epoch: 309, Loss: 1.0562, Train: 1.0300, Val: 1.0317\n",
      "Epoch: 310, Loss: 1.0558, Train: 1.0333, Val: 1.0346\n",
      "Epoch: 311, Loss: 1.0576, Train: 1.0289, Val: 1.0307\n",
      "Epoch: 312, Loss: 1.0575, Train: 1.0342, Val: 1.0355\n",
      "Epoch: 313, Loss: 1.0626, Train: 1.0276, Val: 1.0295\n",
      "Epoch: 314, Loss: 1.0545, Train: 1.0324, Val: 1.0347\n",
      "Epoch: 315, Loss: 1.0616, Train: 1.0320, Val: 1.0332\n",
      "Epoch: 316, Loss: 1.0592, Train: 1.0311, Val: 1.0324\n",
      "Epoch: 317, Loss: 1.0575, Train: 1.0305, Val: 1.0329\n",
      "Epoch: 318, Loss: 1.0597, Train: 1.0264, Val: 1.0282\n",
      "Epoch: 319, Loss: 1.0508, Train: 1.0280, Val: 1.0294\n",
      "Epoch: 320, Loss: 1.0536, Train: 1.0261, Val: 1.0280\n",
      "Epoch: 321, Loss: 1.0504, Train: 1.0285, Val: 1.0309\n",
      "Epoch: 322, Loss: 1.0519, Train: 1.0262, Val: 1.0277\n",
      "Epoch: 323, Loss: 1.0500, Train: 1.0260, Val: 1.0275\n",
      "Epoch: 324, Loss: 1.0503, Train: 1.0249, Val: 1.0269\n",
      "Epoch: 325, Loss: 1.0475, Train: 1.0259, Val: 1.0281\n",
      "Epoch: 326, Loss: 1.0486, Train: 1.0246, Val: 1.0262\n",
      "Epoch: 327, Loss: 1.0475, Train: 1.0240, Val: 1.0256\n",
      "Epoch: 328, Loss: 1.0453, Train: 1.0249, Val: 1.0270\n",
      "Epoch: 329, Loss: 1.0480, Train: 1.0252, Val: 1.0267\n",
      "Epoch: 330, Loss: 1.0482, Train: 1.0230, Val: 1.0246\n",
      "Epoch: 331, Loss: 1.0429, Train: 1.0249, Val: 1.0271\n",
      "Epoch: 332, Loss: 1.0489, Train: 1.0250, Val: 1.0266\n",
      "Epoch: 333, Loss: 1.0526, Train: 1.0223, Val: 1.0242\n",
      "Epoch: 334, Loss: 1.0467, Train: 1.0217, Val: 1.0237\n",
      "Epoch: 335, Loss: 1.0467, Train: 1.0227, Val: 1.0248\n",
      "Epoch: 336, Loss: 1.0486, Train: 1.0230, Val: 1.0246\n",
      "Epoch: 337, Loss: 1.0489, Train: 1.0237, Val: 1.0258\n",
      "Epoch: 338, Loss: 1.0451, Train: 1.0293, Val: 1.0319\n",
      "Epoch: 339, Loss: 1.0478, Train: 1.0257, Val: 1.0269\n",
      "Epoch: 340, Loss: 1.0524, Train: 1.0229, Val: 1.0244\n",
      "Epoch: 341, Loss: 1.0457, Train: 1.0269, Val: 1.0297\n",
      "Epoch: 342, Loss: 1.0500, Train: 1.0207, Val: 1.0230\n",
      "Epoch: 343, Loss: 1.0424, Train: 1.0232, Val: 1.0249\n",
      "Epoch: 344, Loss: 1.0457, Train: 1.0199, Val: 1.0217\n",
      "Epoch: 345, Loss: 1.0406, Train: 1.0208, Val: 1.0232\n",
      "Epoch: 346, Loss: 1.0408, Train: 1.0185, Val: 1.0206\n",
      "Epoch: 347, Loss: 1.0389, Train: 1.0190, Val: 1.0206\n",
      "Epoch: 348, Loss: 1.0378, Train: 1.0182, Val: 1.0200\n",
      "Epoch: 349, Loss: 1.0374, Train: 1.0180, Val: 1.0202\n",
      "Epoch: 350, Loss: 1.0371, Train: 1.0187, Val: 1.0205\n",
      "Epoch: 351, Loss: 1.0338, Train: 1.0248, Val: 1.0261\n",
      "Epoch: 352, Loss: 1.0436, Train: 1.0213, Val: 1.0232\n",
      "Epoch: 353, Loss: 1.0566, Train: 1.0316, Val: 1.0347\n",
      "Epoch: 354, Loss: 1.0540, Train: 1.0251, Val: 1.0274\n",
      "Epoch: 355, Loss: 1.0434, Train: 1.0278, Val: 1.0295\n",
      "Epoch: 356, Loss: 1.0533, Train: 1.0169, Val: 1.0191\n",
      "Epoch: 357, Loss: 1.0444, Train: 1.0248, Val: 1.0280\n",
      "Epoch: 358, Loss: 1.0455, Train: 1.0204, Val: 1.0225\n",
      "Epoch: 359, Loss: 1.0410, Train: 1.0256, Val: 1.0272\n",
      "Epoch: 360, Loss: 1.0435, Train: 1.0174, Val: 1.0196\n",
      "Epoch: 361, Loss: 1.0324, Train: 1.0244, Val: 1.0277\n",
      "Epoch: 362, Loss: 1.0428, Train: 1.0151, Val: 1.0175\n",
      "Epoch: 363, Loss: 1.0305, Train: 1.0216, Val: 1.0233\n",
      "Epoch: 364, Loss: 1.0355, Train: 1.0169, Val: 1.0188\n",
      "Epoch: 365, Loss: 1.0305, Train: 1.0151, Val: 1.0180\n",
      "Epoch: 366, Loss: 1.0351, Train: 1.0147, Val: 1.0175\n",
      "Epoch: 367, Loss: 1.0302, Train: 1.0152, Val: 1.0173\n",
      "Epoch: 368, Loss: 1.0310, Train: 1.0248, Val: 1.0266\n",
      "Epoch: 369, Loss: 1.0382, Train: 1.0139, Val: 1.0167\n",
      "Epoch: 370, Loss: 1.0237, Train: 1.0163, Val: 1.0193\n",
      "Epoch: 371, Loss: 1.0302, Train: 1.0306, Val: 1.0320\n",
      "Epoch: 372, Loss: 1.0660, Train: 1.0472, Val: 1.0506\n",
      "Epoch: 373, Loss: 1.0912, Train: 1.0337, Val: 1.0369\n",
      "Epoch: 374, Loss: 1.0506, Train: 1.0177, Val: 1.0203\n",
      "Epoch: 375, Loss: 1.0736, Train: 1.0185, Val: 1.0210\n",
      "Epoch: 376, Loss: 1.0344, Train: 1.0276, Val: 1.0300\n",
      "Epoch: 377, Loss: 1.0344, Train: 1.0299, Val: 1.0325\n",
      "Epoch: 378, Loss: 1.0355, Train: 1.0335, Val: 1.0367\n",
      "Epoch: 379, Loss: 1.0420, Train: 1.0429, Val: 1.0464\n",
      "Epoch: 380, Loss: 1.0358, Train: 1.0280, Val: 1.0304\n",
      "Epoch: 381, Loss: 1.0236, Train: 1.0243, Val: 1.0262\n",
      "Epoch: 382, Loss: 1.0378, Train: 1.0149, Val: 1.0176\n",
      "Epoch: 383, Loss: 1.0227, Train: 1.0312, Val: 1.0347\n",
      "Epoch: 384, Loss: 1.0278, Train: 1.0197, Val: 1.0227\n",
      "Epoch: 385, Loss: 1.0244, Train: 1.0113, Val: 1.0134\n",
      "Epoch: 386, Loss: 1.0206, Train: 1.0132, Val: 1.0151\n",
      "Epoch: 387, Loss: 1.0211, Train: 1.0085, Val: 1.0113\n",
      "Epoch: 388, Loss: 1.0159, Train: 1.0110, Val: 1.0141\n",
      "Epoch: 389, Loss: 1.0357, Train: 1.0826, Val: 1.0855\n",
      "Epoch: 390, Loss: 1.3036, Train: 1.2719, Val: 1.2752\n",
      "Epoch: 391, Loss: 1.2731, Train: 1.2494, Val: 1.2521\n",
      "Epoch: 392, Loss: 1.1046, Train: 1.1084, Val: 1.1108\n",
      "Epoch: 393, Loss: 1.1106, Train: 1.1132, Val: 1.1159\n",
      "Epoch: 394, Loss: 1.0836, Train: 1.2271, Val: 1.2301\n",
      "Epoch: 395, Loss: 1.0898, Train: 1.2427, Val: 1.2449\n",
      "Epoch: 396, Loss: 1.0932, Train: 1.1967, Val: 1.1980\n",
      "Epoch: 397, Loss: 1.1071, Train: 1.1898, Val: 1.1915\n",
      "Epoch: 398, Loss: 1.0683, Train: 1.1359, Val: 1.1378\n",
      "Epoch: 399, Loss: 1.0569, Train: 1.0734, Val: 1.0755\n",
      "Epoch: 400, Loss: 1.0671, Train: 1.0750, Val: 1.0776\n",
      "Epoch: 401, Loss: 1.0630, Train: 1.0856, Val: 1.0884\n",
      "Epoch: 402, Loss: 1.0627, Train: 1.0628, Val: 1.0652\n",
      "Epoch: 403, Loss: 1.0448, Train: 1.0411, Val: 1.0429\n",
      "Epoch: 404, Loss: 1.0499, Train: 1.0605, Val: 1.0625\n",
      "Epoch: 405, Loss: 1.0450, Train: 1.0670, Val: 1.0692\n",
      "Epoch: 406, Loss: 1.0535, Train: 1.0411, Val: 1.0431\n",
      "Epoch: 407, Loss: 1.0475, Train: 1.0293, Val: 1.0316\n",
      "Epoch: 408, Loss: 1.0372, Train: 1.0307, Val: 1.0334\n",
      "Epoch: 409, Loss: 1.0460, Train: 1.0223, Val: 1.0250\n",
      "Epoch: 410, Loss: 1.0447, Train: 1.0193, Val: 1.0219\n",
      "Epoch: 411, Loss: 1.0420, Train: 1.0228, Val: 1.0255\n",
      "Epoch: 412, Loss: 1.0307, Train: 1.0357, Val: 1.0384\n",
      "Epoch: 413, Loss: 1.0318, Train: 1.0441, Val: 1.0466\n",
      "Epoch: 414, Loss: 1.0364, Train: 1.0299, Val: 1.0319\n",
      "Epoch: 415, Loss: 1.0342, Train: 1.0234, Val: 1.0257\n",
      "Epoch: 416, Loss: 1.0232, Train: 1.0184, Val: 1.0212\n",
      "Epoch: 417, Loss: 1.0209, Train: 1.0162, Val: 1.0192\n",
      "Epoch: 418, Loss: 1.0241, Train: 1.0176, Val: 1.0207\n",
      "Epoch: 419, Loss: 1.0225, Train: 1.0157, Val: 1.0184\n",
      "Epoch: 420, Loss: 1.0131, Train: 1.0195, Val: 1.0218\n",
      "Epoch: 421, Loss: 1.0145, Train: 1.0246, Val: 1.0271\n",
      "Epoch: 422, Loss: 1.0138, Train: 1.0199, Val: 1.0227\n",
      "Epoch: 423, Loss: 1.0089, Train: 1.0161, Val: 1.0191\n",
      "Epoch: 424, Loss: 1.0105, Train: 1.0153, Val: 1.0183\n",
      "Epoch: 425, Loss: 1.0073, Train: 1.0111, Val: 1.0137\n",
      "Epoch: 426, Loss: 1.0052, Train: 1.0104, Val: 1.0127\n",
      "Epoch: 427, Loss: 1.0066, Train: 1.0090, Val: 1.0114\n",
      "Epoch: 428, Loss: 1.0042, Train: 1.0069, Val: 1.0097\n",
      "Epoch: 429, Loss: 1.0015, Train: 1.0050, Val: 1.0080\n",
      "Epoch: 430, Loss: 1.0009, Train: 1.0030, Val: 1.0058\n",
      "Epoch: 431, Loss: 0.9990, Train: 1.0035, Val: 1.0063\n",
      "Epoch: 432, Loss: 0.9983, Train: 1.0033, Val: 1.0059\n",
      "Epoch: 433, Loss: 0.9976, Train: 1.0018, Val: 1.0048\n",
      "Epoch: 434, Loss: 0.9960, Train: 1.0014, Val: 1.0041\n",
      "Epoch: 435, Loss: 0.9975, Train: 1.0065, Val: 1.0098\n",
      "Epoch: 436, Loss: 1.0071, Train: 1.0007, Val: 1.0034\n",
      "Epoch: 437, Loss: 0.9961, Train: 1.0098, Val: 1.0118\n",
      "Epoch: 438, Loss: 1.0104, Train: 1.0509, Val: 1.0551\n",
      "Epoch: 439, Loss: 1.0619, Train: 1.0051, Val: 1.0081\n",
      "Epoch: 440, Loss: 1.0049, Train: 1.0070, Val: 1.0090\n",
      "Epoch: 441, Loss: 1.0342, Train: 1.0028, Val: 1.0057\n",
      "Epoch: 442, Loss: 1.0024, Train: 1.0128, Val: 1.0157\n",
      "Epoch: 443, Loss: 1.0121, Train: 1.0134, Val: 1.0151\n",
      "Epoch: 444, Loss: 1.0088, Train: 1.0026, Val: 1.0058\n",
      "Epoch: 445, Loss: 1.0024, Train: 1.0181, Val: 1.0222\n",
      "Epoch: 446, Loss: 1.0116, Train: 1.0092, Val: 1.0134\n",
      "Epoch: 447, Loss: 1.0047, Train: 0.9982, Val: 1.0014\n",
      "Epoch: 448, Loss: 0.9942, Train: 1.0085, Val: 1.0107\n",
      "Epoch: 449, Loss: 0.9974, Train: 1.0013, Val: 1.0041\n",
      "Epoch: 450, Loss: 1.0046, Train: 0.9986, Val: 1.0014\n",
      "Epoch: 451, Loss: 0.9925, Train: 0.9941, Val: 0.9976\n",
      "Epoch: 452, Loss: 0.9918, Train: 1.0021, Val: 1.0064\n",
      "Epoch: 453, Loss: 0.9921, Train: 1.0001, Val: 1.0041\n",
      "Epoch: 454, Loss: 0.9899, Train: 0.9942, Val: 0.9973\n",
      "Epoch: 455, Loss: 0.9867, Train: 0.9992, Val: 1.0016\n",
      "Epoch: 456, Loss: 0.9892, Train: 0.9949, Val: 0.9984\n",
      "Epoch: 457, Loss: 0.9855, Train: 0.9951, Val: 0.9990\n",
      "Epoch: 458, Loss: 0.9850, Train: 0.9920, Val: 0.9953\n",
      "Epoch: 459, Loss: 0.9824, Train: 0.9921, Val: 0.9955\n",
      "Epoch: 460, Loss: 0.9828, Train: 0.9907, Val: 0.9942\n",
      "Epoch: 461, Loss: 0.9799, Train: 0.9966, Val: 0.9995\n",
      "Epoch: 462, Loss: 0.9803, Train: 0.9956, Val: 0.9986\n",
      "Epoch: 463, Loss: 0.9789, Train: 0.9892, Val: 0.9931\n",
      "Epoch: 464, Loss: 0.9768, Train: 0.9902, Val: 0.9942\n",
      "Epoch: 465, Loss: 0.9785, Train: 0.9887, Val: 0.9924\n",
      "Epoch: 466, Loss: 0.9750, Train: 0.9920, Val: 0.9953\n",
      "Epoch: 467, Loss: 0.9756, Train: 0.9898, Val: 0.9933\n",
      "Epoch: 468, Loss: 0.9752, Train: 0.9892, Val: 0.9925\n",
      "Epoch: 469, Loss: 0.9733, Train: 0.9868, Val: 0.9907\n",
      "Epoch: 470, Loss: 0.9724, Train: 0.9875, Val: 0.9921\n",
      "Epoch: 471, Loss: 0.9724, Train: 0.9858, Val: 0.9901\n",
      "Epoch: 472, Loss: 0.9700, Train: 0.9894, Val: 0.9928\n",
      "Epoch: 473, Loss: 0.9705, Train: 0.9855, Val: 0.9896\n",
      "Epoch: 474, Loss: 0.9689, Train: 0.9849, Val: 0.9897\n",
      "Epoch: 475, Loss: 0.9680, Train: 0.9839, Val: 0.9884\n",
      "Epoch: 476, Loss: 0.9677, Train: 0.9836, Val: 0.9879\n",
      "Epoch: 477, Loss: 0.9660, Train: 0.9837, Val: 0.9880\n",
      "Epoch: 478, Loss: 0.9661, Train: 0.9832, Val: 0.9874\n",
      "Epoch: 479, Loss: 0.9645, Train: 0.9824, Val: 0.9870\n",
      "Epoch: 480, Loss: 0.9641, Train: 0.9829, Val: 0.9876\n",
      "Epoch: 481, Loss: 0.9634, Train: 0.9820, Val: 0.9863\n",
      "Epoch: 482, Loss: 0.9622, Train: 0.9828, Val: 0.9869\n",
      "Epoch: 483, Loss: 0.9618, Train: 0.9812, Val: 0.9861\n",
      "Epoch: 484, Loss: 0.9609, Train: 0.9806, Val: 0.9853\n",
      "Epoch: 485, Loss: 0.9602, Train: 0.9809, Val: 0.9853\n",
      "Epoch: 486, Loss: 0.9589, Train: 0.9802, Val: 0.9849\n",
      "Epoch: 487, Loss: 0.9638, Train: 0.9903, Val: 0.9937\n",
      "Epoch: 488, Loss: 0.9764, Train: 1.0700, Val: 1.0766\n",
      "Epoch: 489, Loss: 1.0076, Train: 0.9960, Val: 1.0013\n",
      "Epoch: 490, Loss: 0.9853, Train: 1.0124, Val: 1.0145\n",
      "Epoch: 491, Loss: 0.9904, Train: 1.0011, Val: 1.0062\n",
      "Epoch: 492, Loss: 0.9758, Train: 1.0674, Val: 1.0730\n",
      "Epoch: 493, Loss: 0.9896, Train: 1.0731, Val: 1.0783\n",
      "Epoch: 494, Loss: 0.9765, Train: 1.0184, Val: 1.0232\n",
      "Epoch: 495, Loss: 0.9705, Train: 0.9946, Val: 0.9984\n",
      "Epoch: 496, Loss: 0.9818, Train: 1.0109, Val: 1.0136\n",
      "Epoch: 497, Loss: 0.9870, Train: 0.9905, Val: 0.9953\n",
      "Epoch: 498, Loss: 0.9617, Train: 1.0543, Val: 1.0604\n",
      "Epoch: 499, Loss: 0.9866, Train: 1.0058, Val: 1.0117\n",
      "Epoch: 500, Loss: 0.9759, Train: 0.9905, Val: 0.9944\n",
      "Test RMSE: 0.9996\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.494725       3.583673\n",
      "std      1727.484387     741.673176       0.500306       1.116938\n",
      "min         0.000000       0.000000       1.503089       1.000000\n",
      "25%      1500.000000     259.000000       3.154609       3.000000\n",
      "50%      3066.000000     693.000000       3.522085       4.000000\n",
      "75%      4472.000000    1292.000000       3.863722       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1097.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0167757087737\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  2\n",
      "Kernel:  spd\n",
      "Model Name:  KPGIN\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 12.7842, Train: 2.9094, Val: 2.9113\n",
      "Epoch: 002, Loss: 10.5313, Train: 2.6234, Val: 2.6257\n",
      "Epoch: 003, Loss: 7.1719, Train: 2.3273, Val: 2.3268\n",
      "Epoch: 004, Loss: 5.0017, Train: 1.9181, Val: 1.9149\n",
      "Epoch: 005, Loss: 3.0616, Train: 1.5473, Val: 1.5412\n",
      "Epoch: 006, Loss: 2.7355, Train: 1.4756, Val: 1.4701\n",
      "Epoch: 007, Loss: 1.8078, Train: 1.4576, Val: 1.4522\n",
      "Epoch: 008, Loss: 1.7212, Train: 1.4539, Val: 1.4485\n",
      "Epoch: 009, Loss: 1.6651, Train: 1.4616, Val: 1.4561\n",
      "Epoch: 010, Loss: 1.6118, Train: 1.4667, Val: 1.4611\n",
      "Epoch: 011, Loss: 1.4967, Train: 1.4722, Val: 1.4666\n",
      "Epoch: 012, Loss: 1.5688, Train: 1.4573, Val: 1.4519\n",
      "Epoch: 013, Loss: 1.4541, Train: 1.4372, Val: 1.4319\n",
      "Epoch: 014, Loss: 1.4030, Train: 1.4181, Val: 1.4139\n",
      "Epoch: 015, Loss: 1.3890, Train: 1.4164, Val: 1.4126\n",
      "Epoch: 016, Loss: 1.3907, Train: 1.3744, Val: 1.3715\n",
      "Epoch: 017, Loss: 1.3485, Train: 1.3482, Val: 1.3449\n",
      "Epoch: 018, Loss: 1.3127, Train: 1.3307, Val: 1.3271\n",
      "Epoch: 019, Loss: 1.2899, Train: 1.3316, Val: 1.3281\n",
      "Epoch: 020, Loss: 1.2793, Train: 1.3308, Val: 1.3273\n",
      "Epoch: 021, Loss: 1.2756, Train: 1.3180, Val: 1.3144\n",
      "Epoch: 022, Loss: 1.2705, Train: 1.2892, Val: 1.2852\n",
      "Epoch: 023, Loss: 1.2634, Train: 1.2440, Val: 1.2405\n",
      "Epoch: 024, Loss: 1.2569, Train: 1.1880, Val: 1.1861\n",
      "Epoch: 025, Loss: 1.2531, Train: 1.1491, Val: 1.1483\n",
      "Epoch: 026, Loss: 1.2511, Train: 1.1318, Val: 1.1317\n",
      "Epoch: 027, Loss: 1.2495, Train: 1.1278, Val: 1.1280\n",
      "Epoch: 028, Loss: 1.2510, Train: 1.1214, Val: 1.1217\n",
      "Epoch: 029, Loss: 1.2630, Train: 1.1266, Val: 1.1272\n",
      "Epoch: 030, Loss: 1.2513, Train: 1.1414, Val: 1.1422\n",
      "Epoch: 031, Loss: 1.2623, Train: 1.1362, Val: 1.1370\n",
      "Epoch: 032, Loss: 1.2527, Train: 1.1237, Val: 1.1243\n",
      "Epoch: 033, Loss: 1.2577, Train: 1.1243, Val: 1.1250\n",
      "Epoch: 034, Loss: 1.2561, Train: 1.1362, Val: 1.1370\n",
      "Epoch: 035, Loss: 1.2499, Train: 1.1455, Val: 1.1463\n",
      "Epoch: 036, Loss: 1.2579, Train: 1.1359, Val: 1.1367\n",
      "Epoch: 037, Loss: 1.2504, Train: 1.1230, Val: 1.1235\n",
      "Epoch: 038, Loss: 1.2503, Train: 1.1196, Val: 1.1200\n",
      "Epoch: 039, Loss: 1.2533, Train: 1.1236, Val: 1.1242\n",
      "Epoch: 040, Loss: 1.2470, Train: 1.1306, Val: 1.1312\n",
      "Epoch: 041, Loss: 1.2498, Train: 1.1287, Val: 1.1292\n",
      "Epoch: 042, Loss: 1.2491, Train: 1.1202, Val: 1.1207\n",
      "Epoch: 043, Loss: 1.2451, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 044, Loss: 1.2484, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 045, Loss: 1.2444, Train: 1.1195, Val: 1.1199\n",
      "Epoch: 046, Loss: 1.2448, Train: 1.1189, Val: 1.1193\n",
      "Epoch: 047, Loss: 1.2433, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 048, Loss: 1.2415, Train: 1.1148, Val: 1.1151\n",
      "Epoch: 049, Loss: 1.2421, Train: 1.1156, Val: 1.1159\n",
      "Epoch: 050, Loss: 1.2389, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 051, Loss: 1.2383, Train: 1.1154, Val: 1.1158\n",
      "Epoch: 052, Loss: 1.2322, Train: 1.1152, Val: 1.1155\n",
      "Epoch: 053, Loss: 1.2771, Train: 1.1501, Val: 1.1510\n",
      "Epoch: 054, Loss: 1.2896, Train: 1.1424, Val: 1.1432\n",
      "Epoch: 055, Loss: 1.2366, Train: 1.1189, Val: 1.1193\n",
      "Epoch: 056, Loss: 1.2891, Train: 1.1208, Val: 1.1206\n",
      "Epoch: 057, Loss: 1.3810, Train: 1.1104, Val: 1.1106\n",
      "Epoch: 058, Loss: 1.2966, Train: 1.1397, Val: 1.1406\n",
      "Epoch: 059, Loss: 1.2446, Train: 1.1961, Val: 1.1974\n",
      "Epoch: 060, Loss: 1.3374, Train: 1.1619, Val: 1.1629\n",
      "Epoch: 061, Loss: 1.2840, Train: 1.1092, Val: 1.1096\n",
      "Epoch: 062, Loss: 1.2276, Train: 1.1178, Val: 1.1179\n",
      "Epoch: 063, Loss: 1.2775, Train: 1.1104, Val: 1.1105\n",
      "Epoch: 064, Loss: 1.2262, Train: 1.1062, Val: 1.1065\n",
      "Epoch: 065, Loss: 1.2285, Train: 1.1056, Val: 1.1059\n",
      "Epoch: 066, Loss: 1.2231, Train: 1.1050, Val: 1.1053\n",
      "Epoch: 067, Loss: 1.2235, Train: 1.1044, Val: 1.1046\n",
      "Epoch: 068, Loss: 1.2241, Train: 1.1036, Val: 1.1039\n",
      "Epoch: 069, Loss: 1.2194, Train: 1.1047, Val: 1.1050\n",
      "Epoch: 070, Loss: 1.2147, Train: 1.1055, Val: 1.1058\n",
      "Epoch: 071, Loss: 1.2165, Train: 1.1015, Val: 1.1016\n",
      "Epoch: 072, Loss: 1.2087, Train: 1.1006, Val: 1.1005\n",
      "Epoch: 073, Loss: 1.2103, Train: 1.1023, Val: 1.1023\n",
      "Epoch: 074, Loss: 1.2000, Train: 1.1044, Val: 1.1044\n",
      "Epoch: 075, Loss: 1.2057, Train: 1.0998, Val: 1.0997\n",
      "Epoch: 076, Loss: 1.2006, Train: 1.1006, Val: 1.1007\n",
      "Epoch: 077, Loss: 1.1971, Train: 1.1102, Val: 1.1105\n",
      "Epoch: 078, Loss: 1.1945, Train: 1.1143, Val: 1.1147\n",
      "Epoch: 079, Loss: 1.1965, Train: 1.1052, Val: 1.1055\n",
      "Epoch: 080, Loss: 1.1910, Train: 1.1006, Val: 1.1008\n",
      "Epoch: 081, Loss: 1.1940, Train: 1.1065, Val: 1.1068\n",
      "Epoch: 082, Loss: 1.1884, Train: 1.1112, Val: 1.1116\n",
      "Epoch: 083, Loss: 1.1902, Train: 1.1039, Val: 1.1042\n",
      "Epoch: 084, Loss: 1.1864, Train: 1.0954, Val: 1.0957\n",
      "Epoch: 085, Loss: 1.1871, Train: 1.0939, Val: 1.0941\n",
      "Epoch: 086, Loss: 1.1853, Train: 1.0962, Val: 1.0965\n",
      "Epoch: 087, Loss: 1.1838, Train: 1.0953, Val: 1.0956\n",
      "Epoch: 088, Loss: 1.1840, Train: 1.0904, Val: 1.0906\n",
      "Epoch: 089, Loss: 1.1816, Train: 1.0879, Val: 1.0880\n",
      "Epoch: 090, Loss: 1.1822, Train: 1.0878, Val: 1.0879\n",
      "Epoch: 091, Loss: 1.1802, Train: 1.0892, Val: 1.0893\n",
      "Epoch: 092, Loss: 1.1801, Train: 1.0889, Val: 1.0890\n",
      "Epoch: 093, Loss: 1.1784, Train: 1.0870, Val: 1.0871\n",
      "Epoch: 094, Loss: 1.1776, Train: 1.0864, Val: 1.0864\n",
      "Epoch: 095, Loss: 1.1771, Train: 1.0873, Val: 1.0873\n",
      "Epoch: 096, Loss: 1.1759, Train: 1.0878, Val: 1.0879\n",
      "Epoch: 097, Loss: 1.1753, Train: 1.0863, Val: 1.0863\n",
      "Epoch: 098, Loss: 1.1741, Train: 1.0849, Val: 1.0848\n",
      "Epoch: 099, Loss: 1.1738, Train: 1.0853, Val: 1.0853\n",
      "Epoch: 100, Loss: 1.1724, Train: 1.0861, Val: 1.0861\n",
      "Epoch: 101, Loss: 1.1718, Train: 1.0848, Val: 1.0848\n",
      "Epoch: 102, Loss: 1.1707, Train: 1.0832, Val: 1.0831\n",
      "Epoch: 103, Loss: 1.1701, Train: 1.0831, Val: 1.0830\n",
      "Epoch: 104, Loss: 1.1690, Train: 1.0839, Val: 1.0839\n",
      "Epoch: 105, Loss: 1.1684, Train: 1.0832, Val: 1.0831\n",
      "Epoch: 106, Loss: 1.1675, Train: 1.0818, Val: 1.0817\n",
      "Epoch: 107, Loss: 1.1667, Train: 1.0814, Val: 1.0814\n",
      "Epoch: 108, Loss: 1.1658, Train: 1.0816, Val: 1.0815\n",
      "Epoch: 109, Loss: 1.1651, Train: 1.0808, Val: 1.0808\n",
      "Epoch: 110, Loss: 1.1643, Train: 1.0800, Val: 1.0799\n",
      "Epoch: 111, Loss: 1.1635, Train: 1.0800, Val: 1.0800\n",
      "Epoch: 112, Loss: 1.1627, Train: 1.0804, Val: 1.0803\n",
      "Epoch: 113, Loss: 1.1620, Train: 1.0798, Val: 1.0797\n",
      "Epoch: 114, Loss: 1.1613, Train: 1.0794, Val: 1.0793\n",
      "Epoch: 115, Loss: 1.1605, Train: 1.0791, Val: 1.0791\n",
      "Epoch: 116, Loss: 1.1598, Train: 1.0787, Val: 1.0786\n",
      "Epoch: 117, Loss: 1.1590, Train: 1.0784, Val: 1.0783\n",
      "Epoch: 118, Loss: 1.1582, Train: 1.0787, Val: 1.0787\n",
      "Epoch: 119, Loss: 1.1575, Train: 1.0786, Val: 1.0786\n",
      "Epoch: 120, Loss: 1.1567, Train: 1.0777, Val: 1.0777\n",
      "Epoch: 121, Loss: 1.1559, Train: 1.0779, Val: 1.0779\n",
      "Epoch: 122, Loss: 1.1550, Train: 1.0779, Val: 1.0779\n",
      "Epoch: 123, Loss: 1.1541, Train: 1.0775, Val: 1.0775\n",
      "Epoch: 124, Loss: 1.1533, Train: 1.0772, Val: 1.0772\n",
      "Epoch: 125, Loss: 1.1527, Train: 1.0769, Val: 1.0769\n",
      "Epoch: 126, Loss: 1.1515, Train: 1.0772, Val: 1.0772\n",
      "Epoch: 127, Loss: 1.1520, Train: 1.0758, Val: 1.0758\n",
      "Epoch: 128, Loss: 1.1521, Train: 1.0765, Val: 1.0765\n",
      "Epoch: 129, Loss: 1.1517, Train: 1.0765, Val: 1.0765\n",
      "Epoch: 130, Loss: 1.1488, Train: 1.0753, Val: 1.0752\n",
      "Epoch: 131, Loss: 1.1770, Train: 1.0776, Val: 1.0773\n",
      "Epoch: 132, Loss: 1.1945, Train: 1.0802, Val: 1.0805\n",
      "Epoch: 133, Loss: 1.1584, Train: 1.0970, Val: 1.0975\n",
      "Epoch: 134, Loss: 1.1834, Train: 1.0763, Val: 1.0765\n",
      "Epoch: 135, Loss: 1.1508, Train: 1.0755, Val: 1.0754\n",
      "Epoch: 136, Loss: 1.1549, Train: 1.1032, Val: 1.1028\n",
      "Epoch: 137, Loss: 1.1751, Train: 1.0810, Val: 1.0808\n",
      "Epoch: 138, Loss: 1.1487, Train: 1.0749, Val: 1.0751\n",
      "Epoch: 139, Loss: 1.1645, Train: 1.0723, Val: 1.0723\n",
      "Epoch: 140, Loss: 1.1507, Train: 1.0784, Val: 1.0780\n",
      "Epoch: 141, Loss: 1.1539, Train: 1.0731, Val: 1.0727\n",
      "Epoch: 142, Loss: 1.1464, Train: 1.0711, Val: 1.0709\n",
      "Epoch: 143, Loss: 1.1495, Train: 1.0709, Val: 1.0707\n",
      "Epoch: 144, Loss: 1.1478, Train: 1.0708, Val: 1.0703\n",
      "Epoch: 145, Loss: 1.1449, Train: 1.0716, Val: 1.0711\n",
      "Epoch: 146, Loss: 1.1512, Train: 1.0716, Val: 1.0716\n",
      "Epoch: 147, Loss: 1.1437, Train: 1.0762, Val: 1.0763\n",
      "Epoch: 148, Loss: 1.1500, Train: 1.0685, Val: 1.0684\n",
      "Epoch: 149, Loss: 1.1413, Train: 1.0689, Val: 1.0687\n",
      "Epoch: 150, Loss: 1.1452, Train: 1.0729, Val: 1.0729\n",
      "Epoch: 151, Loss: 1.1411, Train: 1.0774, Val: 1.0776\n",
      "Epoch: 152, Loss: 1.1443, Train: 1.0698, Val: 1.0699\n",
      "Epoch: 153, Loss: 1.1388, Train: 1.0685, Val: 1.0686\n",
      "Epoch: 154, Loss: 1.1404, Train: 1.0747, Val: 1.0751\n",
      "Epoch: 155, Loss: 1.1387, Train: 1.0759, Val: 1.0763\n",
      "Epoch: 156, Loss: 1.1393, Train: 1.0686, Val: 1.0687\n",
      "Epoch: 157, Loss: 1.1356, Train: 1.0676, Val: 1.0675\n",
      "Epoch: 158, Loss: 1.1370, Train: 1.0714, Val: 1.0715\n",
      "Epoch: 159, Loss: 1.1449, Train: 1.0659, Val: 1.0659\n",
      "Epoch: 160, Loss: 1.1389, Train: 1.0691, Val: 1.0693\n",
      "Epoch: 161, Loss: 1.1337, Train: 1.0754, Val: 1.0756\n",
      "Epoch: 162, Loss: 1.1379, Train: 1.0675, Val: 1.0676\n",
      "Epoch: 163, Loss: 1.1314, Train: 1.0652, Val: 1.0651\n",
      "Epoch: 164, Loss: 1.1353, Train: 1.0679, Val: 1.0679\n",
      "Epoch: 165, Loss: 1.1319, Train: 1.0700, Val: 1.0700\n",
      "Epoch: 166, Loss: 1.1327, Train: 1.0643, Val: 1.0642\n",
      "Epoch: 167, Loss: 1.1300, Train: 1.0640, Val: 1.0639\n",
      "Epoch: 168, Loss: 1.1306, Train: 1.0683, Val: 1.0683\n",
      "Epoch: 169, Loss: 1.1295, Train: 1.0662, Val: 1.0662\n",
      "Epoch: 170, Loss: 1.1273, Train: 1.0638, Val: 1.0636\n",
      "Epoch: 171, Loss: 1.1292, Train: 1.0652, Val: 1.0652\n",
      "Epoch: 172, Loss: 1.1258, Train: 1.0677, Val: 1.0678\n",
      "Epoch: 173, Loss: 1.1264, Train: 1.0630, Val: 1.0630\n",
      "Epoch: 174, Loss: 1.1251, Train: 1.0625, Val: 1.0625\n",
      "Epoch: 175, Loss: 1.1240, Train: 1.0662, Val: 1.0663\n",
      "Epoch: 176, Loss: 1.1235, Train: 1.0644, Val: 1.0645\n",
      "Epoch: 177, Loss: 1.1223, Train: 1.0620, Val: 1.0622\n",
      "Epoch: 178, Loss: 1.1218, Train: 1.0648, Val: 1.0650\n",
      "Epoch: 179, Loss: 1.1214, Train: 1.0627, Val: 1.0629\n",
      "Epoch: 180, Loss: 1.1198, Train: 1.0607, Val: 1.0609\n",
      "Epoch: 181, Loss: 1.1199, Train: 1.0646, Val: 1.0649\n",
      "Epoch: 182, Loss: 1.1195, Train: 1.0610, Val: 1.0612\n",
      "Epoch: 183, Loss: 1.1169, Train: 1.0589, Val: 1.0591\n",
      "Epoch: 184, Loss: 1.1173, Train: 1.0626, Val: 1.0629\n",
      "Epoch: 185, Loss: 1.1169, Train: 1.0593, Val: 1.0595\n",
      "Epoch: 186, Loss: 1.1144, Train: 1.0587, Val: 1.0589\n",
      "Epoch: 187, Loss: 1.1143, Train: 1.0605, Val: 1.0607\n",
      "Epoch: 188, Loss: 1.1139, Train: 1.0575, Val: 1.0577\n",
      "Epoch: 189, Loss: 1.1121, Train: 1.0576, Val: 1.0578\n",
      "Epoch: 190, Loss: 1.1107, Train: 1.0586, Val: 1.0588\n",
      "Epoch: 191, Loss: 1.1113, Train: 1.0570, Val: 1.0571\n",
      "Epoch: 192, Loss: 1.1126, Train: 1.0567, Val: 1.0570\n",
      "Epoch: 193, Loss: 1.1254, Train: 1.0584, Val: 1.0589\n",
      "Epoch: 194, Loss: 1.1228, Train: 1.0592, Val: 1.0598\n",
      "Epoch: 195, Loss: 1.1114, Train: 1.0646, Val: 1.0653\n",
      "Epoch: 196, Loss: 1.3600, Train: 1.2400, Val: 1.2392\n",
      "Epoch: 197, Loss: 1.7941, Train: 1.1026, Val: 1.1031\n",
      "Epoch: 198, Loss: 1.1596, Train: 1.3526, Val: 1.3542\n",
      "Epoch: 199, Loss: 1.5291, Train: 1.2815, Val: 1.2830\n",
      "Epoch: 200, Loss: 1.2175, Train: 1.1341, Val: 1.1351\n",
      "Epoch: 201, Loss: 1.4851, Train: 1.2317, Val: 1.2331\n",
      "Epoch: 202, Loss: 1.2151, Train: 1.4136, Val: 1.4155\n",
      "Epoch: 203, Loss: 1.3104, Train: 1.4093, Val: 1.4111\n",
      "Epoch: 204, Loss: 1.3632, Train: 1.2185, Val: 1.2197\n",
      "Epoch: 205, Loss: 1.1869, Train: 1.0902, Val: 1.0905\n",
      "Epoch: 206, Loss: 1.3143, Train: 1.0915, Val: 1.0919\n",
      "Epoch: 207, Loss: 1.2280, Train: 1.1589, Val: 1.1597\n",
      "Epoch: 208, Loss: 1.1825, Train: 1.2080, Val: 1.2091\n",
      "Epoch: 209, Loss: 1.3172, Train: 1.1233, Val: 1.1238\n",
      "Epoch: 210, Loss: 1.1782, Train: 1.0726, Val: 1.0723\n",
      "Epoch: 211, Loss: 1.2398, Train: 1.0730, Val: 1.0726\n",
      "Epoch: 212, Loss: 1.2313, Train: 1.0961, Val: 1.0965\n",
      "Epoch: 213, Loss: 1.1483, Train: 1.1551, Val: 1.1560\n",
      "Epoch: 214, Loss: 1.2348, Train: 1.1130, Val: 1.1135\n",
      "Epoch: 215, Loss: 1.1679, Train: 1.0716, Val: 1.0713\n",
      "Epoch: 216, Loss: 1.1543, Train: 1.0883, Val: 1.0877\n",
      "Epoch: 217, Loss: 1.1822, Train: 1.0810, Val: 1.0806\n",
      "Epoch: 218, Loss: 1.1599, Train: 1.0708, Val: 1.0707\n",
      "Epoch: 219, Loss: 1.1496, Train: 1.0772, Val: 1.0774\n",
      "Epoch: 220, Loss: 1.1655, Train: 1.0706, Val: 1.0707\n",
      "Epoch: 221, Loss: 1.1475, Train: 1.0708, Val: 1.0707\n",
      "Epoch: 222, Loss: 1.1490, Train: 1.0740, Val: 1.0738\n",
      "Epoch: 223, Loss: 1.1583, Train: 1.0682, Val: 1.0682\n",
      "Epoch: 224, Loss: 1.1413, Train: 1.0736, Val: 1.0738\n",
      "Epoch: 225, Loss: 1.1453, Train: 1.0722, Val: 1.0723\n",
      "Epoch: 226, Loss: 1.1408, Train: 1.0658, Val: 1.0656\n",
      "Epoch: 227, Loss: 1.1341, Train: 1.0673, Val: 1.0670\n",
      "Epoch: 228, Loss: 1.1425, Train: 1.0655, Val: 1.0653\n",
      "Epoch: 229, Loss: 1.1333, Train: 1.0654, Val: 1.0654\n",
      "Epoch: 230, Loss: 1.1311, Train: 1.0649, Val: 1.0649\n",
      "Epoch: 231, Loss: 1.1278, Train: 1.0640, Val: 1.0639\n",
      "Epoch: 232, Loss: 1.1265, Train: 1.0643, Val: 1.0642\n",
      "Epoch: 233, Loss: 1.1276, Train: 1.0649, Val: 1.0649\n",
      "Epoch: 234, Loss: 1.1238, Train: 1.0665, Val: 1.0665\n",
      "Epoch: 235, Loss: 1.1296, Train: 1.0632, Val: 1.0630\n",
      "Epoch: 236, Loss: 1.1264, Train: 1.0635, Val: 1.0632\n",
      "Epoch: 237, Loss: 1.1232, Train: 1.0713, Val: 1.0713\n",
      "Epoch: 238, Loss: 1.1246, Train: 1.0704, Val: 1.0703\n",
      "Epoch: 239, Loss: 1.1243, Train: 1.0650, Val: 1.0649\n",
      "Epoch: 240, Loss: 1.1218, Train: 1.0624, Val: 1.0622\n",
      "Epoch: 241, Loss: 1.1205, Train: 1.0652, Val: 1.0651\n",
      "Epoch: 242, Loss: 1.1169, Train: 1.0778, Val: 1.0780\n",
      "Epoch: 243, Loss: 1.1200, Train: 1.0760, Val: 1.0763\n",
      "Epoch: 244, Loss: 1.1168, Train: 1.0648, Val: 1.0649\n",
      "Epoch: 245, Loss: 1.1153, Train: 1.0636, Val: 1.0637\n",
      "Epoch: 246, Loss: 1.1158, Train: 1.0705, Val: 1.0708\n",
      "Epoch: 247, Loss: 1.1127, Train: 1.0750, Val: 1.0754\n",
      "Epoch: 248, Loss: 1.1149, Train: 1.0684, Val: 1.0687\n",
      "Epoch: 249, Loss: 1.1104, Train: 1.0620, Val: 1.0623\n",
      "Epoch: 250, Loss: 1.1124, Train: 1.0640, Val: 1.0643\n",
      "Epoch: 251, Loss: 1.1094, Train: 1.0688, Val: 1.0692\n",
      "Epoch: 252, Loss: 1.1100, Train: 1.0658, Val: 1.0662\n",
      "Epoch: 253, Loss: 1.1088, Train: 1.0611, Val: 1.0615\n",
      "Epoch: 254, Loss: 1.1078, Train: 1.0604, Val: 1.0607\n",
      "Epoch: 255, Loss: 1.1272, Train: 1.0580, Val: 1.0580\n",
      "Epoch: 256, Loss: 1.1526, Train: 1.0593, Val: 1.0596\n",
      "Epoch: 257, Loss: 1.1156, Train: 1.0813, Val: 1.0819\n",
      "Epoch: 258, Loss: 1.1356, Train: 1.0711, Val: 1.0717\n",
      "Epoch: 259, Loss: 1.1181, Train: 1.0583, Val: 1.0586\n",
      "Epoch: 260, Loss: 1.1162, Train: 1.0580, Val: 1.0581\n",
      "Epoch: 261, Loss: 1.1253, Train: 1.0577, Val: 1.0581\n",
      "Epoch: 262, Loss: 1.1173, Train: 1.0577, Val: 1.0581\n",
      "Epoch: 263, Loss: 1.1117, Train: 1.0588, Val: 1.0593\n",
      "Epoch: 264, Loss: 1.1193, Train: 1.0582, Val: 1.0587\n",
      "Epoch: 265, Loss: 1.1164, Train: 1.0562, Val: 1.0567\n",
      "Epoch: 266, Loss: 1.1102, Train: 1.0561, Val: 1.0564\n",
      "Epoch: 267, Loss: 1.1086, Train: 1.0567, Val: 1.0569\n",
      "Epoch: 268, Loss: 1.1218, Train: 1.0554, Val: 1.0557\n",
      "Epoch: 269, Loss: 1.1068, Train: 1.0561, Val: 1.0566\n",
      "Epoch: 270, Loss: 1.1071, Train: 1.0582, Val: 1.0588\n",
      "Epoch: 271, Loss: 1.1105, Train: 1.0581, Val: 1.0586\n",
      "Epoch: 272, Loss: 1.1092, Train: 1.0544, Val: 1.0548\n",
      "Epoch: 273, Loss: 1.1050, Train: 1.0541, Val: 1.0544\n",
      "Epoch: 274, Loss: 1.1050, Train: 1.0560, Val: 1.0564\n",
      "Epoch: 275, Loss: 1.1058, Train: 1.0581, Val: 1.0586\n",
      "Epoch: 276, Loss: 1.1046, Train: 1.0573, Val: 1.0578\n",
      "Epoch: 277, Loss: 1.1028, Train: 1.0546, Val: 1.0551\n",
      "Epoch: 278, Loss: 1.1025, Train: 1.0536, Val: 1.0540\n",
      "Epoch: 279, Loss: 1.1025, Train: 1.0538, Val: 1.0542\n",
      "Epoch: 280, Loss: 1.1018, Train: 1.0549, Val: 1.0553\n",
      "Epoch: 281, Loss: 1.1004, Train: 1.0572, Val: 1.0577\n",
      "Epoch: 282, Loss: 1.1033, Train: 1.0638, Val: 1.0644\n",
      "Epoch: 283, Loss: 1.1145, Train: 1.0547, Val: 1.0551\n",
      "Epoch: 284, Loss: 1.1068, Train: 1.0520, Val: 1.0521\n",
      "Epoch: 285, Loss: 1.1121, Train: 1.0506, Val: 1.0509\n",
      "Epoch: 286, Loss: 1.1049, Train: 1.0538, Val: 1.0543\n",
      "Epoch: 287, Loss: 1.1059, Train: 1.0531, Val: 1.0537\n",
      "Epoch: 288, Loss: 1.1043, Train: 1.0504, Val: 1.0506\n",
      "Epoch: 289, Loss: 1.1030, Train: 1.0506, Val: 1.0507\n",
      "Epoch: 290, Loss: 1.1057, Train: 1.0501, Val: 1.0504\n",
      "Epoch: 291, Loss: 1.1014, Train: 1.0529, Val: 1.0534\n",
      "Epoch: 292, Loss: 1.1035, Train: 1.0498, Val: 1.0502\n",
      "Epoch: 293, Loss: 1.0998, Train: 1.0486, Val: 1.0488\n",
      "Epoch: 294, Loss: 1.1000, Train: 1.0491, Val: 1.0492\n",
      "Epoch: 295, Loss: 1.0999, Train: 1.0502, Val: 1.0505\n",
      "Epoch: 296, Loss: 1.0992, Train: 1.0513, Val: 1.0516\n",
      "Epoch: 297, Loss: 1.0999, Train: 1.0490, Val: 1.0493\n",
      "Epoch: 298, Loss: 1.0971, Train: 1.0479, Val: 1.0481\n",
      "Epoch: 299, Loss: 1.0979, Train: 1.0476, Val: 1.0479\n",
      "Epoch: 300, Loss: 1.0967, Train: 1.0492, Val: 1.0496\n",
      "Epoch: 301, Loss: 1.0969, Train: 1.0492, Val: 1.0496\n",
      "Epoch: 302, Loss: 1.0963, Train: 1.0476, Val: 1.0480\n",
      "Epoch: 303, Loss: 1.0953, Train: 1.0471, Val: 1.0474\n",
      "Epoch: 304, Loss: 1.0953, Train: 1.0475, Val: 1.0479\n",
      "Epoch: 305, Loss: 1.0948, Train: 1.0480, Val: 1.0485\n",
      "Epoch: 306, Loss: 1.0949, Train: 1.0468, Val: 1.0472\n",
      "Epoch: 307, Loss: 1.0935, Train: 1.0463, Val: 1.0466\n",
      "Epoch: 308, Loss: 1.0937, Train: 1.0461, Val: 1.0465\n",
      "Epoch: 309, Loss: 1.0929, Train: 1.0467, Val: 1.0472\n",
      "Epoch: 310, Loss: 1.0931, Train: 1.0459, Val: 1.0462\n",
      "Epoch: 311, Loss: 1.0923, Train: 1.0452, Val: 1.0455\n",
      "Epoch: 312, Loss: 1.0921, Train: 1.0453, Val: 1.0456\n",
      "Epoch: 313, Loss: 1.0915, Train: 1.0458, Val: 1.0461\n",
      "Epoch: 314, Loss: 1.0914, Train: 1.0455, Val: 1.0458\n",
      "Epoch: 315, Loss: 1.0908, Train: 1.0448, Val: 1.0450\n",
      "Epoch: 316, Loss: 1.0904, Train: 1.0446, Val: 1.0449\n",
      "Epoch: 317, Loss: 1.0903, Train: 1.0451, Val: 1.0455\n",
      "Epoch: 318, Loss: 1.0899, Train: 1.0448, Val: 1.0451\n",
      "Epoch: 319, Loss: 1.0896, Train: 1.0443, Val: 1.0445\n",
      "Epoch: 320, Loss: 1.0893, Train: 1.0440, Val: 1.0443\n",
      "Epoch: 321, Loss: 1.0888, Train: 1.0441, Val: 1.0444\n",
      "Epoch: 322, Loss: 1.0887, Train: 1.0439, Val: 1.0441\n",
      "Epoch: 323, Loss: 1.0883, Train: 1.0439, Val: 1.0442\n",
      "Epoch: 324, Loss: 1.0880, Train: 1.0438, Val: 1.0441\n",
      "Epoch: 325, Loss: 1.0877, Train: 1.0433, Val: 1.0435\n",
      "Epoch: 326, Loss: 1.0873, Train: 1.0432, Val: 1.0433\n",
      "Epoch: 327, Loss: 1.0870, Train: 1.0433, Val: 1.0436\n",
      "Epoch: 328, Loss: 1.0867, Train: 1.0432, Val: 1.0435\n",
      "Epoch: 329, Loss: 1.0865, Train: 1.0429, Val: 1.0431\n",
      "Epoch: 330, Loss: 1.0862, Train: 1.0428, Val: 1.0430\n",
      "Epoch: 331, Loss: 1.0860, Train: 1.0428, Val: 1.0431\n",
      "Epoch: 332, Loss: 1.0872, Train: 1.0439, Val: 1.0438\n",
      "Epoch: 333, Loss: 1.0916, Train: 1.0427, Val: 1.0430\n",
      "Epoch: 334, Loss: 1.0880, Train: 1.0448, Val: 1.0453\n",
      "Epoch: 335, Loss: 1.0890, Train: 1.0432, Val: 1.0433\n",
      "Epoch: 336, Loss: 1.0874, Train: 1.0425, Val: 1.0425\n",
      "Epoch: 337, Loss: 1.0878, Train: 1.0436, Val: 1.0439\n",
      "Epoch: 338, Loss: 1.0881, Train: 1.0425, Val: 1.0427\n",
      "Epoch: 339, Loss: 1.0855, Train: 1.0430, Val: 1.0430\n",
      "Epoch: 340, Loss: 1.0866, Train: 1.0435, Val: 1.0437\n",
      "Epoch: 341, Loss: 1.0862, Train: 1.0440, Val: 1.0443\n",
      "Epoch: 342, Loss: 1.0860, Train: 1.0421, Val: 1.0423\n",
      "Epoch: 343, Loss: 1.0841, Train: 1.0419, Val: 1.0420\n",
      "Epoch: 344, Loss: 1.0856, Train: 1.0418, Val: 1.0422\n",
      "Epoch: 345, Loss: 1.0845, Train: 1.0420, Val: 1.0424\n",
      "Epoch: 346, Loss: 1.0835, Train: 1.0419, Val: 1.0422\n",
      "Epoch: 347, Loss: 1.0839, Train: 1.0418, Val: 1.0421\n",
      "Epoch: 348, Loss: 1.0835, Train: 1.0418, Val: 1.0423\n",
      "Epoch: 349, Loss: 1.0830, Train: 1.0409, Val: 1.0411\n",
      "Epoch: 350, Loss: 1.0824, Train: 1.0406, Val: 1.0408\n",
      "Epoch: 351, Loss: 1.0825, Train: 1.0408, Val: 1.0412\n",
      "Epoch: 352, Loss: 1.0822, Train: 1.0405, Val: 1.0408\n",
      "Epoch: 353, Loss: 1.0815, Train: 1.0406, Val: 1.0408\n",
      "Epoch: 354, Loss: 1.0815, Train: 1.0406, Val: 1.0410\n",
      "Epoch: 355, Loss: 1.0813, Train: 1.0400, Val: 1.0403\n",
      "Epoch: 356, Loss: 1.0806, Train: 1.0400, Val: 1.0402\n",
      "Epoch: 357, Loss: 1.0807, Train: 1.0399, Val: 1.0402\n",
      "Epoch: 358, Loss: 1.0804, Train: 1.0399, Val: 1.0401\n",
      "Epoch: 359, Loss: 1.0801, Train: 1.0398, Val: 1.0401\n",
      "Epoch: 360, Loss: 1.0799, Train: 1.0397, Val: 1.0399\n",
      "Epoch: 361, Loss: 1.0794, Train: 1.0395, Val: 1.0397\n",
      "Epoch: 362, Loss: 1.0791, Train: 1.0394, Val: 1.0396\n",
      "Epoch: 363, Loss: 1.0792, Train: 1.0398, Val: 1.0399\n",
      "Epoch: 364, Loss: 1.0798, Train: 1.0395, Val: 1.0398\n",
      "Epoch: 365, Loss: 1.0799, Train: 1.0401, Val: 1.0401\n",
      "Epoch: 366, Loss: 1.0809, Train: 1.0391, Val: 1.0395\n",
      "Epoch: 367, Loss: 1.0785, Train: 1.0391, Val: 1.0395\n",
      "Epoch: 368, Loss: 1.0788, Train: 1.0397, Val: 1.0396\n",
      "Epoch: 369, Loss: 1.0806, Train: 1.0389, Val: 1.0392\n",
      "Epoch: 370, Loss: 1.0783, Train: 1.0385, Val: 1.0388\n",
      "Epoch: 371, Loss: 1.0776, Train: 1.0390, Val: 1.0390\n",
      "Epoch: 372, Loss: 1.0785, Train: 1.0382, Val: 1.0385\n",
      "Epoch: 373, Loss: 1.0770, Train: 1.0383, Val: 1.0386\n",
      "Epoch: 374, Loss: 1.0772, Train: 1.0386, Val: 1.0387\n",
      "Epoch: 375, Loss: 1.0779, Train: 1.0381, Val: 1.0386\n",
      "Epoch: 376, Loss: 1.0771, Train: 1.0378, Val: 1.0382\n",
      "Epoch: 377, Loss: 1.0764, Train: 1.0384, Val: 1.0385\n",
      "Epoch: 378, Loss: 1.0767, Train: 1.0378, Val: 1.0382\n",
      "Epoch: 379, Loss: 1.0766, Train: 1.0376, Val: 1.0378\n",
      "Epoch: 380, Loss: 1.0752, Train: 1.0377, Val: 1.0379\n",
      "Epoch: 381, Loss: 1.0751, Train: 1.0377, Val: 1.0380\n",
      "Epoch: 382, Loss: 1.0752, Train: 1.0376, Val: 1.0378\n",
      "Epoch: 383, Loss: 1.0763, Train: 1.0380, Val: 1.0387\n",
      "Epoch: 384, Loss: 1.0811, Train: 1.0399, Val: 1.0400\n",
      "Epoch: 385, Loss: 1.0821, Train: 1.0383, Val: 1.0389\n",
      "Epoch: 386, Loss: 1.0768, Train: 1.0404, Val: 1.0412\n",
      "Epoch: 387, Loss: 1.0805, Train: 1.0412, Val: 1.0412\n",
      "Epoch: 388, Loss: 1.0826, Train: 1.0383, Val: 1.0388\n",
      "Epoch: 389, Loss: 1.0752, Train: 1.0407, Val: 1.0414\n",
      "Epoch: 390, Loss: 1.0795, Train: 1.0392, Val: 1.0393\n",
      "Epoch: 391, Loss: 1.0777, Train: 1.0375, Val: 1.0379\n",
      "Epoch: 392, Loss: 1.0748, Train: 1.0390, Val: 1.0398\n",
      "Epoch: 393, Loss: 1.0779, Train: 1.0379, Val: 1.0381\n",
      "Epoch: 394, Loss: 1.0783, Train: 1.0373, Val: 1.0379\n",
      "Epoch: 395, Loss: 1.0746, Train: 1.0402, Val: 1.0411\n",
      "Epoch: 396, Loss: 1.0768, Train: 1.0369, Val: 1.0373\n",
      "Epoch: 397, Loss: 1.0750, Train: 1.0362, Val: 1.0367\n",
      "Epoch: 398, Loss: 1.0736, Train: 1.0378, Val: 1.0385\n",
      "Epoch: 399, Loss: 1.0748, Train: 1.0369, Val: 1.0372\n",
      "Epoch: 400, Loss: 1.0740, Train: 1.0365, Val: 1.0370\n",
      "Epoch: 401, Loss: 1.0726, Train: 1.0373, Val: 1.0381\n",
      "Epoch: 402, Loss: 1.0737, Train: 1.0357, Val: 1.0361\n",
      "Epoch: 403, Loss: 1.0737, Train: 1.0357, Val: 1.0363\n",
      "Epoch: 404, Loss: 1.0713, Train: 1.0365, Val: 1.0372\n",
      "Epoch: 405, Loss: 1.0723, Train: 1.0356, Val: 1.0360\n",
      "Epoch: 406, Loss: 1.0716, Train: 1.0353, Val: 1.0358\n",
      "Epoch: 407, Loss: 1.0709, Train: 1.0353, Val: 1.0359\n",
      "Epoch: 408, Loss: 1.0709, Train: 1.0354, Val: 1.0357\n",
      "Epoch: 409, Loss: 1.0712, Train: 1.0356, Val: 1.0362\n",
      "Epoch: 410, Loss: 1.0707, Train: 1.0348, Val: 1.0353\n",
      "Epoch: 411, Loss: 1.0698, Train: 1.0347, Val: 1.0351\n",
      "Epoch: 412, Loss: 1.0700, Train: 1.0352, Val: 1.0359\n",
      "Epoch: 413, Loss: 1.0704, Train: 1.0349, Val: 1.0351\n",
      "Epoch: 414, Loss: 1.0699, Train: 1.0347, Val: 1.0352\n",
      "Epoch: 415, Loss: 1.0692, Train: 1.0342, Val: 1.0347\n",
      "Epoch: 416, Loss: 1.0685, Train: 1.0342, Val: 1.0346\n",
      "Epoch: 417, Loss: 1.0690, Train: 1.0347, Val: 1.0354\n",
      "Epoch: 418, Loss: 1.0692, Train: 1.0343, Val: 1.0346\n",
      "Epoch: 419, Loss: 1.0692, Train: 1.0341, Val: 1.0348\n",
      "Epoch: 420, Loss: 1.0680, Train: 1.0336, Val: 1.0342\n",
      "Epoch: 421, Loss: 1.0673, Train: 1.0336, Val: 1.0340\n",
      "Epoch: 422, Loss: 1.0677, Train: 1.0342, Val: 1.0350\n",
      "Epoch: 423, Loss: 1.0682, Train: 1.0340, Val: 1.0343\n",
      "Epoch: 424, Loss: 1.0684, Train: 1.0339, Val: 1.0347\n",
      "Epoch: 425, Loss: 1.0674, Train: 1.0331, Val: 1.0335\n",
      "Epoch: 426, Loss: 1.0665, Train: 1.0330, Val: 1.0336\n",
      "Epoch: 427, Loss: 1.0659, Train: 1.0330, Val: 1.0337\n",
      "Epoch: 428, Loss: 1.0658, Train: 1.0329, Val: 1.0333\n",
      "Epoch: 429, Loss: 1.0659, Train: 1.0334, Val: 1.0342\n",
      "Epoch: 430, Loss: 1.0664, Train: 1.0330, Val: 1.0333\n",
      "Epoch: 431, Loss: 1.0669, Train: 1.0335, Val: 1.0344\n",
      "Epoch: 432, Loss: 1.0665, Train: 1.0325, Val: 1.0329\n",
      "Epoch: 433, Loss: 1.0654, Train: 1.0324, Val: 1.0331\n",
      "Epoch: 434, Loss: 1.0645, Train: 1.0320, Val: 1.0326\n",
      "Epoch: 435, Loss: 1.0639, Train: 1.0319, Val: 1.0324\n",
      "Epoch: 436, Loss: 1.0638, Train: 1.0322, Val: 1.0330\n",
      "Epoch: 437, Loss: 1.0639, Train: 1.0320, Val: 1.0324\n",
      "Epoch: 438, Loss: 1.0645, Train: 1.0331, Val: 1.0341\n",
      "Epoch: 439, Loss: 1.0656, Train: 1.0330, Val: 1.0332\n",
      "Epoch: 440, Loss: 1.0672, Train: 1.0346, Val: 1.0358\n",
      "Epoch: 441, Loss: 1.0687, Train: 1.0327, Val: 1.0328\n",
      "Epoch: 442, Loss: 1.0670, Train: 1.0320, Val: 1.0329\n",
      "Epoch: 443, Loss: 1.0633, Train: 1.0309, Val: 1.0315\n",
      "Epoch: 444, Loss: 1.0616, Train: 1.0310, Val: 1.0313\n",
      "Epoch: 445, Loss: 1.0623, Train: 1.0320, Val: 1.0330\n",
      "Epoch: 446, Loss: 1.0627, Train: 1.0309, Val: 1.0313\n",
      "Epoch: 447, Loss: 1.0629, Train: 1.0323, Val: 1.0333\n",
      "Epoch: 448, Loss: 1.0655, Train: 1.0302, Val: 1.0307\n",
      "Epoch: 449, Loss: 1.0607, Train: 1.0299, Val: 1.0305\n",
      "Epoch: 450, Loss: 1.0647, Train: 1.0324, Val: 1.0331\n",
      "Epoch: 451, Loss: 1.0771, Train: 1.0313, Val: 1.0316\n",
      "Epoch: 452, Loss: 1.0775, Train: 1.0320, Val: 1.0322\n",
      "Epoch: 453, Loss: 1.0684, Train: 1.0370, Val: 1.0374\n",
      "Epoch: 454, Loss: 1.0756, Train: 1.0380, Val: 1.0376\n",
      "Epoch: 455, Loss: 1.0831, Train: 1.0400, Val: 1.0407\n",
      "Epoch: 456, Loss: 1.0767, Train: 1.0360, Val: 1.0358\n",
      "Epoch: 457, Loss: 1.0707, Train: 1.0374, Val: 1.0367\n",
      "Epoch: 458, Loss: 1.0837, Train: 1.0499, Val: 1.0493\n",
      "Epoch: 459, Loss: 1.1303, Train: 1.0820, Val: 1.0841\n",
      "Epoch: 460, Loss: 1.1340, Train: 1.0567, Val: 1.0583\n",
      "Epoch: 461, Loss: 1.0863, Train: 1.0555, Val: 1.0549\n",
      "Epoch: 462, Loss: 1.1222, Train: 1.0535, Val: 1.0531\n",
      "Epoch: 463, Loss: 1.1133, Train: 1.0515, Val: 1.0524\n",
      "Epoch: 464, Loss: 1.0882, Train: 1.0585, Val: 1.0600\n",
      "Epoch: 465, Loss: 1.0982, Train: 1.0403, Val: 1.0410\n",
      "Epoch: 466, Loss: 1.0939, Train: 1.0386, Val: 1.0388\n",
      "Epoch: 467, Loss: 1.0917, Train: 1.0459, Val: 1.0466\n",
      "Epoch: 468, Loss: 1.0838, Train: 1.0467, Val: 1.0473\n",
      "Epoch: 469, Loss: 1.0892, Train: 1.0364, Val: 1.0362\n",
      "Epoch: 470, Loss: 1.0761, Train: 1.0388, Val: 1.0388\n",
      "Epoch: 471, Loss: 1.0848, Train: 1.0399, Val: 1.0409\n",
      "Epoch: 472, Loss: 1.0771, Train: 1.0406, Val: 1.0417\n",
      "Epoch: 473, Loss: 1.0729, Train: 1.0362, Val: 1.0363\n",
      "Epoch: 474, Loss: 1.0798, Train: 1.0327, Val: 1.0330\n",
      "Epoch: 475, Loss: 1.0669, Train: 1.0372, Val: 1.0382\n",
      "Epoch: 476, Loss: 1.0703, Train: 1.0354, Val: 1.0362\n",
      "Epoch: 477, Loss: 1.0703, Train: 1.0319, Val: 1.0322\n",
      "Epoch: 478, Loss: 1.0670, Train: 1.0311, Val: 1.0317\n",
      "Epoch: 479, Loss: 1.0628, Train: 1.0388, Val: 1.0398\n",
      "Epoch: 480, Loss: 1.0700, Train: 1.0300, Val: 1.0308\n",
      "Epoch: 481, Loss: 1.0581, Train: 1.0317, Val: 1.0320\n",
      "Epoch: 482, Loss: 1.0673, Train: 1.0293, Val: 1.0299\n",
      "Epoch: 483, Loss: 1.0587, Train: 1.0327, Val: 1.0336\n",
      "Epoch: 484, Loss: 1.0617, Train: 1.0298, Val: 1.0304\n",
      "Epoch: 485, Loss: 1.0599, Train: 1.0283, Val: 1.0288\n",
      "Epoch: 486, Loss: 1.0580, Train: 1.0302, Val: 1.0311\n",
      "Epoch: 487, Loss: 1.0596, Train: 1.0285, Val: 1.0293\n",
      "Epoch: 488, Loss: 1.0559, Train: 1.0290, Val: 1.0293\n",
      "Epoch: 489, Loss: 1.0571, Train: 1.0281, Val: 1.0287\n",
      "Epoch: 490, Loss: 1.0553, Train: 1.0275, Val: 1.0283\n",
      "Epoch: 491, Loss: 1.0536, Train: 1.0282, Val: 1.0287\n",
      "Epoch: 492, Loss: 1.0555, Train: 1.0267, Val: 1.0272\n",
      "Epoch: 493, Loss: 1.0514, Train: 1.0275, Val: 1.0278\n",
      "Epoch: 494, Loss: 1.0524, Train: 1.0262, Val: 1.0269\n",
      "Epoch: 495, Loss: 1.0501, Train: 1.0264, Val: 1.0271\n",
      "Epoch: 496, Loss: 1.0524, Train: 1.0261, Val: 1.0270\n",
      "Epoch: 497, Loss: 1.0495, Train: 1.0260, Val: 1.0266\n",
      "Epoch: 498, Loss: 1.0525, Train: 1.0343, Val: 1.0342\n",
      "Epoch: 499, Loss: 1.0706, Train: 1.0319, Val: 1.0332\n",
      "Epoch: 500, Loss: 1.0619, Train: 1.0315, Val: 1.0328\n",
      "Test RMSE: 1.0329\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.487279       3.583673\n",
      "std      1727.484387     741.673176       0.382648       1.116938\n",
      "min         0.000000       0.000000       2.113023       1.000000\n",
      "25%      1500.000000     259.000000       3.248260       3.000000\n",
      "50%      3066.000000     693.000000       3.480398       4.000000\n",
      "75%      4472.000000    1292.000000       3.740126       4.000000\n",
      "max      6039.000000    3702.000000       4.686332       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1094.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.21137393054858\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  3\n",
      "Kernel:  gd\n",
      "Model Name:  KPGIN\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.9353, Train: 3.2150, Val: 3.2183\n",
      "Epoch: 002, Loss: 11.8956, Train: 2.8655, Val: 2.8698\n",
      "Epoch: 003, Loss: 8.1341, Train: 2.6160, Val: 2.6184\n",
      "Epoch: 004, Loss: 4.4683, Train: 2.1680, Val: 2.1678\n",
      "Epoch: 005, Loss: 3.5722, Train: 1.6895, Val: 1.6864\n",
      "Epoch: 006, Loss: 3.6057, Train: 1.5784, Val: 1.5749\n",
      "Epoch: 007, Loss: 2.3485, Train: 1.5960, Val: 1.5943\n",
      "Epoch: 008, Loss: 1.7868, Train: 1.6321, Val: 1.6277\n",
      "Epoch: 009, Loss: 2.0043, Train: 1.6451, Val: 1.6417\n",
      "Epoch: 010, Loss: 1.9485, Train: 1.5144, Val: 1.5104\n",
      "Epoch: 011, Loss: 1.6644, Train: 1.4330, Val: 1.4289\n",
      "Epoch: 012, Loss: 1.5711, Train: 1.4242, Val: 1.4211\n",
      "Epoch: 013, Loss: 1.7482, Train: 1.4281, Val: 1.4241\n",
      "Epoch: 014, Loss: 1.6932, Train: 1.4209, Val: 1.4173\n",
      "Epoch: 015, Loss: 1.5682, Train: 1.4115, Val: 1.4089\n",
      "Epoch: 016, Loss: 1.5129, Train: 1.3910, Val: 1.3881\n",
      "Epoch: 017, Loss: 1.4537, Train: 1.3504, Val: 1.3463\n",
      "Epoch: 018, Loss: 1.3788, Train: 1.2923, Val: 1.2886\n",
      "Epoch: 019, Loss: 1.3650, Train: 1.2450, Val: 1.2433\n",
      "Epoch: 020, Loss: 1.3449, Train: 1.2142, Val: 1.2137\n",
      "Epoch: 021, Loss: 1.3226, Train: 1.1938, Val: 1.1939\n",
      "Epoch: 022, Loss: 1.3076, Train: 1.1743, Val: 1.1744\n",
      "Epoch: 023, Loss: 1.2884, Train: 1.1586, Val: 1.1584\n",
      "Epoch: 024, Loss: 1.2723, Train: 1.1505, Val: 1.1501\n",
      "Epoch: 025, Loss: 1.2636, Train: 1.1477, Val: 1.1471\n",
      "Epoch: 026, Loss: 1.2577, Train: 1.1459, Val: 1.1453\n",
      "Epoch: 027, Loss: 1.2526, Train: 1.1439, Val: 1.1433\n",
      "Epoch: 028, Loss: 1.2504, Train: 1.1406, Val: 1.1403\n",
      "Epoch: 029, Loss: 1.2513, Train: 1.1346, Val: 1.1346\n",
      "Epoch: 030, Loss: 1.2509, Train: 1.1272, Val: 1.1274\n",
      "Epoch: 031, Loss: 1.2483, Train: 1.1220, Val: 1.1224\n",
      "Epoch: 032, Loss: 1.2477, Train: 1.1218, Val: 1.1223\n",
      "Epoch: 033, Loss: 1.2496, Train: 1.1265, Val: 1.1272\n",
      "Epoch: 034, Loss: 1.2503, Train: 1.1329, Val: 1.1336\n",
      "Epoch: 035, Loss: 1.2506, Train: 1.1330, Val: 1.1338\n",
      "Epoch: 036, Loss: 1.2524, Train: 1.1266, Val: 1.1272\n",
      "Epoch: 037, Loss: 1.2490, Train: 1.1206, Val: 1.1211\n",
      "Epoch: 038, Loss: 1.2484, Train: 1.1182, Val: 1.1185\n",
      "Epoch: 039, Loss: 1.2450, Train: 1.1179, Val: 1.1183\n",
      "Epoch: 040, Loss: 1.2419, Train: 1.1175, Val: 1.1179\n",
      "Epoch: 041, Loss: 1.2513, Train: 1.1132, Val: 1.1133\n",
      "Epoch: 042, Loss: 1.2572, Train: 1.1130, Val: 1.1130\n",
      "Epoch: 043, Loss: 1.2476, Train: 1.1193, Val: 1.1196\n",
      "Epoch: 044, Loss: 1.2373, Train: 1.1270, Val: 1.1275\n",
      "Epoch: 045, Loss: 1.2381, Train: 1.1255, Val: 1.1260\n",
      "Epoch: 046, Loss: 1.2376, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 047, Loss: 1.2292, Train: 1.1113, Val: 1.1113\n",
      "Epoch: 048, Loss: 1.2250, Train: 1.1100, Val: 1.1099\n",
      "Epoch: 049, Loss: 1.2258, Train: 1.1088, Val: 1.1087\n",
      "Epoch: 050, Loss: 1.2236, Train: 1.1075, Val: 1.1075\n",
      "Epoch: 051, Loss: 1.2169, Train: 1.1087, Val: 1.1090\n",
      "Epoch: 052, Loss: 1.2139, Train: 1.1142, Val: 1.1147\n",
      "Epoch: 053, Loss: 1.2498, Train: 1.1075, Val: 1.1077\n",
      "Epoch: 054, Loss: 1.2330, Train: 1.1056, Val: 1.1053\n",
      "Epoch: 055, Loss: 1.2198, Train: 1.1133, Val: 1.1128\n",
      "Epoch: 056, Loss: 1.2363, Train: 1.1077, Val: 1.1073\n",
      "Epoch: 057, Loss: 1.2254, Train: 1.1018, Val: 1.1017\n",
      "Epoch: 058, Loss: 1.2077, Train: 1.1097, Val: 1.1099\n",
      "Epoch: 059, Loss: 1.2150, Train: 1.1095, Val: 1.1097\n",
      "Epoch: 060, Loss: 1.2228, Train: 1.0982, Val: 1.0980\n",
      "Epoch: 061, Loss: 1.1985, Train: 1.0981, Val: 1.0977\n",
      "Epoch: 062, Loss: 1.2141, Train: 1.0958, Val: 1.0955\n",
      "Epoch: 063, Loss: 1.1939, Train: 1.0957, Val: 1.0955\n",
      "Epoch: 064, Loss: 1.1948, Train: 1.0945, Val: 1.0943\n",
      "Epoch: 065, Loss: 1.1901, Train: 1.0931, Val: 1.0929\n",
      "Epoch: 066, Loss: 1.2011, Train: 1.0941, Val: 1.0941\n",
      "Epoch: 067, Loss: 1.1877, Train: 1.0977, Val: 1.0979\n",
      "Epoch: 068, Loss: 1.1879, Train: 1.0989, Val: 1.0991\n",
      "Epoch: 069, Loss: 1.1878, Train: 1.0964, Val: 1.0966\n",
      "Epoch: 070, Loss: 1.1866, Train: 1.0948, Val: 1.0949\n",
      "Epoch: 071, Loss: 1.1865, Train: 1.0963, Val: 1.0965\n",
      "Epoch: 072, Loss: 1.1845, Train: 1.0990, Val: 1.0993\n",
      "Epoch: 073, Loss: 1.1835, Train: 1.0988, Val: 1.0991\n",
      "Epoch: 074, Loss: 1.1822, Train: 1.0961, Val: 1.0963\n",
      "Epoch: 075, Loss: 1.1811, Train: 1.0953, Val: 1.0955\n",
      "Epoch: 076, Loss: 1.1809, Train: 1.0977, Val: 1.0979\n",
      "Epoch: 077, Loss: 1.1797, Train: 1.0987, Val: 1.0990\n",
      "Epoch: 078, Loss: 1.1796, Train: 1.0957, Val: 1.0960\n",
      "Epoch: 079, Loss: 1.1785, Train: 1.0940, Val: 1.0943\n",
      "Epoch: 080, Loss: 1.1774, Train: 1.0933, Val: 1.0936\n",
      "Epoch: 081, Loss: 1.1764, Train: 1.0911, Val: 1.0913\n",
      "Epoch: 082, Loss: 1.1754, Train: 1.0887, Val: 1.0890\n",
      "Epoch: 083, Loss: 1.1748, Train: 1.0880, Val: 1.0882\n",
      "Epoch: 084, Loss: 1.1739, Train: 1.0874, Val: 1.0876\n",
      "Epoch: 085, Loss: 1.1733, Train: 1.0859, Val: 1.0860\n",
      "Epoch: 086, Loss: 1.1724, Train: 1.0847, Val: 1.0848\n",
      "Epoch: 087, Loss: 1.1717, Train: 1.0843, Val: 1.0845\n",
      "Epoch: 088, Loss: 1.1707, Train: 1.0838, Val: 1.0839\n",
      "Epoch: 089, Loss: 1.1699, Train: 1.0829, Val: 1.0830\n",
      "Epoch: 090, Loss: 1.1690, Train: 1.0824, Val: 1.0825\n",
      "Epoch: 091, Loss: 1.1682, Train: 1.0823, Val: 1.0824\n",
      "Epoch: 092, Loss: 1.1675, Train: 1.0818, Val: 1.0819\n",
      "Epoch: 093, Loss: 1.1667, Train: 1.0811, Val: 1.0813\n",
      "Epoch: 094, Loss: 1.1661, Train: 1.0809, Val: 1.0810\n",
      "Epoch: 095, Loss: 1.1653, Train: 1.0803, Val: 1.0805\n",
      "Epoch: 096, Loss: 1.1646, Train: 1.0797, Val: 1.0798\n",
      "Epoch: 097, Loss: 1.1639, Train: 1.0794, Val: 1.0795\n",
      "Epoch: 098, Loss: 1.1632, Train: 1.0791, Val: 1.0792\n",
      "Epoch: 099, Loss: 1.1626, Train: 1.0787, Val: 1.0788\n",
      "Epoch: 100, Loss: 1.1619, Train: 1.0785, Val: 1.0786\n",
      "Epoch: 101, Loss: 1.1612, Train: 1.0783, Val: 1.0784\n",
      "Epoch: 102, Loss: 1.1606, Train: 1.0780, Val: 1.0780\n",
      "Epoch: 103, Loss: 1.1600, Train: 1.0780, Val: 1.0780\n",
      "Epoch: 104, Loss: 1.1595, Train: 1.0773, Val: 1.0774\n",
      "Epoch: 105, Loss: 1.1589, Train: 1.0773, Val: 1.0773\n",
      "Epoch: 106, Loss: 1.1582, Train: 1.0767, Val: 1.0767\n",
      "Epoch: 107, Loss: 1.1575, Train: 1.0765, Val: 1.0765\n",
      "Epoch: 108, Loss: 1.1568, Train: 1.0764, Val: 1.0764\n",
      "Epoch: 109, Loss: 1.1562, Train: 1.0760, Val: 1.0760\n",
      "Epoch: 110, Loss: 1.1556, Train: 1.0758, Val: 1.0759\n",
      "Epoch: 111, Loss: 1.1549, Train: 1.0755, Val: 1.0755\n",
      "Epoch: 112, Loss: 1.1544, Train: 1.0753, Val: 1.0754\n",
      "Epoch: 113, Loss: 1.1536, Train: 1.0751, Val: 1.0752\n",
      "Epoch: 114, Loss: 1.1530, Train: 1.0747, Val: 1.0746\n",
      "Epoch: 115, Loss: 1.1556, Train: 1.0762, Val: 1.0764\n",
      "Epoch: 116, Loss: 1.1642, Train: 1.0752, Val: 1.0751\n",
      "Epoch: 117, Loss: 1.1538, Train: 1.0756, Val: 1.0754\n",
      "Epoch: 118, Loss: 1.1538, Train: 1.0740, Val: 1.0741\n",
      "Epoch: 119, Loss: 1.1600, Train: 1.0748, Val: 1.0748\n",
      "Epoch: 120, Loss: 1.1511, Train: 1.0778, Val: 1.0776\n",
      "Epoch: 121, Loss: 1.1570, Train: 1.0730, Val: 1.0730\n",
      "Epoch: 122, Loss: 1.1528, Train: 1.0724, Val: 1.0724\n",
      "Epoch: 123, Loss: 1.1515, Train: 1.0742, Val: 1.0740\n",
      "Epoch: 124, Loss: 1.1543, Train: 1.0716, Val: 1.0716\n",
      "Epoch: 125, Loss: 1.1497, Train: 1.0715, Val: 1.0715\n",
      "Epoch: 126, Loss: 1.1488, Train: 1.0744, Val: 1.0742\n",
      "Epoch: 127, Loss: 1.1486, Train: 1.0729, Val: 1.0728\n",
      "Epoch: 128, Loss: 1.1461, Train: 1.0743, Val: 1.0741\n",
      "Epoch: 129, Loss: 1.1479, Train: 1.0719, Val: 1.0719\n",
      "Epoch: 130, Loss: 1.1469, Train: 1.0710, Val: 1.0710\n",
      "Epoch: 131, Loss: 1.1462, Train: 1.0726, Val: 1.0724\n",
      "Epoch: 132, Loss: 1.1448, Train: 1.0716, Val: 1.0714\n",
      "Epoch: 133, Loss: 1.1441, Train: 1.0688, Val: 1.0688\n",
      "Epoch: 134, Loss: 1.1425, Train: 1.0688, Val: 1.0688\n",
      "Epoch: 135, Loss: 1.1417, Train: 1.0680, Val: 1.0680\n",
      "Epoch: 136, Loss: 1.1433, Train: 1.0688, Val: 1.0690\n",
      "Epoch: 137, Loss: 1.1428, Train: 1.0681, Val: 1.0680\n",
      "Epoch: 138, Loss: 1.1383, Train: 1.0711, Val: 1.0709\n",
      "Epoch: 139, Loss: 1.1411, Train: 1.0677, Val: 1.0677\n",
      "Epoch: 140, Loss: 1.1374, Train: 1.0671, Val: 1.0672\n",
      "Epoch: 141, Loss: 1.1371, Train: 1.0664, Val: 1.0664\n",
      "Epoch: 142, Loss: 1.1345, Train: 1.0660, Val: 1.0661\n",
      "Epoch: 143, Loss: 1.1328, Train: 1.0659, Val: 1.0661\n",
      "Epoch: 144, Loss: 1.1324, Train: 1.0661, Val: 1.0661\n",
      "Epoch: 145, Loss: 1.1310, Train: 1.0660, Val: 1.0660\n",
      "Epoch: 146, Loss: 1.1299, Train: 1.0648, Val: 1.0650\n",
      "Epoch: 147, Loss: 1.1313, Train: 1.0658, Val: 1.0658\n",
      "Epoch: 148, Loss: 1.1385, Train: 1.0677, Val: 1.0682\n",
      "Epoch: 149, Loss: 1.1364, Train: 1.0647, Val: 1.0650\n",
      "Epoch: 150, Loss: 1.1294, Train: 1.0669, Val: 1.0668\n",
      "Epoch: 151, Loss: 1.1629, Train: 1.0704, Val: 1.0709\n",
      "Epoch: 152, Loss: 1.1808, Train: 1.0878, Val: 1.0869\n",
      "Epoch: 153, Loss: 1.1962, Train: 1.2372, Val: 1.2347\n",
      "Epoch: 154, Loss: 1.1567, Train: 1.3221, Val: 1.3193\n",
      "Epoch: 155, Loss: 1.2114, Train: 1.1839, Val: 1.1817\n",
      "Epoch: 156, Loss: 1.1376, Train: 1.1055, Val: 1.1044\n",
      "Epoch: 157, Loss: 1.1670, Train: 1.1000, Val: 1.0993\n",
      "Epoch: 158, Loss: 1.1361, Train: 1.1080, Val: 1.1075\n",
      "Epoch: 159, Loss: 1.2218, Train: 1.0695, Val: 1.0697\n",
      "Epoch: 160, Loss: 1.2196, Train: 1.0698, Val: 1.0697\n",
      "Epoch: 161, Loss: 1.2009, Train: 1.1065, Val: 1.1055\n",
      "Epoch: 162, Loss: 1.1393, Train: 1.1602, Val: 1.1588\n",
      "Epoch: 163, Loss: 1.2077, Train: 1.0891, Val: 1.0883\n",
      "Epoch: 164, Loss: 1.1627, Train: 1.0786, Val: 1.0781\n",
      "Epoch: 165, Loss: 1.1567, Train: 1.1038, Val: 1.1033\n",
      "Epoch: 166, Loss: 1.1291, Train: 1.1356, Val: 1.1350\n",
      "Epoch: 167, Loss: 1.1514, Train: 1.1015, Val: 1.1012\n",
      "Epoch: 168, Loss: 1.1341, Train: 1.0678, Val: 1.0681\n",
      "Epoch: 169, Loss: 1.1340, Train: 1.0627, Val: 1.0631\n",
      "Epoch: 170, Loss: 1.1406, Train: 1.0686, Val: 1.0688\n",
      "Epoch: 171, Loss: 1.1231, Train: 1.0754, Val: 1.0754\n",
      "Epoch: 172, Loss: 1.1310, Train: 1.0632, Val: 1.0635\n",
      "Epoch: 173, Loss: 1.1194, Train: 1.0610, Val: 1.0615\n",
      "Epoch: 174, Loss: 1.1255, Train: 1.0599, Val: 1.0603\n",
      "Epoch: 175, Loss: 1.1192, Train: 1.0620, Val: 1.0619\n",
      "Epoch: 176, Loss: 1.1221, Train: 1.0603, Val: 1.0603\n",
      "Epoch: 177, Loss: 1.1171, Train: 1.0593, Val: 1.0596\n",
      "Epoch: 178, Loss: 1.1164, Train: 1.0596, Val: 1.0600\n",
      "Epoch: 179, Loss: 1.1158, Train: 1.0572, Val: 1.0575\n",
      "Epoch: 180, Loss: 1.1110, Train: 1.0568, Val: 1.0571\n",
      "Epoch: 181, Loss: 1.1142, Train: 1.0589, Val: 1.0594\n",
      "Epoch: 182, Loss: 1.1132, Train: 1.0599, Val: 1.0605\n",
      "Epoch: 183, Loss: 1.1152, Train: 1.0563, Val: 1.0567\n",
      "Epoch: 184, Loss: 1.1094, Train: 1.0551, Val: 1.0553\n",
      "Epoch: 185, Loss: 1.1134, Train: 1.0557, Val: 1.0560\n",
      "Epoch: 186, Loss: 1.1073, Train: 1.0581, Val: 1.0585\n",
      "Epoch: 187, Loss: 1.1082, Train: 1.0562, Val: 1.0564\n",
      "Epoch: 188, Loss: 1.1071, Train: 1.0536, Val: 1.0536\n",
      "Epoch: 189, Loss: 1.1053, Train: 1.0538, Val: 1.0538\n",
      "Epoch: 190, Loss: 1.1067, Train: 1.0556, Val: 1.0558\n",
      "Epoch: 191, Loss: 1.1038, Train: 1.0559, Val: 1.0562\n",
      "Epoch: 192, Loss: 1.1054, Train: 1.0539, Val: 1.0540\n",
      "Epoch: 193, Loss: 1.1032, Train: 1.0531, Val: 1.0532\n",
      "Epoch: 194, Loss: 1.1019, Train: 1.0538, Val: 1.0540\n",
      "Epoch: 195, Loss: 1.1101, Train: 1.0664, Val: 1.0669\n",
      "Epoch: 196, Loss: 1.1371, Train: 1.0529, Val: 1.0527\n",
      "Epoch: 197, Loss: 1.1054, Train: 1.0648, Val: 1.0641\n",
      "Epoch: 198, Loss: 1.1266, Train: 1.0557, Val: 1.0554\n",
      "Epoch: 199, Loss: 1.1153, Train: 1.0638, Val: 1.0632\n",
      "Epoch: 200, Loss: 1.1045, Train: 1.0698, Val: 1.0691\n",
      "Epoch: 201, Loss: 1.1154, Train: 1.0539, Val: 1.0537\n",
      "Epoch: 202, Loss: 1.1027, Train: 1.0516, Val: 1.0516\n",
      "Epoch: 203, Loss: 1.1045, Train: 1.0531, Val: 1.0528\n",
      "Epoch: 204, Loss: 1.1031, Train: 1.0536, Val: 1.0533\n",
      "Epoch: 205, Loss: 1.1028, Train: 1.0534, Val: 1.0536\n",
      "Epoch: 206, Loss: 1.2499, Train: 1.1094, Val: 1.1085\n",
      "Epoch: 207, Loss: 1.3187, Train: 1.0598, Val: 1.0596\n",
      "Epoch: 208, Loss: 1.1685, Train: 1.1441, Val: 1.1453\n",
      "Epoch: 209, Loss: 1.1387, Train: 1.3088, Val: 1.3106\n",
      "Epoch: 210, Loss: 1.2784, Train: 1.2477, Val: 1.2490\n",
      "Epoch: 211, Loss: 1.1825, Train: 1.1144, Val: 1.1148\n",
      "Epoch: 212, Loss: 1.1893, Train: 1.1086, Val: 1.1090\n",
      "Epoch: 213, Loss: 1.1800, Train: 1.1840, Val: 1.1849\n",
      "Epoch: 214, Loss: 1.2323, Train: 1.1388, Val: 1.1396\n",
      "Epoch: 215, Loss: 1.1481, Train: 1.0754, Val: 1.0757\n",
      "Epoch: 216, Loss: 1.1425, Train: 1.0693, Val: 1.0691\n",
      "Epoch: 217, Loss: 1.2016, Train: 1.0666, Val: 1.0667\n",
      "Epoch: 218, Loss: 1.1692, Train: 1.0974, Val: 1.0983\n",
      "Epoch: 219, Loss: 1.1951, Train: 1.0929, Val: 1.0937\n",
      "Epoch: 220, Loss: 1.1644, Train: 1.0653, Val: 1.0653\n",
      "Epoch: 221, Loss: 1.1251, Train: 1.0746, Val: 1.0742\n",
      "Epoch: 222, Loss: 1.1211, Train: 1.0800, Val: 1.0795\n",
      "Epoch: 223, Loss: 1.1694, Train: 1.1445, Val: 1.1436\n",
      "Epoch: 224, Loss: 1.2023, Train: 1.1042, Val: 1.1036\n",
      "Epoch: 225, Loss: 1.1252, Train: 1.0690, Val: 1.0690\n",
      "Epoch: 226, Loss: 1.1700, Train: 1.0670, Val: 1.0672\n",
      "Epoch: 227, Loss: 1.2118, Train: 1.0925, Val: 1.0924\n",
      "Epoch: 228, Loss: 1.1502, Train: 1.1327, Val: 1.1323\n",
      "Epoch: 229, Loss: 1.2065, Train: 1.0779, Val: 1.0778\n",
      "Epoch: 230, Loss: 1.1370, Train: 1.0685, Val: 1.0689\n",
      "Epoch: 231, Loss: 1.1653, Train: 1.0728, Val: 1.0732\n",
      "Epoch: 232, Loss: 1.3730, Train: 1.0616, Val: 1.0618\n",
      "Epoch: 233, Loss: 1.1325, Train: 1.0825, Val: 1.0823\n",
      "Epoch: 234, Loss: 1.1769, Train: 1.0828, Val: 1.0827\n",
      "Epoch: 235, Loss: 1.1825, Train: 1.0793, Val: 1.0796\n",
      "Epoch: 236, Loss: 1.1708, Train: 1.0919, Val: 1.0926\n",
      "Epoch: 237, Loss: 1.1592, Train: 1.1156, Val: 1.1166\n",
      "Epoch: 238, Loss: 1.1360, Train: 1.1131, Val: 1.1143\n",
      "Epoch: 239, Loss: 1.2192, Train: 1.0737, Val: 1.0744\n",
      "Epoch: 240, Loss: 1.1563, Train: 1.0831, Val: 1.0835\n",
      "Epoch: 241, Loss: 1.2089, Train: 1.0656, Val: 1.0664\n",
      "Epoch: 242, Loss: 1.1397, Train: 1.0983, Val: 1.0995\n",
      "Epoch: 243, Loss: 1.1402, Train: 1.1244, Val: 1.1257\n",
      "Epoch: 244, Loss: 1.1852, Train: 1.0893, Val: 1.0903\n",
      "Epoch: 245, Loss: 1.1324, Train: 1.0855, Val: 1.0864\n",
      "Epoch: 246, Loss: 1.1340, Train: 1.1086, Val: 1.1097\n",
      "Epoch: 247, Loss: 1.1342, Train: 1.1182, Val: 1.1193\n",
      "Epoch: 248, Loss: 1.1491, Train: 1.0911, Val: 1.0920\n",
      "Epoch: 249, Loss: 1.1357, Train: 1.0669, Val: 1.0674\n",
      "Epoch: 250, Loss: 1.1356, Train: 1.0638, Val: 1.0642\n",
      "Epoch: 251, Loss: 1.1361, Train: 1.0730, Val: 1.0736\n",
      "Epoch: 252, Loss: 1.1218, Train: 1.0872, Val: 1.0879\n",
      "Epoch: 253, Loss: 1.1248, Train: 1.0817, Val: 1.0823\n",
      "Epoch: 254, Loss: 1.1172, Train: 1.0798, Val: 1.0804\n",
      "Epoch: 255, Loss: 1.1159, Train: 1.0828, Val: 1.0835\n",
      "Epoch: 256, Loss: 1.1150, Train: 1.0813, Val: 1.0820\n",
      "Epoch: 257, Loss: 1.1151, Train: 1.0724, Val: 1.0730\n",
      "Epoch: 258, Loss: 1.1134, Train: 1.0644, Val: 1.0649\n",
      "Epoch: 259, Loss: 1.1121, Train: 1.0616, Val: 1.0621\n",
      "Epoch: 260, Loss: 1.1096, Train: 1.0617, Val: 1.0621\n",
      "Epoch: 261, Loss: 1.1078, Train: 1.0604, Val: 1.0608\n",
      "Epoch: 262, Loss: 1.1073, Train: 1.0576, Val: 1.0579\n",
      "Epoch: 263, Loss: 1.1064, Train: 1.0562, Val: 1.0564\n",
      "Epoch: 264, Loss: 1.1061, Train: 1.0560, Val: 1.0562\n",
      "Epoch: 265, Loss: 1.1049, Train: 1.0560, Val: 1.0564\n",
      "Epoch: 266, Loss: 1.1033, Train: 1.0554, Val: 1.0558\n",
      "Epoch: 267, Loss: 1.1022, Train: 1.0537, Val: 1.0541\n",
      "Epoch: 268, Loss: 1.1010, Train: 1.0524, Val: 1.0528\n",
      "Epoch: 269, Loss: 1.1007, Train: 1.0518, Val: 1.0521\n",
      "Epoch: 270, Loss: 1.0996, Train: 1.0516, Val: 1.0519\n",
      "Epoch: 271, Loss: 1.0982, Train: 1.0514, Val: 1.0517\n",
      "Epoch: 272, Loss: 1.0971, Train: 1.0515, Val: 1.0518\n",
      "Epoch: 273, Loss: 1.0965, Train: 1.0517, Val: 1.0520\n",
      "Epoch: 274, Loss: 1.0960, Train: 1.0512, Val: 1.0514\n",
      "Epoch: 275, Loss: 1.0948, Train: 1.0504, Val: 1.0507\n",
      "Epoch: 276, Loss: 1.0937, Train: 1.0498, Val: 1.0502\n",
      "Epoch: 277, Loss: 1.0930, Train: 1.0496, Val: 1.0499\n",
      "Epoch: 278, Loss: 1.0926, Train: 1.0495, Val: 1.0498\n",
      "Epoch: 279, Loss: 1.0917, Train: 1.0494, Val: 1.0497\n",
      "Epoch: 280, Loss: 1.0908, Train: 1.0495, Val: 1.0498\n",
      "Epoch: 281, Loss: 1.0903, Train: 1.0496, Val: 1.0498\n",
      "Epoch: 282, Loss: 1.0897, Train: 1.0491, Val: 1.0493\n",
      "Epoch: 283, Loss: 1.0889, Train: 1.0481, Val: 1.0483\n",
      "Epoch: 284, Loss: 1.0883, Train: 1.0475, Val: 1.0477\n",
      "Epoch: 285, Loss: 1.0880, Train: 1.0473, Val: 1.0476\n",
      "Epoch: 286, Loss: 1.0872, Train: 1.0476, Val: 1.0478\n",
      "Epoch: 287, Loss: 1.0866, Train: 1.0474, Val: 1.0476\n",
      "Epoch: 288, Loss: 1.0861, Train: 1.0467, Val: 1.0470\n",
      "Epoch: 289, Loss: 1.0855, Train: 1.0461, Val: 1.0464\n",
      "Epoch: 290, Loss: 1.0848, Train: 1.0457, Val: 1.0461\n",
      "Epoch: 291, Loss: 1.0844, Train: 1.0454, Val: 1.0457\n",
      "Epoch: 292, Loss: 1.0837, Train: 1.0451, Val: 1.0454\n",
      "Epoch: 293, Loss: 1.0831, Train: 1.0449, Val: 1.0452\n",
      "Epoch: 294, Loss: 1.0826, Train: 1.0445, Val: 1.0448\n",
      "Epoch: 295, Loss: 1.0820, Train: 1.0438, Val: 1.0441\n",
      "Epoch: 296, Loss: 1.0815, Train: 1.0432, Val: 1.0436\n",
      "Epoch: 297, Loss: 1.0809, Train: 1.0430, Val: 1.0433\n",
      "Epoch: 298, Loss: 1.0803, Train: 1.0428, Val: 1.0431\n",
      "Epoch: 299, Loss: 1.0798, Train: 1.0422, Val: 1.0426\n",
      "Epoch: 300, Loss: 1.0792, Train: 1.0415, Val: 1.0419\n",
      "Epoch: 301, Loss: 1.0786, Train: 1.0411, Val: 1.0415\n",
      "Epoch: 302, Loss: 1.0781, Train: 1.0409, Val: 1.0413\n",
      "Epoch: 303, Loss: 1.0775, Train: 1.0405, Val: 1.0409\n",
      "Epoch: 304, Loss: 1.0770, Train: 1.0401, Val: 1.0405\n",
      "Epoch: 305, Loss: 1.0764, Train: 1.0397, Val: 1.0401\n",
      "Epoch: 306, Loss: 1.0759, Train: 1.0395, Val: 1.0399\n",
      "Epoch: 307, Loss: 1.0754, Train: 1.0393, Val: 1.0396\n",
      "Epoch: 308, Loss: 1.0749, Train: 1.0390, Val: 1.0393\n",
      "Epoch: 309, Loss: 1.0743, Train: 1.0386, Val: 1.0390\n",
      "Epoch: 310, Loss: 1.0738, Train: 1.0384, Val: 1.0388\n",
      "Epoch: 311, Loss: 1.0733, Train: 1.0382, Val: 1.0386\n",
      "Epoch: 312, Loss: 1.0728, Train: 1.0381, Val: 1.0385\n",
      "Epoch: 313, Loss: 1.0722, Train: 1.0377, Val: 1.0382\n",
      "Epoch: 314, Loss: 1.0717, Train: 1.0376, Val: 1.0381\n",
      "Epoch: 315, Loss: 1.0711, Train: 1.0375, Val: 1.0379\n",
      "Epoch: 316, Loss: 1.0706, Train: 1.0371, Val: 1.0375\n",
      "Epoch: 317, Loss: 1.0700, Train: 1.0368, Val: 1.0373\n",
      "Epoch: 318, Loss: 1.0695, Train: 1.0367, Val: 1.0371\n",
      "Epoch: 319, Loss: 1.0689, Train: 1.0363, Val: 1.0367\n",
      "Epoch: 320, Loss: 1.0684, Train: 1.0361, Val: 1.0365\n",
      "Epoch: 321, Loss: 1.0678, Train: 1.0359, Val: 1.0364\n",
      "Epoch: 322, Loss: 1.0673, Train: 1.0355, Val: 1.0359\n",
      "Epoch: 323, Loss: 1.0667, Train: 1.0353, Val: 1.0357\n",
      "Epoch: 324, Loss: 1.0662, Train: 1.0350, Val: 1.0354\n",
      "Epoch: 325, Loss: 1.0656, Train: 1.0347, Val: 1.0352\n",
      "Epoch: 326, Loss: 1.0650, Train: 1.0344, Val: 1.0348\n",
      "Epoch: 327, Loss: 1.0644, Train: 1.0343, Val: 1.0347\n",
      "Epoch: 328, Loss: 1.0640, Train: 1.0336, Val: 1.0341\n",
      "Epoch: 329, Loss: 1.0638, Train: 1.0341, Val: 1.0345\n",
      "Epoch: 330, Loss: 1.0655, Train: 1.0325, Val: 1.0330\n",
      "Epoch: 331, Loss: 1.0645, Train: 1.0331, Val: 1.0335\n",
      "Epoch: 332, Loss: 1.0679, Train: 1.0314, Val: 1.0320\n",
      "Epoch: 333, Loss: 1.0615, Train: 1.0307, Val: 1.0315\n",
      "Epoch: 334, Loss: 1.0751, Train: 1.0319, Val: 1.0324\n",
      "Epoch: 335, Loss: 1.0792, Train: 1.0323, Val: 1.0329\n",
      "Epoch: 336, Loss: 1.0858, Train: 1.0338, Val: 1.0347\n",
      "Epoch: 337, Loss: 1.0676, Train: 1.0552, Val: 1.0566\n",
      "Epoch: 338, Loss: 1.0953, Train: 1.0407, Val: 1.0418\n",
      "Epoch: 339, Loss: 1.0697, Train: 1.0338, Val: 1.0346\n",
      "Epoch: 340, Loss: 1.0704, Train: 1.0330, Val: 1.0337\n",
      "Epoch: 341, Loss: 1.0690, Train: 1.0346, Val: 1.0354\n",
      "Epoch: 342, Loss: 1.0681, Train: 1.0330, Val: 1.0337\n",
      "Epoch: 343, Loss: 1.0644, Train: 1.0322, Val: 1.0327\n",
      "Epoch: 344, Loss: 1.0648, Train: 1.0316, Val: 1.0322\n",
      "Epoch: 345, Loss: 1.0616, Train: 1.0322, Val: 1.0329\n",
      "Epoch: 346, Loss: 1.0612, Train: 1.0311, Val: 1.0316\n",
      "Epoch: 347, Loss: 1.0587, Train: 1.0317, Val: 1.0321\n",
      "Epoch: 348, Loss: 1.0591, Train: 1.0317, Val: 1.0323\n",
      "Epoch: 349, Loss: 1.0575, Train: 1.0313, Val: 1.0319\n",
      "Epoch: 350, Loss: 1.0563, Train: 1.0309, Val: 1.0313\n",
      "Epoch: 351, Loss: 1.0543, Train: 1.0310, Val: 1.0313\n",
      "Epoch: 352, Loss: 1.0535, Train: 1.0301, Val: 1.0306\n",
      "Epoch: 353, Loss: 1.0523, Train: 1.0292, Val: 1.0297\n",
      "Epoch: 354, Loss: 1.0506, Train: 1.0287, Val: 1.0292\n",
      "Epoch: 355, Loss: 1.0493, Train: 1.0282, Val: 1.0288\n",
      "Epoch: 356, Loss: 1.0479, Train: 1.0280, Val: 1.0286\n",
      "Epoch: 357, Loss: 1.0464, Train: 1.0280, Val: 1.0285\n",
      "Epoch: 358, Loss: 1.0448, Train: 1.0277, Val: 1.0283\n",
      "Epoch: 359, Loss: 1.0437, Train: 1.0273, Val: 1.0279\n",
      "Epoch: 360, Loss: 1.0421, Train: 1.0267, Val: 1.0273\n",
      "Epoch: 361, Loss: 1.0401, Train: 1.0252, Val: 1.0260\n",
      "Epoch: 362, Loss: 1.0381, Train: 1.0245, Val: 1.0254\n",
      "Epoch: 363, Loss: 1.0363, Train: 1.0251, Val: 1.0259\n",
      "Epoch: 364, Loss: 1.0346, Train: 1.0248, Val: 1.0256\n",
      "Epoch: 365, Loss: 1.0325, Train: 1.0254, Val: 1.0261\n",
      "Epoch: 366, Loss: 1.0304, Train: 1.0241, Val: 1.0249\n",
      "Epoch: 367, Loss: 1.0282, Train: 1.0231, Val: 1.0240\n",
      "Epoch: 368, Loss: 1.0261, Train: 1.0215, Val: 1.0224\n",
      "Epoch: 369, Loss: 1.0242, Train: 1.0212, Val: 1.0221\n",
      "Epoch: 370, Loss: 1.0232, Train: 1.0182, Val: 1.0193\n",
      "Epoch: 371, Loss: 1.0259, Train: 1.0223, Val: 1.0234\n",
      "Epoch: 372, Loss: 1.0561, Train: 1.0169, Val: 1.0188\n",
      "Epoch: 373, Loss: 1.0316, Train: 1.0343, Val: 1.0365\n",
      "Epoch: 374, Loss: 1.0431, Train: 1.0153, Val: 1.0168\n",
      "Epoch: 375, Loss: 1.0322, Train: 1.0114, Val: 1.0130\n",
      "Epoch: 376, Loss: 1.0265, Train: 1.0247, Val: 1.0267\n",
      "Epoch: 377, Loss: 1.0320, Train: 1.0139, Val: 1.0159\n",
      "Epoch: 378, Loss: 1.0170, Train: 1.0113, Val: 1.0134\n",
      "Epoch: 379, Loss: 1.0216, Train: 1.0092, Val: 1.0117\n",
      "Epoch: 380, Loss: 1.0130, Train: 1.0123, Val: 1.0148\n",
      "Epoch: 381, Loss: 1.0179, Train: 1.0062, Val: 1.0086\n",
      "Epoch: 382, Loss: 1.0091, Train: 1.0072, Val: 1.0096\n",
      "Epoch: 383, Loss: 1.0148, Train: 1.0061, Val: 1.0091\n",
      "Epoch: 384, Loss: 1.0061, Train: 1.0087, Val: 1.0119\n",
      "Epoch: 385, Loss: 1.0105, Train: 1.0038, Val: 1.0067\n",
      "Epoch: 386, Loss: 1.0053, Train: 1.0038, Val: 1.0066\n",
      "Epoch: 387, Loss: 1.0060, Train: 1.0037, Val: 1.0066\n",
      "Epoch: 388, Loss: 1.0039, Train: 1.0031, Val: 1.0061\n",
      "Epoch: 389, Loss: 1.0033, Train: 1.0014, Val: 1.0041\n",
      "Epoch: 390, Loss: 1.0027, Train: 1.0009, Val: 1.0036\n",
      "Epoch: 391, Loss: 0.9997, Train: 1.0027, Val: 1.0058\n",
      "Epoch: 392, Loss: 1.0012, Train: 1.0003, Val: 1.0032\n",
      "Epoch: 393, Loss: 0.9973, Train: 1.0002, Val: 1.0029\n",
      "Epoch: 394, Loss: 0.9986, Train: 0.9990, Val: 1.0018\n",
      "Epoch: 395, Loss: 0.9954, Train: 0.9983, Val: 1.0012\n",
      "Epoch: 396, Loss: 0.9962, Train: 0.9981, Val: 1.0009\n",
      "Epoch: 397, Loss: 0.9939, Train: 0.9985, Val: 1.0013\n",
      "Epoch: 398, Loss: 0.9934, Train: 0.9979, Val: 1.0009\n",
      "Epoch: 399, Loss: 0.9922, Train: 0.9967, Val: 0.9996\n",
      "Epoch: 400, Loss: 0.9913, Train: 0.9967, Val: 0.9997\n",
      "Epoch: 401, Loss: 0.9899, Train: 0.9982, Val: 1.0011\n",
      "Epoch: 402, Loss: 0.9890, Train: 0.9994, Val: 1.0023\n",
      "Epoch: 403, Loss: 0.9881, Train: 0.9975, Val: 1.0005\n",
      "Epoch: 404, Loss: 0.9868, Train: 0.9973, Val: 1.0002\n",
      "Epoch: 405, Loss: 0.9866, Train: 1.0001, Val: 1.0029\n",
      "Epoch: 406, Loss: 0.9850, Train: 1.0002, Val: 1.0031\n",
      "Epoch: 407, Loss: 0.9843, Train: 0.9970, Val: 1.0001\n",
      "Epoch: 408, Loss: 0.9837, Train: 0.9972, Val: 1.0002\n",
      "Epoch: 409, Loss: 0.9829, Train: 1.0004, Val: 1.0033\n",
      "Epoch: 410, Loss: 0.9817, Train: 1.0016, Val: 1.0044\n",
      "Epoch: 411, Loss: 0.9808, Train: 0.9989, Val: 1.0018\n",
      "Epoch: 412, Loss: 0.9803, Train: 0.9993, Val: 1.0022\n",
      "Epoch: 413, Loss: 0.9792, Train: 1.0012, Val: 1.0040\n",
      "Epoch: 414, Loss: 0.9784, Train: 0.9988, Val: 1.0016\n",
      "Epoch: 415, Loss: 0.9772, Train: 0.9972, Val: 1.0002\n",
      "Epoch: 416, Loss: 0.9767, Train: 0.9968, Val: 0.9998\n",
      "Epoch: 417, Loss: 0.9763, Train: 1.0006, Val: 1.0035\n",
      "Epoch: 418, Loss: 0.9764, Train: 0.9923, Val: 0.9956\n",
      "Epoch: 419, Loss: 0.9761, Train: 0.9985, Val: 1.0017\n",
      "Epoch: 420, Loss: 0.9811, Train: 0.9903, Val: 0.9938\n",
      "Epoch: 421, Loss: 0.9781, Train: 0.9922, Val: 0.9956\n",
      "Epoch: 422, Loss: 0.9778, Train: 0.9879, Val: 0.9917\n",
      "Epoch: 423, Loss: 0.9757, Train: 0.9872, Val: 0.9906\n",
      "Epoch: 424, Loss: 0.9769, Train: 0.9989, Val: 1.0020\n",
      "Epoch: 425, Loss: 0.9927, Train: 0.9961, Val: 1.0001\n",
      "Epoch: 426, Loss: 0.9956, Train: 0.9942, Val: 0.9978\n",
      "Epoch: 427, Loss: 0.9896, Train: 0.9969, Val: 0.9999\n",
      "Epoch: 428, Loss: 0.9911, Train: 1.0000, Val: 1.0034\n",
      "Epoch: 429, Loss: 1.0017, Train: 0.9865, Val: 0.9902\n",
      "Epoch: 430, Loss: 0.9745, Train: 0.9988, Val: 1.0017\n",
      "Epoch: 431, Loss: 0.9970, Train: 0.9864, Val: 0.9902\n",
      "Epoch: 432, Loss: 0.9735, Train: 0.9929, Val: 0.9966\n",
      "Epoch: 433, Loss: 0.9841, Train: 0.9907, Val: 0.9941\n",
      "Epoch: 434, Loss: 0.9734, Train: 0.9895, Val: 0.9928\n",
      "Epoch: 435, Loss: 0.9766, Train: 0.9870, Val: 0.9910\n",
      "Epoch: 436, Loss: 0.9751, Train: 0.9880, Val: 0.9923\n",
      "Epoch: 437, Loss: 0.9778, Train: 0.9928, Val: 0.9963\n",
      "Epoch: 438, Loss: 0.9801, Train: 1.0004, Val: 1.0039\n",
      "Epoch: 439, Loss: 1.0655, Train: 1.0173, Val: 1.0208\n",
      "Epoch: 440, Loss: 1.0728, Train: 1.0048, Val: 1.0092\n",
      "Epoch: 441, Loss: 1.0754, Train: 1.0055, Val: 1.0088\n",
      "Epoch: 442, Loss: 1.0930, Train: 1.0076, Val: 1.0106\n",
      "Epoch: 443, Loss: 1.0324, Train: 1.0187, Val: 1.0216\n",
      "Epoch: 444, Loss: 1.1232, Train: 1.1381, Val: 1.1413\n",
      "Epoch: 445, Loss: 1.0821, Train: 1.1715, Val: 1.1745\n",
      "Epoch: 446, Loss: 1.0956, Train: 1.0577, Val: 1.0600\n",
      "Epoch: 447, Loss: 1.1737, Train: 1.0781, Val: 1.0805\n",
      "Epoch: 448, Loss: 1.1583, Train: 1.0585, Val: 1.0590\n",
      "Epoch: 449, Loss: 1.1129, Train: 1.1075, Val: 1.1064\n",
      "Epoch: 450, Loss: 1.1619, Train: 1.0797, Val: 1.0792\n",
      "Epoch: 451, Loss: 1.1094, Train: 1.0569, Val: 1.0578\n",
      "Epoch: 452, Loss: 1.1003, Train: 1.0593, Val: 1.0607\n",
      "Epoch: 453, Loss: 1.1010, Train: 1.0428, Val: 1.0433\n",
      "Epoch: 454, Loss: 1.0685, Train: 1.0444, Val: 1.0446\n",
      "Epoch: 455, Loss: 1.0723, Train: 1.0563, Val: 1.0577\n",
      "Epoch: 456, Loss: 1.0502, Train: 1.0733, Val: 1.0751\n",
      "Epoch: 457, Loss: 1.0767, Train: 1.0281, Val: 1.0292\n",
      "Epoch: 458, Loss: 1.0395, Train: 1.0208, Val: 1.0218\n",
      "Epoch: 459, Loss: 1.0410, Train: 1.0156, Val: 1.0176\n",
      "Epoch: 460, Loss: 1.0132, Train: 1.0268, Val: 1.0292\n",
      "Epoch: 461, Loss: 1.0384, Train: 1.0124, Val: 1.0146\n",
      "Epoch: 462, Loss: 1.0123, Train: 1.0099, Val: 1.0119\n",
      "Epoch: 463, Loss: 1.0267, Train: 1.0097, Val: 1.0121\n",
      "Epoch: 464, Loss: 0.9992, Train: 1.0389, Val: 1.0419\n",
      "Epoch: 465, Loss: 1.0100, Train: 1.0399, Val: 1.0430\n",
      "Epoch: 466, Loss: 1.0013, Train: 1.0227, Val: 1.0258\n",
      "Epoch: 467, Loss: 1.0124, Train: 1.0350, Val: 1.0383\n",
      "Epoch: 468, Loss: 0.9920, Train: 1.0499, Val: 1.0535\n",
      "Epoch: 469, Loss: 0.9998, Train: 1.0318, Val: 1.0353\n",
      "Epoch: 470, Loss: 0.9933, Train: 1.0156, Val: 1.0188\n",
      "Epoch: 471, Loss: 0.9999, Train: 1.0189, Val: 1.0224\n",
      "Epoch: 472, Loss: 0.9898, Train: 1.0322, Val: 1.0359\n",
      "Epoch: 473, Loss: 0.9940, Train: 1.0117, Val: 1.0151\n",
      "Epoch: 474, Loss: 0.9847, Train: 1.0027, Val: 1.0058\n",
      "Epoch: 475, Loss: 0.9895, Train: 1.0099, Val: 1.0127\n",
      "Epoch: 476, Loss: 0.9831, Train: 1.0136, Val: 1.0163\n",
      "Epoch: 477, Loss: 0.9832, Train: 1.0071, Val: 1.0098\n",
      "Epoch: 478, Loss: 0.9794, Train: 1.0016, Val: 1.0044\n",
      "Epoch: 479, Loss: 0.9815, Train: 1.0001, Val: 1.0033\n",
      "Epoch: 480, Loss: 0.9780, Train: 1.0024, Val: 1.0060\n",
      "Epoch: 481, Loss: 0.9779, Train: 0.9990, Val: 1.0027\n",
      "Epoch: 482, Loss: 0.9747, Train: 0.9945, Val: 0.9981\n",
      "Epoch: 483, Loss: 0.9757, Train: 0.9943, Val: 0.9981\n",
      "Epoch: 484, Loss: 0.9724, Train: 0.9934, Val: 0.9973\n",
      "Epoch: 485, Loss: 0.9718, Train: 0.9894, Val: 0.9931\n",
      "Epoch: 486, Loss: 0.9691, Train: 0.9894, Val: 0.9930\n",
      "Epoch: 487, Loss: 0.9686, Train: 0.9895, Val: 0.9932\n",
      "Epoch: 488, Loss: 0.9662, Train: 0.9893, Val: 0.9931\n",
      "Epoch: 489, Loss: 0.9647, Train: 0.9900, Val: 0.9942\n",
      "Epoch: 490, Loss: 0.9760, Train: 0.9949, Val: 0.9986\n",
      "Epoch: 491, Loss: 0.9940, Train: 0.9992, Val: 1.0021\n",
      "Epoch: 492, Loss: 0.9970, Train: 1.0105, Val: 1.0126\n",
      "Epoch: 493, Loss: 1.0016, Train: 1.0091, Val: 1.0113\n",
      "Epoch: 494, Loss: 0.9948, Train: 1.0057, Val: 1.0086\n",
      "Epoch: 495, Loss: 0.9924, Train: 1.0037, Val: 1.0066\n",
      "Epoch: 496, Loss: 0.9900, Train: 1.0111, Val: 1.0133\n",
      "Epoch: 497, Loss: 0.9851, Train: 1.0200, Val: 1.0219\n",
      "Epoch: 498, Loss: 0.9790, Train: 1.0175, Val: 1.0195\n",
      "Epoch: 499, Loss: 0.9829, Train: 1.0223, Val: 1.0242\n",
      "Epoch: 500, Loss: 0.9816, Train: 1.0297, Val: 1.0316\n",
      "Test RMSE: 1.0392\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.840162       3.583673\n",
      "std      1727.484387     741.673176       0.479460       1.116938\n",
      "min         0.000000       0.000000       1.280306       1.000000\n",
      "25%      1500.000000     259.000000       3.549158       3.000000\n",
      "50%      3066.000000     693.000000       3.882849       4.000000\n",
      "75%      4472.000000    1292.000000       4.179859       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1042.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.21137393054858\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  3\n",
      "Kernel:  spd\n",
      "Model Name:  KPGIN\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.4564, Train: 2.9615, Val: 2.9630\n",
      "Epoch: 002, Loss: 10.9897, Train: 2.7226, Val: 2.7259\n",
      "Epoch: 003, Loss: 8.8140, Train: 2.5456, Val: 2.5475\n",
      "Epoch: 004, Loss: 6.7964, Train: 2.2690, Val: 2.2681\n",
      "Epoch: 005, Loss: 5.6582, Train: 1.9070, Val: 1.9043\n",
      "Epoch: 006, Loss: 2.6515, Train: 1.5617, Val: 1.5569\n",
      "Epoch: 007, Loss: 2.5734, Train: 1.4856, Val: 1.4810\n",
      "Epoch: 008, Loss: 2.5762, Train: 1.4810, Val: 1.4782\n",
      "Epoch: 009, Loss: 1.8626, Train: 1.4907, Val: 1.4879\n",
      "Epoch: 010, Loss: 1.8584, Train: 1.4977, Val: 1.4949\n",
      "Epoch: 011, Loss: 1.8975, Train: 1.5008, Val: 1.4984\n",
      "Epoch: 012, Loss: 1.7847, Train: 1.4753, Val: 1.4716\n",
      "Epoch: 013, Loss: 1.6095, Train: 1.4478, Val: 1.4432\n",
      "Epoch: 014, Loss: 1.5624, Train: 1.4369, Val: 1.4318\n",
      "Epoch: 015, Loss: 1.5811, Train: 1.4286, Val: 1.4239\n",
      "Epoch: 016, Loss: 1.5161, Train: 1.4157, Val: 1.4116\n",
      "Epoch: 017, Loss: 1.4405, Train: 1.3923, Val: 1.3891\n",
      "Epoch: 018, Loss: 1.3894, Train: 1.3621, Val: 1.3594\n",
      "Epoch: 019, Loss: 1.3252, Train: 1.3189, Val: 1.3156\n",
      "Epoch: 020, Loss: 1.3247, Train: 1.2700, Val: 1.2661\n",
      "Epoch: 021, Loss: 1.2914, Train: 1.2298, Val: 1.2266\n",
      "Epoch: 022, Loss: 1.2865, Train: 1.1998, Val: 1.1976\n",
      "Epoch: 023, Loss: 1.2718, Train: 1.1791, Val: 1.1773\n",
      "Epoch: 024, Loss: 1.2616, Train: 1.1670, Val: 1.1654\n",
      "Epoch: 025, Loss: 1.2557, Train: 1.1637, Val: 1.1620\n",
      "Epoch: 026, Loss: 1.2456, Train: 1.1683, Val: 1.1665\n",
      "Epoch: 027, Loss: 1.2447, Train: 1.1743, Val: 1.1724\n",
      "Epoch: 028, Loss: 1.2474, Train: 1.1792, Val: 1.1770\n",
      "Epoch: 029, Loss: 1.2472, Train: 1.1825, Val: 1.1801\n",
      "Epoch: 030, Loss: 1.2524, Train: 1.1795, Val: 1.1772\n",
      "Epoch: 031, Loss: 1.2487, Train: 1.1711, Val: 1.1690\n",
      "Epoch: 032, Loss: 1.2399, Train: 1.1554, Val: 1.1537\n",
      "Epoch: 033, Loss: 1.2360, Train: 1.1357, Val: 1.1342\n",
      "Epoch: 034, Loss: 1.2432, Train: 1.1279, Val: 1.1266\n",
      "Epoch: 035, Loss: 1.2239, Train: 1.1276, Val: 1.1264\n",
      "Epoch: 036, Loss: 1.2997, Train: 1.1192, Val: 1.1181\n",
      "Epoch: 037, Loss: 1.2178, Train: 1.1316, Val: 1.1305\n",
      "Epoch: 038, Loss: 1.2609, Train: 1.1151, Val: 1.1143\n",
      "Epoch: 039, Loss: 1.2332, Train: 1.1016, Val: 1.1015\n",
      "Epoch: 040, Loss: 1.2520, Train: 1.1059, Val: 1.1065\n",
      "Epoch: 041, Loss: 1.2063, Train: 1.1066, Val: 1.1073\n",
      "Epoch: 042, Loss: 1.2059, Train: 1.1084, Val: 1.1090\n",
      "Epoch: 043, Loss: 1.2048, Train: 1.1121, Val: 1.1128\n",
      "Epoch: 044, Loss: 1.2020, Train: 1.1156, Val: 1.1162\n",
      "Epoch: 045, Loss: 1.1992, Train: 1.1160, Val: 1.1166\n",
      "Epoch: 046, Loss: 1.1968, Train: 1.1125, Val: 1.1131\n",
      "Epoch: 047, Loss: 1.1951, Train: 1.1086, Val: 1.1092\n",
      "Epoch: 048, Loss: 1.1943, Train: 1.1089, Val: 1.1095\n",
      "Epoch: 049, Loss: 1.1920, Train: 1.1130, Val: 1.1136\n",
      "Epoch: 050, Loss: 1.1891, Train: 1.1143, Val: 1.1150\n",
      "Epoch: 051, Loss: 1.1873, Train: 1.1104, Val: 1.1111\n",
      "Epoch: 052, Loss: 1.1856, Train: 1.1064, Val: 1.1072\n",
      "Epoch: 053, Loss: 1.1852, Train: 1.1055, Val: 1.1063\n",
      "Epoch: 054, Loss: 1.1847, Train: 1.1057, Val: 1.1065\n",
      "Epoch: 055, Loss: 1.1841, Train: 1.1038, Val: 1.1047\n",
      "Epoch: 056, Loss: 1.1833, Train: 1.0995, Val: 1.1004\n",
      "Epoch: 057, Loss: 1.1818, Train: 1.0957, Val: 1.0966\n",
      "Epoch: 058, Loss: 1.1805, Train: 1.0942, Val: 1.0951\n",
      "Epoch: 059, Loss: 1.1789, Train: 1.0938, Val: 1.0947\n",
      "Epoch: 060, Loss: 1.1777, Train: 1.0919, Val: 1.0928\n",
      "Epoch: 061, Loss: 1.1768, Train: 1.0887, Val: 1.0895\n",
      "Epoch: 062, Loss: 1.1760, Train: 1.0869, Val: 1.0876\n",
      "Epoch: 063, Loss: 1.1754, Train: 1.0865, Val: 1.0873\n",
      "Epoch: 064, Loss: 1.1742, Train: 1.0864, Val: 1.0871\n",
      "Epoch: 065, Loss: 1.1732, Train: 1.0852, Val: 1.0859\n",
      "Epoch: 066, Loss: 1.1721, Train: 1.0839, Val: 1.0845\n",
      "Epoch: 067, Loss: 1.1713, Train: 1.0832, Val: 1.0838\n",
      "Epoch: 068, Loss: 1.1705, Train: 1.0828, Val: 1.0834\n",
      "Epoch: 069, Loss: 1.1696, Train: 1.0820, Val: 1.0825\n",
      "Epoch: 070, Loss: 1.1687, Train: 1.0811, Val: 1.0815\n",
      "Epoch: 071, Loss: 1.1677, Train: 1.0805, Val: 1.0809\n",
      "Epoch: 072, Loss: 1.1669, Train: 1.0802, Val: 1.0806\n",
      "Epoch: 073, Loss: 1.1663, Train: 1.0799, Val: 1.0803\n",
      "Epoch: 074, Loss: 1.1658, Train: 1.0797, Val: 1.0800\n",
      "Epoch: 075, Loss: 1.1652, Train: 1.0794, Val: 1.0797\n",
      "Epoch: 076, Loss: 1.1645, Train: 1.0792, Val: 1.0795\n",
      "Epoch: 077, Loss: 1.1639, Train: 1.0789, Val: 1.0792\n",
      "Epoch: 078, Loss: 1.1632, Train: 1.0786, Val: 1.0789\n",
      "Epoch: 079, Loss: 1.1626, Train: 1.0784, Val: 1.0786\n",
      "Epoch: 080, Loss: 1.1619, Train: 1.0781, Val: 1.0783\n",
      "Epoch: 081, Loss: 1.1612, Train: 1.0779, Val: 1.0780\n",
      "Epoch: 082, Loss: 1.1608, Train: 1.0776, Val: 1.0778\n",
      "Epoch: 083, Loss: 1.1601, Train: 1.0773, Val: 1.0775\n",
      "Epoch: 084, Loss: 1.1595, Train: 1.0770, Val: 1.0773\n",
      "Epoch: 085, Loss: 1.1588, Train: 1.0768, Val: 1.0771\n",
      "Epoch: 086, Loss: 1.1583, Train: 1.0765, Val: 1.0768\n",
      "Epoch: 087, Loss: 1.1576, Train: 1.0763, Val: 1.0766\n",
      "Epoch: 088, Loss: 1.1571, Train: 1.0763, Val: 1.0766\n",
      "Epoch: 089, Loss: 1.1566, Train: 1.0759, Val: 1.0762\n",
      "Epoch: 090, Loss: 1.1559, Train: 1.0757, Val: 1.0760\n",
      "Epoch: 091, Loss: 1.1553, Train: 1.0756, Val: 1.0759\n",
      "Epoch: 092, Loss: 1.1550, Train: 1.0754, Val: 1.0757\n",
      "Epoch: 093, Loss: 1.1546, Train: 1.0754, Val: 1.0757\n",
      "Epoch: 094, Loss: 1.1553, Train: 1.0751, Val: 1.0754\n",
      "Epoch: 095, Loss: 1.1536, Train: 1.0748, Val: 1.0751\n",
      "Epoch: 096, Loss: 1.1526, Train: 1.0746, Val: 1.0749\n",
      "Epoch: 097, Loss: 1.1519, Train: 1.0743, Val: 1.0747\n",
      "Epoch: 098, Loss: 1.1517, Train: 1.0742, Val: 1.0745\n",
      "Epoch: 099, Loss: 1.1509, Train: 1.0746, Val: 1.0749\n",
      "Epoch: 100, Loss: 1.1532, Train: 1.0744, Val: 1.0748\n",
      "Epoch: 101, Loss: 1.1540, Train: 1.0764, Val: 1.0766\n",
      "Epoch: 102, Loss: 1.1570, Train: 1.0733, Val: 1.0736\n",
      "Epoch: 103, Loss: 1.1600, Train: 1.0756, Val: 1.0762\n",
      "Epoch: 104, Loss: 1.1667, Train: 1.0800, Val: 1.0808\n",
      "Epoch: 105, Loss: 1.1636, Train: 1.0960, Val: 1.0969\n",
      "Epoch: 106, Loss: 1.1655, Train: 1.0882, Val: 1.0891\n",
      "Epoch: 107, Loss: 1.1975, Train: 1.1612, Val: 1.1625\n",
      "Epoch: 108, Loss: 1.2555, Train: 1.0838, Val: 1.0845\n",
      "Epoch: 109, Loss: 1.1540, Train: 1.0825, Val: 1.0827\n",
      "Epoch: 110, Loss: 1.2407, Train: 1.0837, Val: 1.0843\n",
      "Epoch: 111, Loss: 1.1560, Train: 1.1227, Val: 1.1235\n",
      "Epoch: 112, Loss: 1.2067, Train: 1.0858, Val: 1.0864\n",
      "Epoch: 113, Loss: 1.1613, Train: 1.0801, Val: 1.0803\n",
      "Epoch: 114, Loss: 1.1805, Train: 1.0777, Val: 1.0780\n",
      "Epoch: 115, Loss: 1.1739, Train: 1.0857, Val: 1.0864\n",
      "Epoch: 116, Loss: 1.1623, Train: 1.0954, Val: 1.0962\n",
      "Epoch: 117, Loss: 1.1778, Train: 1.0726, Val: 1.0731\n",
      "Epoch: 118, Loss: 1.1511, Train: 1.0832, Val: 1.0835\n",
      "Epoch: 119, Loss: 1.1738, Train: 1.0732, Val: 1.0737\n",
      "Epoch: 120, Loss: 1.1519, Train: 1.0814, Val: 1.0823\n",
      "Epoch: 121, Loss: 1.1641, Train: 1.0786, Val: 1.0795\n",
      "Epoch: 122, Loss: 1.1585, Train: 1.0726, Val: 1.0732\n",
      "Epoch: 123, Loss: 1.1518, Train: 1.0750, Val: 1.0755\n",
      "Epoch: 124, Loss: 1.1602, Train: 1.0730, Val: 1.0737\n",
      "Epoch: 125, Loss: 1.1480, Train: 1.0834, Val: 1.0843\n",
      "Epoch: 126, Loss: 1.1594, Train: 1.0721, Val: 1.0728\n",
      "Epoch: 127, Loss: 1.1468, Train: 1.0732, Val: 1.0737\n",
      "Epoch: 128, Loss: 1.1539, Train: 1.0711, Val: 1.0716\n",
      "Epoch: 129, Loss: 1.1478, Train: 1.0741, Val: 1.0748\n",
      "Epoch: 130, Loss: 1.1496, Train: 1.0746, Val: 1.0753\n",
      "Epoch: 131, Loss: 1.1496, Train: 1.0699, Val: 1.0704\n",
      "Epoch: 132, Loss: 1.1451, Train: 1.0705, Val: 1.0710\n",
      "Epoch: 133, Loss: 1.1488, Train: 1.0706, Val: 1.0712\n",
      "Epoch: 134, Loss: 1.1434, Train: 1.0761, Val: 1.0768\n",
      "Epoch: 135, Loss: 1.1475, Train: 1.0699, Val: 1.0705\n",
      "Epoch: 136, Loss: 1.1421, Train: 1.0696, Val: 1.0701\n",
      "Epoch: 137, Loss: 1.1449, Train: 1.0688, Val: 1.0693\n",
      "Epoch: 138, Loss: 1.1418, Train: 1.0708, Val: 1.0715\n",
      "Epoch: 139, Loss: 1.1427, Train: 1.0700, Val: 1.0707\n",
      "Epoch: 140, Loss: 1.1410, Train: 1.0679, Val: 1.0685\n",
      "Epoch: 141, Loss: 1.1401, Train: 1.0676, Val: 1.0682\n",
      "Epoch: 142, Loss: 1.1397, Train: 1.0694, Val: 1.0702\n",
      "Epoch: 143, Loss: 1.1385, Train: 1.0688, Val: 1.0696\n",
      "Epoch: 144, Loss: 1.1375, Train: 1.0672, Val: 1.0679\n",
      "Epoch: 145, Loss: 1.1364, Train: 1.0672, Val: 1.0679\n",
      "Epoch: 146, Loss: 1.1355, Train: 1.0681, Val: 1.0690\n",
      "Epoch: 147, Loss: 1.1335, Train: 1.0690, Val: 1.0699\n",
      "Epoch: 148, Loss: 1.1302, Train: 1.0678, Val: 1.0687\n",
      "Epoch: 149, Loss: 1.1803, Train: 1.1195, Val: 1.1208\n",
      "Epoch: 150, Loss: 1.2062, Train: 1.0753, Val: 1.0762\n",
      "Epoch: 151, Loss: 1.1304, Train: 1.0726, Val: 1.0731\n",
      "Epoch: 152, Loss: 1.1524, Train: 1.0729, Val: 1.0734\n",
      "Epoch: 153, Loss: 1.1470, Train: 1.0790, Val: 1.0797\n",
      "Epoch: 154, Loss: 1.1356, Train: 1.0932, Val: 1.0940\n",
      "Epoch: 155, Loss: 1.1350, Train: 1.0934, Val: 1.0941\n",
      "Epoch: 156, Loss: 1.1281, Train: 1.0797, Val: 1.0802\n",
      "Epoch: 157, Loss: 1.1241, Train: 1.0737, Val: 1.0741\n",
      "Epoch: 158, Loss: 1.1273, Train: 1.0779, Val: 1.0784\n",
      "Epoch: 159, Loss: 1.1265, Train: 1.0811, Val: 1.0817\n",
      "Epoch: 160, Loss: 1.1268, Train: 1.0749, Val: 1.0755\n",
      "Epoch: 161, Loss: 1.1236, Train: 1.0694, Val: 1.0701\n",
      "Epoch: 162, Loss: 1.1210, Train: 1.0685, Val: 1.0693\n",
      "Epoch: 163, Loss: 1.1165, Train: 1.0744, Val: 1.0754\n",
      "Epoch: 164, Loss: 1.1137, Train: 1.0793, Val: 1.0804\n",
      "Epoch: 165, Loss: 1.1162, Train: 1.0730, Val: 1.0740\n",
      "Epoch: 166, Loss: 1.1108, Train: 1.0660, Val: 1.0670\n",
      "Epoch: 167, Loss: 1.1081, Train: 1.0636, Val: 1.0645\n",
      "Epoch: 168, Loss: 1.1095, Train: 1.0637, Val: 1.0646\n",
      "Epoch: 169, Loss: 1.1081, Train: 1.0678, Val: 1.0689\n",
      "Epoch: 170, Loss: 1.1055, Train: 1.0713, Val: 1.0724\n",
      "Epoch: 171, Loss: 1.1063, Train: 1.0650, Val: 1.0661\n",
      "Epoch: 172, Loss: 1.1042, Train: 1.0610, Val: 1.0622\n",
      "Epoch: 173, Loss: 1.1036, Train: 1.0649, Val: 1.0663\n",
      "Epoch: 174, Loss: 1.1015, Train: 1.0707, Val: 1.0722\n",
      "Epoch: 175, Loss: 1.1019, Train: 1.0659, Val: 1.0674\n",
      "Epoch: 176, Loss: 1.1000, Train: 1.0594, Val: 1.0609\n",
      "Epoch: 177, Loss: 1.0991, Train: 1.0591, Val: 1.0607\n",
      "Epoch: 178, Loss: 1.0982, Train: 1.0621, Val: 1.0638\n",
      "Epoch: 179, Loss: 1.0979, Train: 1.0596, Val: 1.0613\n",
      "Epoch: 180, Loss: 1.0963, Train: 1.0571, Val: 1.0589\n",
      "Epoch: 181, Loss: 1.0955, Train: 1.0569, Val: 1.0587\n",
      "Epoch: 182, Loss: 1.0950, Train: 1.0566, Val: 1.0585\n",
      "Epoch: 183, Loss: 1.0938, Train: 1.0545, Val: 1.0563\n",
      "Epoch: 184, Loss: 1.0929, Train: 1.0523, Val: 1.0541\n",
      "Epoch: 185, Loss: 1.0921, Train: 1.0518, Val: 1.0535\n",
      "Epoch: 186, Loss: 1.0914, Train: 1.0511, Val: 1.0529\n",
      "Epoch: 187, Loss: 1.0901, Train: 1.0509, Val: 1.0528\n",
      "Epoch: 188, Loss: 1.0894, Train: 1.0505, Val: 1.0524\n",
      "Epoch: 189, Loss: 1.0884, Train: 1.0501, Val: 1.0518\n",
      "Epoch: 190, Loss: 1.0873, Train: 1.0495, Val: 1.0513\n",
      "Epoch: 191, Loss: 1.0862, Train: 1.0490, Val: 1.0508\n",
      "Epoch: 192, Loss: 1.0851, Train: 1.0493, Val: 1.0510\n",
      "Epoch: 193, Loss: 1.0834, Train: 1.0498, Val: 1.0515\n",
      "Epoch: 194, Loss: 1.0822, Train: 1.0487, Val: 1.0506\n",
      "Epoch: 195, Loss: 1.0826, Train: 1.0473, Val: 1.0492\n",
      "Epoch: 196, Loss: 1.0812, Train: 1.0473, Val: 1.0490\n",
      "Epoch: 197, Loss: 1.0784, Train: 1.0504, Val: 1.0519\n",
      "Epoch: 198, Loss: 1.0794, Train: 1.0472, Val: 1.0491\n",
      "Epoch: 199, Loss: 1.0796, Train: 1.0467, Val: 1.0486\n",
      "Epoch: 200, Loss: 1.0892, Train: 1.0455, Val: 1.0477\n",
      "Epoch: 201, Loss: 1.0843, Train: 1.0482, Val: 1.0505\n",
      "Epoch: 202, Loss: 1.0820, Train: 1.0430, Val: 1.0449\n",
      "Epoch: 203, Loss: 1.0807, Train: 1.0465, Val: 1.0483\n",
      "Epoch: 204, Loss: 1.0930, Train: 1.0434, Val: 1.0458\n",
      "Epoch: 205, Loss: 1.0785, Train: 1.0533, Val: 1.0559\n",
      "Epoch: 206, Loss: 1.0843, Train: 1.0434, Val: 1.0454\n",
      "Epoch: 207, Loss: 1.0814, Train: 1.0448, Val: 1.0470\n",
      "Epoch: 208, Loss: 1.0861, Train: 1.0395, Val: 1.0421\n",
      "Epoch: 209, Loss: 1.0820, Train: 1.0446, Val: 1.0472\n",
      "Epoch: 210, Loss: 1.0836, Train: 1.0381, Val: 1.0404\n",
      "Epoch: 211, Loss: 1.0753, Train: 1.0423, Val: 1.0443\n",
      "Epoch: 212, Loss: 1.0865, Train: 1.0398, Val: 1.0426\n",
      "Epoch: 213, Loss: 1.0891, Train: 1.0393, Val: 1.0420\n",
      "Epoch: 214, Loss: 1.0849, Train: 1.0413, Val: 1.0438\n",
      "Epoch: 215, Loss: 1.0989, Train: 1.0485, Val: 1.0509\n",
      "Epoch: 216, Loss: 1.0919, Train: 1.0440, Val: 1.0464\n",
      "Epoch: 217, Loss: 1.0845, Train: 1.0396, Val: 1.0415\n",
      "Epoch: 218, Loss: 1.0863, Train: 1.0396, Val: 1.0415\n",
      "Epoch: 219, Loss: 1.0837, Train: 1.0404, Val: 1.0427\n",
      "Epoch: 220, Loss: 1.0764, Train: 1.0376, Val: 1.0400\n",
      "Epoch: 221, Loss: 1.0735, Train: 1.0360, Val: 1.0384\n",
      "Epoch: 222, Loss: 1.0719, Train: 1.0357, Val: 1.0383\n",
      "Epoch: 223, Loss: 1.0664, Train: 1.0363, Val: 1.0386\n",
      "Epoch: 224, Loss: 1.0663, Train: 1.0406, Val: 1.0427\n",
      "Epoch: 225, Loss: 1.0680, Train: 1.0337, Val: 1.0364\n",
      "Epoch: 226, Loss: 1.0615, Train: 1.0358, Val: 1.0387\n",
      "Epoch: 227, Loss: 1.0649, Train: 1.0345, Val: 1.0370\n",
      "Epoch: 228, Loss: 1.0592, Train: 1.0359, Val: 1.0383\n",
      "Epoch: 229, Loss: 1.0592, Train: 1.0315, Val: 1.0343\n",
      "Epoch: 230, Loss: 1.0572, Train: 1.0375, Val: 1.0400\n",
      "Epoch: 231, Loss: 1.0546, Train: 1.0625, Val: 1.0641\n",
      "Epoch: 232, Loss: 1.0560, Train: 1.0658, Val: 1.0673\n",
      "Epoch: 233, Loss: 1.0523, Train: 1.0636, Val: 1.0651\n",
      "Epoch: 234, Loss: 1.0513, Train: 1.0627, Val: 1.0642\n",
      "Epoch: 235, Loss: 1.0495, Train: 1.0557, Val: 1.0574\n",
      "Epoch: 236, Loss: 1.0489, Train: 1.0502, Val: 1.0520\n",
      "Epoch: 237, Loss: 1.0532, Train: 1.0361, Val: 1.0386\n",
      "Epoch: 238, Loss: 1.0519, Train: 1.0338, Val: 1.0363\n",
      "Epoch: 239, Loss: 1.0485, Train: 1.0444, Val: 1.0463\n",
      "Epoch: 240, Loss: 1.0510, Train: 1.0310, Val: 1.0335\n",
      "Epoch: 241, Loss: 1.0478, Train: 1.0323, Val: 1.0347\n",
      "Epoch: 242, Loss: 1.0437, Train: 1.0378, Val: 1.0400\n",
      "Epoch: 243, Loss: 1.0467, Train: 1.0267, Val: 1.0293\n",
      "Epoch: 244, Loss: 1.0465, Train: 1.0314, Val: 1.0339\n",
      "Epoch: 245, Loss: 1.0480, Train: 1.0222, Val: 1.0252\n",
      "Epoch: 246, Loss: 1.0427, Train: 1.0294, Val: 1.0329\n",
      "Epoch: 247, Loss: 1.0449, Train: 1.0224, Val: 1.0254\n",
      "Epoch: 248, Loss: 1.0412, Train: 1.0225, Val: 1.0256\n",
      "Epoch: 249, Loss: 1.0386, Train: 1.0279, Val: 1.0314\n",
      "Epoch: 250, Loss: 1.0400, Train: 1.0197, Val: 1.0230\n",
      "Epoch: 251, Loss: 1.0380, Train: 1.0211, Val: 1.0246\n",
      "Epoch: 252, Loss: 1.0360, Train: 1.0217, Val: 1.0250\n",
      "Epoch: 253, Loss: 1.0327, Train: 1.0237, Val: 1.0271\n",
      "Epoch: 254, Loss: 1.0350, Train: 1.0258, Val: 1.0292\n",
      "Epoch: 255, Loss: 1.0338, Train: 1.0193, Val: 1.0226\n",
      "Epoch: 256, Loss: 1.0309, Train: 1.0291, Val: 1.0326\n",
      "Epoch: 257, Loss: 1.0316, Train: 1.0196, Val: 1.0228\n",
      "Epoch: 258, Loss: 1.0302, Train: 1.0279, Val: 1.0314\n",
      "Epoch: 259, Loss: 1.0277, Train: 1.0217, Val: 1.0250\n",
      "Epoch: 260, Loss: 1.0277, Train: 1.0210, Val: 1.0244\n",
      "Epoch: 261, Loss: 1.0256, Train: 1.0183, Val: 1.0217\n",
      "Epoch: 262, Loss: 1.0232, Train: 1.0196, Val: 1.0230\n",
      "Epoch: 263, Loss: 1.0235, Train: 1.0212, Val: 1.0246\n",
      "Epoch: 264, Loss: 1.0225, Train: 1.0226, Val: 1.0261\n",
      "Epoch: 265, Loss: 1.0198, Train: 1.0232, Val: 1.0267\n",
      "Epoch: 266, Loss: 1.0186, Train: 1.0210, Val: 1.0243\n",
      "Epoch: 267, Loss: 1.0191, Train: 1.0240, Val: 1.0274\n",
      "Epoch: 268, Loss: 1.0174, Train: 1.0183, Val: 1.0216\n",
      "Epoch: 269, Loss: 1.0162, Train: 1.0278, Val: 1.0312\n",
      "Epoch: 270, Loss: 1.0169, Train: 1.0146, Val: 1.0176\n",
      "Epoch: 271, Loss: 1.0225, Train: 1.0385, Val: 1.0421\n",
      "Epoch: 272, Loss: 1.0232, Train: 1.0146, Val: 1.0175\n",
      "Epoch: 273, Loss: 1.0280, Train: 1.0349, Val: 1.0386\n",
      "Epoch: 274, Loss: 1.0230, Train: 1.0091, Val: 1.0126\n",
      "Epoch: 275, Loss: 1.0129, Train: 1.0076, Val: 1.0109\n",
      "Epoch: 276, Loss: 1.0122, Train: 1.0149, Val: 1.0185\n",
      "Epoch: 277, Loss: 1.0147, Train: 1.0141, Val: 1.0178\n",
      "Epoch: 278, Loss: 1.0099, Train: 1.0095, Val: 1.0128\n",
      "Epoch: 279, Loss: 1.0126, Train: 1.0156, Val: 1.0193\n",
      "Epoch: 280, Loss: 1.0118, Train: 1.0222, Val: 1.0260\n",
      "Epoch: 281, Loss: 1.0330, Train: 1.0430, Val: 1.0453\n",
      "Epoch: 282, Loss: 1.1193, Train: 1.0379, Val: 1.0415\n",
      "Epoch: 283, Loss: 1.0706, Train: 1.1010, Val: 1.1051\n",
      "Epoch: 284, Loss: 1.1163, Train: 1.0262, Val: 1.0300\n",
      "Epoch: 285, Loss: 1.0918, Train: 1.0262, Val: 1.0295\n",
      "Epoch: 286, Loss: 1.0903, Train: 1.0647, Val: 1.0680\n",
      "Epoch: 287, Loss: 1.0836, Train: 1.0344, Val: 1.0375\n",
      "Epoch: 288, Loss: 1.0583, Train: 1.0446, Val: 1.0464\n",
      "Epoch: 289, Loss: 1.0877, Train: 1.0296, Val: 1.0321\n",
      "Epoch: 290, Loss: 1.0532, Train: 1.0377, Val: 1.0412\n",
      "Epoch: 291, Loss: 1.0571, Train: 1.0249, Val: 1.0287\n",
      "Epoch: 292, Loss: 1.0535, Train: 1.0333, Val: 1.0366\n",
      "Epoch: 293, Loss: 1.0382, Train: 1.0452, Val: 1.0475\n",
      "Epoch: 294, Loss: 1.0512, Train: 1.0217, Val: 1.0246\n",
      "Epoch: 295, Loss: 1.0451, Train: 1.0207, Val: 1.0235\n",
      "Epoch: 296, Loss: 1.0503, Train: 1.0221, Val: 1.0249\n",
      "Epoch: 297, Loss: 1.0504, Train: 1.0170, Val: 1.0198\n",
      "Epoch: 298, Loss: 1.0340, Train: 1.0225, Val: 1.0250\n",
      "Epoch: 299, Loss: 1.0306, Train: 1.0419, Val: 1.0435\n",
      "Epoch: 300, Loss: 1.0368, Train: 1.0275, Val: 1.0295\n",
      "Epoch: 301, Loss: 1.0301, Train: 1.0216, Val: 1.0238\n",
      "Epoch: 302, Loss: 1.0280, Train: 1.0306, Val: 1.0327\n",
      "Epoch: 303, Loss: 1.0230, Train: 1.0181, Val: 1.0210\n",
      "Epoch: 304, Loss: 1.0193, Train: 1.0131, Val: 1.0163\n",
      "Epoch: 305, Loss: 1.0237, Train: 1.0173, Val: 1.0199\n",
      "Epoch: 306, Loss: 1.0166, Train: 1.0136, Val: 1.0163\n",
      "Epoch: 307, Loss: 1.0096, Train: 1.0124, Val: 1.0151\n",
      "Epoch: 308, Loss: 1.0130, Train: 1.0113, Val: 1.0140\n",
      "Epoch: 309, Loss: 1.0069, Train: 1.0052, Val: 1.0086\n",
      "Epoch: 310, Loss: 1.0053, Train: 1.0038, Val: 1.0072\n",
      "Epoch: 311, Loss: 1.0047, Train: 1.0071, Val: 1.0100\n",
      "Epoch: 312, Loss: 1.0014, Train: 1.0043, Val: 1.0073\n",
      "Epoch: 313, Loss: 0.9986, Train: 1.0088, Val: 1.0116\n",
      "Epoch: 314, Loss: 1.0009, Train: 1.0060, Val: 1.0098\n",
      "Epoch: 315, Loss: 1.0187, Train: 1.0196, Val: 1.0221\n",
      "Epoch: 316, Loss: 1.0421, Train: 1.0180, Val: 1.0220\n",
      "Epoch: 317, Loss: 1.1215, Train: 1.0748, Val: 1.0765\n",
      "Epoch: 318, Loss: 1.0333, Train: 1.0652, Val: 1.0674\n",
      "Epoch: 319, Loss: 1.0384, Train: 1.0145, Val: 1.0184\n",
      "Epoch: 320, Loss: 1.0287, Train: 1.0049, Val: 1.0084\n",
      "Epoch: 321, Loss: 1.0132, Train: 1.0382, Val: 1.0401\n",
      "Epoch: 322, Loss: 1.0204, Train: 1.0028, Val: 1.0065\n",
      "Epoch: 323, Loss: 1.0046, Train: 1.0062, Val: 1.0103\n",
      "Epoch: 324, Loss: 0.9995, Train: 1.0040, Val: 1.0075\n",
      "Epoch: 325, Loss: 1.0033, Train: 1.0011, Val: 1.0043\n",
      "Epoch: 326, Loss: 0.9984, Train: 1.0033, Val: 1.0065\n",
      "Epoch: 327, Loss: 1.0008, Train: 0.9991, Val: 1.0022\n",
      "Epoch: 328, Loss: 1.0269, Train: 1.0323, Val: 1.0367\n",
      "Epoch: 329, Loss: 1.0498, Train: 1.0942, Val: 1.0986\n",
      "Epoch: 330, Loss: 1.0590, Train: 1.1330, Val: 1.1368\n",
      "Epoch: 331, Loss: 1.0566, Train: 1.0812, Val: 1.0842\n",
      "Epoch: 332, Loss: 1.0488, Train: 1.0554, Val: 1.0576\n",
      "Epoch: 333, Loss: 1.0635, Train: 1.0543, Val: 1.0565\n",
      "Epoch: 334, Loss: 1.0566, Train: 1.0619, Val: 1.0643\n",
      "Epoch: 335, Loss: 1.0446, Train: 1.0662, Val: 1.0686\n",
      "Epoch: 336, Loss: 1.0433, Train: 1.0637, Val: 1.0660\n",
      "Epoch: 337, Loss: 1.0472, Train: 1.0622, Val: 1.0646\n",
      "Epoch: 338, Loss: 1.0395, Train: 1.0541, Val: 1.0565\n",
      "Epoch: 339, Loss: 1.0403, Train: 1.0226, Val: 1.0244\n",
      "Epoch: 340, Loss: 1.1536, Train: 1.1399, Val: 1.1428\n",
      "Epoch: 341, Loss: 1.1742, Train: 1.0487, Val: 1.0514\n",
      "Epoch: 342, Loss: 1.0755, Train: 1.0525, Val: 1.0544\n",
      "Epoch: 343, Loss: 1.1142, Train: 1.0405, Val: 1.0426\n",
      "Epoch: 344, Loss: 1.0557, Train: 1.0637, Val: 1.0653\n",
      "Epoch: 345, Loss: 1.2060, Train: 1.0343, Val: 1.0366\n",
      "Epoch: 346, Loss: 1.0913, Train: 1.0361, Val: 1.0373\n",
      "Epoch: 347, Loss: 1.0686, Train: 1.0587, Val: 1.0597\n",
      "Epoch: 348, Loss: 1.1103, Train: 1.0582, Val: 1.0590\n",
      "Epoch: 349, Loss: 1.0924, Train: 1.0553, Val: 1.0558\n",
      "Epoch: 350, Loss: 1.0750, Train: 1.0502, Val: 1.0511\n",
      "Epoch: 351, Loss: 1.0686, Train: 1.0622, Val: 1.0637\n",
      "Epoch: 352, Loss: 1.1357, Train: 1.0484, Val: 1.0495\n",
      "Epoch: 353, Loss: 1.0502, Train: 1.1009, Val: 1.1016\n",
      "Epoch: 354, Loss: 1.0952, Train: 1.0473, Val: 1.0484\n",
      "Epoch: 355, Loss: 1.0386, Train: 1.0311, Val: 1.0332\n",
      "Epoch: 356, Loss: 1.0655, Train: 1.0256, Val: 1.0278\n",
      "Epoch: 357, Loss: 1.0402, Train: 1.0437, Val: 1.0456\n",
      "Epoch: 358, Loss: 1.0508, Train: 1.0350, Val: 1.0371\n",
      "Epoch: 359, Loss: 1.0255, Train: 1.0280, Val: 1.0299\n",
      "Epoch: 360, Loss: 1.0343, Train: 1.0420, Val: 1.0436\n",
      "Epoch: 361, Loss: 1.0232, Train: 1.0526, Val: 1.0544\n",
      "Epoch: 362, Loss: 1.0369, Train: 1.0255, Val: 1.0279\n",
      "Epoch: 363, Loss: 1.0217, Train: 1.0146, Val: 1.0174\n",
      "Epoch: 364, Loss: 1.0273, Train: 1.0153, Val: 1.0179\n",
      "Epoch: 365, Loss: 1.0157, Train: 1.0343, Val: 1.0364\n",
      "Epoch: 366, Loss: 1.0165, Train: 1.0327, Val: 1.0348\n",
      "Epoch: 367, Loss: 1.0112, Train: 1.0175, Val: 1.0197\n",
      "Epoch: 368, Loss: 1.0086, Train: 1.0148, Val: 1.0171\n",
      "Epoch: 369, Loss: 1.0074, Train: 1.0257, Val: 1.0280\n",
      "Epoch: 370, Loss: 1.0046, Train: 1.0291, Val: 1.0312\n",
      "Epoch: 371, Loss: 1.0052, Train: 1.0144, Val: 1.0167\n",
      "Epoch: 372, Loss: 0.9995, Train: 1.0098, Val: 1.0121\n",
      "Epoch: 373, Loss: 1.0000, Train: 1.0112, Val: 1.0136\n",
      "Epoch: 374, Loss: 0.9952, Train: 1.0100, Val: 1.0126\n",
      "Epoch: 375, Loss: 0.9960, Train: 1.0026, Val: 1.0056\n",
      "Epoch: 376, Loss: 0.9916, Train: 1.0015, Val: 1.0045\n",
      "Epoch: 377, Loss: 0.9914, Train: 1.0050, Val: 1.0078\n",
      "Epoch: 378, Loss: 0.9879, Train: 1.0072, Val: 1.0100\n",
      "Epoch: 379, Loss: 0.9880, Train: 1.0019, Val: 1.0049\n",
      "Epoch: 380, Loss: 0.9841, Train: 1.0002, Val: 1.0034\n",
      "Epoch: 381, Loss: 0.9844, Train: 1.0021, Val: 1.0050\n",
      "Epoch: 382, Loss: 0.9815, Train: 1.0062, Val: 1.0088\n",
      "Epoch: 383, Loss: 0.9800, Train: 1.0036, Val: 1.0062\n",
      "Epoch: 384, Loss: 0.9774, Train: 0.9984, Val: 1.0013\n",
      "Epoch: 385, Loss: 0.9756, Train: 0.9981, Val: 1.0010\n",
      "Epoch: 386, Loss: 0.9735, Train: 1.0006, Val: 1.0033\n",
      "Epoch: 387, Loss: 0.9723, Train: 1.0002, Val: 1.0028\n",
      "Epoch: 388, Loss: 0.9701, Train: 1.0000, Val: 1.0026\n",
      "Epoch: 389, Loss: 0.9687, Train: 1.0017, Val: 1.0042\n",
      "Epoch: 390, Loss: 0.9666, Train: 1.0026, Val: 1.0052\n",
      "Epoch: 391, Loss: 0.9651, Train: 1.0004, Val: 1.0031\n",
      "Epoch: 392, Loss: 0.9631, Train: 1.0003, Val: 1.0029\n",
      "Epoch: 393, Loss: 0.9613, Train: 1.0011, Val: 1.0036\n",
      "Epoch: 394, Loss: 0.9599, Train: 0.9985, Val: 1.0011\n",
      "Epoch: 395, Loss: 0.9579, Train: 0.9955, Val: 0.9983\n",
      "Epoch: 396, Loss: 0.9568, Train: 0.9961, Val: 0.9988\n",
      "Epoch: 397, Loss: 0.9549, Train: 0.9983, Val: 1.0008\n",
      "Epoch: 398, Loss: 0.9543, Train: 0.9946, Val: 0.9973\n",
      "Epoch: 399, Loss: 0.9526, Train: 0.9924, Val: 0.9952\n",
      "Epoch: 400, Loss: 0.9518, Train: 0.9920, Val: 0.9948\n",
      "Epoch: 401, Loss: 0.9505, Train: 0.9905, Val: 0.9933\n",
      "Epoch: 402, Loss: 0.9494, Train: 0.9863, Val: 0.9895\n",
      "Epoch: 403, Loss: 0.9485, Train: 0.9864, Val: 0.9895\n",
      "Epoch: 404, Loss: 0.9471, Train: 0.9850, Val: 0.9882\n",
      "Epoch: 405, Loss: 0.9460, Train: 0.9809, Val: 0.9845\n",
      "Epoch: 406, Loss: 0.9449, Train: 0.9804, Val: 0.9839\n",
      "Epoch: 407, Loss: 0.9437, Train: 0.9781, Val: 0.9819\n",
      "Epoch: 408, Loss: 0.9427, Train: 0.9764, Val: 0.9803\n",
      "Epoch: 409, Loss: 0.9416, Train: 0.9750, Val: 0.9790\n",
      "Epoch: 410, Loss: 0.9405, Train: 0.9747, Val: 0.9787\n",
      "Epoch: 411, Loss: 0.9395, Train: 0.9726, Val: 0.9769\n",
      "Epoch: 412, Loss: 0.9385, Train: 0.9733, Val: 0.9774\n",
      "Epoch: 413, Loss: 0.9376, Train: 0.9704, Val: 0.9750\n",
      "Epoch: 414, Loss: 0.9369, Train: 0.9724, Val: 0.9766\n",
      "Epoch: 415, Loss: 0.9366, Train: 0.9711, Val: 0.9764\n",
      "Epoch: 416, Loss: 0.9388, Train: 0.9701, Val: 0.9747\n",
      "Epoch: 417, Loss: 0.9348, Train: 0.9682, Val: 0.9730\n",
      "Epoch: 418, Loss: 0.9331, Train: 0.9678, Val: 0.9729\n",
      "Epoch: 419, Loss: 0.9325, Train: 0.9692, Val: 0.9738\n",
      "Epoch: 420, Loss: 0.9322, Train: 0.9680, Val: 0.9735\n",
      "Epoch: 421, Loss: 0.9334, Train: 0.9677, Val: 0.9727\n",
      "Epoch: 422, Loss: 0.9309, Train: 0.9655, Val: 0.9707\n",
      "Epoch: 423, Loss: 0.9292, Train: 0.9657, Val: 0.9709\n",
      "Epoch: 424, Loss: 0.9280, Train: 0.9656, Val: 0.9706\n",
      "Epoch: 425, Loss: 0.9274, Train: 0.9640, Val: 0.9696\n",
      "Epoch: 426, Loss: 0.9275, Train: 0.9659, Val: 0.9710\n",
      "Epoch: 427, Loss: 0.9275, Train: 0.9655, Val: 0.9715\n",
      "Epoch: 428, Loss: 0.9288, Train: 0.9645, Val: 0.9697\n",
      "Epoch: 429, Loss: 0.9253, Train: 0.9627, Val: 0.9684\n",
      "Epoch: 430, Loss: 0.9238, Train: 0.9616, Val: 0.9671\n",
      "Epoch: 431, Loss: 0.9229, Train: 0.9631, Val: 0.9684\n",
      "Epoch: 432, Loss: 0.9226, Train: 0.9619, Val: 0.9680\n",
      "Epoch: 433, Loss: 0.9235, Train: 0.9642, Val: 0.9693\n",
      "Epoch: 434, Loss: 0.9225, Train: 0.9646, Val: 0.9712\n",
      "Epoch: 435, Loss: 0.9247, Train: 0.9614, Val: 0.9670\n",
      "Epoch: 436, Loss: 0.9207, Train: 0.9621, Val: 0.9673\n",
      "Epoch: 437, Loss: 0.9203, Train: 0.9665, Val: 0.9734\n",
      "Epoch: 438, Loss: 0.9230, Train: 0.9596, Val: 0.9656\n",
      "Epoch: 439, Loss: 0.9181, Train: 0.9727, Val: 0.9770\n",
      "Epoch: 440, Loss: 0.9255, Train: 1.0717, Val: 1.0788\n",
      "Epoch: 441, Loss: 0.9450, Train: 1.0976, Val: 1.1044\n",
      "Epoch: 442, Loss: 0.9447, Train: 1.0181, Val: 1.0255\n",
      "Epoch: 443, Loss: 0.9297, Train: 0.9852, Val: 0.9906\n",
      "Epoch: 444, Loss: 0.9361, Train: 0.9824, Val: 0.9882\n",
      "Epoch: 445, Loss: 0.9271, Train: 0.9739, Val: 0.9800\n",
      "Epoch: 446, Loss: 0.9259, Train: 0.9623, Val: 0.9684\n",
      "Epoch: 447, Loss: 0.9288, Train: 0.9661, Val: 0.9728\n",
      "Epoch: 448, Loss: 0.9241, Train: 0.9686, Val: 0.9750\n",
      "Epoch: 449, Loss: 0.9219, Train: 0.9637, Val: 0.9696\n",
      "Epoch: 450, Loss: 0.9201, Train: 0.9844, Val: 0.9919\n",
      "Epoch: 451, Loss: 0.9240, Train: 0.9776, Val: 0.9855\n",
      "Epoch: 452, Loss: 0.9184, Train: 0.9616, Val: 0.9686\n",
      "Epoch: 453, Loss: 0.9183, Train: 0.9618, Val: 0.9683\n",
      "Epoch: 454, Loss: 0.9164, Train: 0.9822, Val: 0.9855\n",
      "Epoch: 455, Loss: 0.9181, Train: 0.9862, Val: 0.9899\n",
      "Epoch: 456, Loss: 0.9220, Train: 0.9688, Val: 0.9745\n",
      "Epoch: 457, Loss: 0.9128, Train: 0.9734, Val: 0.9805\n",
      "Epoch: 458, Loss: 0.9165, Train: 1.0474, Val: 1.0555\n",
      "Epoch: 459, Loss: 0.9154, Train: 0.9859, Val: 0.9938\n",
      "Epoch: 460, Loss: 0.9130, Train: 0.9593, Val: 0.9659\n",
      "Epoch: 461, Loss: 0.9113, Train: 0.9635, Val: 0.9686\n",
      "Epoch: 462, Loss: 0.9095, Train: 0.9825, Val: 0.9862\n",
      "Epoch: 463, Loss: 0.9102, Train: 0.9782, Val: 0.9821\n",
      "Epoch: 464, Loss: 0.9086, Train: 0.9905, Val: 0.9939\n",
      "Epoch: 465, Loss: 0.9076, Train: 1.0245, Val: 1.0269\n",
      "Epoch: 466, Loss: 0.9064, Train: 1.0277, Val: 1.0301\n",
      "Epoch: 467, Loss: 0.9067, Train: 1.0048, Val: 1.0078\n",
      "Epoch: 468, Loss: 0.9062, Train: 0.9920, Val: 0.9954\n",
      "Epoch: 469, Loss: 0.9055, Train: 1.0195, Val: 1.0220\n",
      "Epoch: 470, Loss: 0.9027, Train: 1.0421, Val: 1.0441\n",
      "Epoch: 471, Loss: 0.9042, Train: 1.0056, Val: 1.0085\n",
      "Epoch: 472, Loss: 0.9029, Train: 1.0079, Val: 1.0107\n",
      "Epoch: 473, Loss: 0.9039, Train: 1.0069, Val: 1.0099\n",
      "Epoch: 474, Loss: 0.9019, Train: 1.0287, Val: 1.0311\n",
      "Epoch: 475, Loss: 0.9020, Train: 0.9947, Val: 0.9982\n",
      "Epoch: 476, Loss: 0.9002, Train: 0.9814, Val: 0.9854\n",
      "Epoch: 477, Loss: 0.8996, Train: 0.9854, Val: 0.9892\n",
      "Epoch: 478, Loss: 0.8989, Train: 0.9909, Val: 0.9945\n",
      "Epoch: 479, Loss: 0.8985, Train: 0.9803, Val: 0.9844\n",
      "Epoch: 480, Loss: 0.8986, Train: 0.9656, Val: 0.9705\n",
      "Epoch: 481, Loss: 0.8986, Train: 0.9755, Val: 0.9800\n",
      "Epoch: 482, Loss: 0.8995, Train: 0.9596, Val: 0.9651\n",
      "Epoch: 483, Loss: 0.8999, Train: 0.9678, Val: 0.9727\n",
      "Epoch: 484, Loss: 0.9010, Train: 0.9564, Val: 0.9622\n",
      "Epoch: 485, Loss: 0.8989, Train: 0.9619, Val: 0.9672\n",
      "Epoch: 486, Loss: 0.8966, Train: 0.9539, Val: 0.9598\n",
      "Epoch: 487, Loss: 0.8946, Train: 0.9528, Val: 0.9589\n",
      "Epoch: 488, Loss: 0.8946, Train: 0.9585, Val: 0.9640\n",
      "Epoch: 489, Loss: 0.8952, Train: 0.9494, Val: 0.9559\n",
      "Epoch: 490, Loss: 0.8961, Train: 0.9512, Val: 0.9577\n",
      "Epoch: 491, Loss: 0.8970, Train: 0.9485, Val: 0.9552\n",
      "Epoch: 492, Loss: 0.8979, Train: 0.9545, Val: 0.9606\n",
      "Epoch: 493, Loss: 0.8967, Train: 0.9467, Val: 0.9535\n",
      "Epoch: 494, Loss: 0.8938, Train: 0.9490, Val: 0.9555\n",
      "Epoch: 495, Loss: 0.8917, Train: 0.9510, Val: 0.9572\n",
      "Epoch: 496, Loss: 0.8914, Train: 0.9455, Val: 0.9526\n",
      "Epoch: 497, Loss: 0.8923, Train: 0.9503, Val: 0.9568\n",
      "Epoch: 498, Loss: 0.8930, Train: 0.9486, Val: 0.9552\n",
      "Epoch: 499, Loss: 0.8927, Train: 0.9477, Val: 0.9548\n",
      "Epoch: 500, Loss: 0.8930, Train: 0.9477, Val: 0.9543\n",
      "Test RMSE: 0.9728\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.644112       3.583673\n",
      "std      1727.484387     741.673176       0.575577       1.116938\n",
      "min         0.000000       0.000000       0.765577       1.000000\n",
      "25%      1500.000000     259.000000       3.275227       3.000000\n",
      "50%      3066.000000     693.000000       3.676763       4.000000\n",
      "75%      4472.000000    1292.000000       4.045634       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1093.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.30699547055863\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## KPGIN\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for k, kernel in variations:\n",
    "    args.K = k\n",
    "    args.kernel = kernel\n",
    "    model = Model(hidden_channels=32).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    def train(loss_type=\"mse\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        splitted_data = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        edge_types=[('user', 'rates', 'movie')],\n",
    "        rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    "    )(data)\n",
    "        pred = model(train_data[\"user\", \"rates\", \"movie\"], train_data['user', 'movie'].edge_label_index)\n",
    "        target = train_data['user', 'movie'].edge_label\n",
    "        if loss_type ==\"mse\":\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        elif loss_type == \"BPR\":\n",
    "            loss = F.mse_loss(pred, target)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return float(loss)\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test(data):\n",
    "        data = data.to(device)\n",
    "        model.eval()\n",
    "        pred = model(data[\"user\", \"rates\", \"movie\"],\n",
    "                     data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        return float(rmse)\n",
    "    \n",
    "    # ep = 500\n",
    "    # for i in [1]:\n",
    "    #     model = get_model__()\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        train_data = train_data.to(device)\n",
    "        loss = train()\n",
    "        train_rmse = test(train_data)\n",
    "        val_rmse = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "              f'Val: {val_rmse:.4f}')\n",
    "    with torch.no_grad():\n",
    "        test_data = test_data.to(device)\n",
    "        pred = model(test_data[\"user\", \"rates\", \"movie\"],\n",
    "                     test_data['user', 'movie'].edge_label_index)\n",
    "        \n",
    "        # pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
    "        #              test_data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = test_data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "    userId = test_data['user', 'movie'].edge_label_index[0].cpu().numpy()\n",
    "    movieId = test_data['user', 'movie'].edge_label_index[1].cpu().numpy()\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "    predicted_df = pd.DataFrame({'userId': userId, 'movieId': movieId, 'rating': pred, 'target': target})\n",
    "    \n",
    "    print(predicted_df.describe())\n",
    "    \n",
    "    print(hit_rate_top_k(k=10, predicted_df=predicted_df))\n",
    "\n",
    "    print(\"-------------------------------------------------\"*10)\n",
    "    print(\"-------------------------------------------------\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "K:  1\n",
      "Kernel:  gd\n",
      "Model Name:  KPGraphSAGE\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.9511, Train: 3.6618, Val: 3.6646\n",
      "Epoch: 002, Loss: 9.6557, Train: 3.4709, Val: 3.4738\n",
      "Epoch: 003, Loss: 3.3306, Train: 3.1450, Val: 3.1478\n",
      "Epoch: 004, Loss: 1.6042, Train: 2.9102, Val: 2.9131\n",
      "Epoch: 005, Loss: 2.8506, Train: 2.8992, Val: 2.9019\n",
      "Epoch: 006, Loss: 1.3729, Train: 2.9302, Val: 2.9328\n",
      "Epoch: 007, Loss: 1.5366, Train: 2.9162, Val: 2.9188\n",
      "Epoch: 008, Loss: 1.4247, Train: 2.8571, Val: 2.8596\n",
      "Epoch: 009, Loss: 1.2482, Train: 2.7896, Val: 2.7921\n",
      "Epoch: 010, Loss: 1.3278, Train: 2.7727, Val: 2.7752\n",
      "Epoch: 011, Loss: 1.3337, Train: 2.8036, Val: 2.8061\n",
      "Epoch: 012, Loss: 1.2289, Train: 2.8383, Val: 2.8409\n",
      "Epoch: 013, Loss: 1.2615, Train: 2.8409, Val: 2.8436\n",
      "Epoch: 014, Loss: 1.2963, Train: 2.7993, Val: 2.8020\n",
      "Epoch: 015, Loss: 1.2293, Train: 2.7326, Val: 2.7353\n",
      "Epoch: 016, Loss: 1.2185, Train: 2.6787, Val: 2.6814\n",
      "Epoch: 017, Loss: 1.2508, Train: 2.6607, Val: 2.6634\n",
      "Epoch: 018, Loss: 1.2187, Train: 2.6625, Val: 2.6651\n",
      "Epoch: 019, Loss: 1.2221, Train: 2.6463, Val: 2.6489\n",
      "Epoch: 020, Loss: 1.2264, Train: 2.6060, Val: 2.6086\n",
      "Epoch: 021, Loss: 1.2030, Train: 2.5527, Val: 2.5554\n",
      "Epoch: 022, Loss: 1.1894, Train: 2.5118, Val: 2.5145\n",
      "Epoch: 023, Loss: 1.1910, Train: 2.5005, Val: 2.5031\n",
      "Epoch: 024, Loss: 1.1839, Train: 2.4856, Val: 2.4882\n",
      "Epoch: 025, Loss: 1.1805, Train: 2.4570, Val: 2.4595\n",
      "Epoch: 026, Loss: 1.1721, Train: 2.4251, Val: 2.4276\n",
      "Epoch: 027, Loss: 1.1724, Train: 2.4038, Val: 2.4062\n",
      "Epoch: 028, Loss: 1.1681, Train: 2.3819, Val: 2.3843\n",
      "Epoch: 029, Loss: 1.1598, Train: 2.3400, Val: 2.3425\n",
      "Epoch: 030, Loss: 1.1525, Train: 2.2802, Val: 2.2827\n",
      "Epoch: 031, Loss: 1.1469, Train: 2.2210, Val: 2.2235\n",
      "Epoch: 032, Loss: 1.1437, Train: 2.1787, Val: 2.1813\n",
      "Epoch: 033, Loss: 1.1386, Train: 2.1518, Val: 2.1543\n",
      "Epoch: 034, Loss: 1.1344, Train: 2.1230, Val: 2.1256\n",
      "Epoch: 035, Loss: 1.1327, Train: 2.0815, Val: 2.0842\n",
      "Epoch: 036, Loss: 1.1293, Train: 2.0351, Val: 2.0379\n",
      "Epoch: 037, Loss: 1.1264, Train: 1.9986, Val: 2.0015\n",
      "Epoch: 038, Loss: 1.1236, Train: 1.9672, Val: 1.9702\n",
      "Epoch: 039, Loss: 1.1215, Train: 1.9219, Val: 1.9249\n",
      "Epoch: 040, Loss: 1.1198, Train: 1.8595, Val: 1.8625\n",
      "Epoch: 041, Loss: 1.1175, Train: 1.8030, Val: 1.8059\n",
      "Epoch: 042, Loss: 1.1167, Train: 1.7699, Val: 1.7729\n",
      "Epoch: 043, Loss: 1.1147, Train: 1.7501, Val: 1.7532\n",
      "Epoch: 044, Loss: 1.1126, Train: 1.7185, Val: 1.7217\n",
      "Epoch: 045, Loss: 1.1110, Train: 1.6706, Val: 1.6737\n",
      "Epoch: 046, Loss: 1.1096, Train: 1.6261, Val: 1.6291\n",
      "Epoch: 047, Loss: 1.1081, Train: 1.5962, Val: 1.5991\n",
      "Epoch: 048, Loss: 1.1062, Train: 1.5678, Val: 1.5707\n",
      "Epoch: 049, Loss: 1.1050, Train: 1.5300, Val: 1.5328\n",
      "Epoch: 050, Loss: 1.1032, Train: 1.4982, Val: 1.5010\n",
      "Epoch: 051, Loss: 1.1016, Train: 1.4867, Val: 1.4896\n",
      "Epoch: 052, Loss: 1.1000, Train: 1.4774, Val: 1.4802\n",
      "Epoch: 053, Loss: 1.0986, Train: 1.4407, Val: 1.4435\n",
      "Epoch: 054, Loss: 1.0966, Train: 1.3950, Val: 1.3977\n",
      "Epoch: 055, Loss: 1.0950, Train: 1.3708, Val: 1.3734\n",
      "Epoch: 056, Loss: 1.0935, Train: 1.3559, Val: 1.3585\n",
      "Epoch: 057, Loss: 1.0920, Train: 1.3275, Val: 1.3301\n",
      "Epoch: 058, Loss: 1.0901, Train: 1.3011, Val: 1.3037\n",
      "Epoch: 059, Loss: 1.0885, Train: 1.2960, Val: 1.2987\n",
      "Epoch: 060, Loss: 1.0869, Train: 1.2878, Val: 1.2905\n",
      "Epoch: 061, Loss: 1.0852, Train: 1.2563, Val: 1.2590\n",
      "Epoch: 062, Loss: 1.0834, Train: 1.2377, Val: 1.2404\n",
      "Epoch: 063, Loss: 1.0817, Train: 1.2370, Val: 1.2397\n",
      "Epoch: 064, Loss: 1.0800, Train: 1.2190, Val: 1.2217\n",
      "Epoch: 065, Loss: 1.0782, Train: 1.2030, Val: 1.2058\n",
      "Epoch: 066, Loss: 1.0764, Train: 1.2016, Val: 1.2044\n",
      "Epoch: 067, Loss: 1.0746, Train: 1.1703, Val: 1.1730\n",
      "Epoch: 068, Loss: 1.0727, Train: 1.1662, Val: 1.1689\n",
      "Epoch: 069, Loss: 1.0717, Train: 1.1039, Val: 1.1064\n",
      "Epoch: 070, Loss: 1.0713, Train: 1.1723, Val: 1.1752\n",
      "Epoch: 071, Loss: 1.0717, Train: 1.0635, Val: 1.0657\n",
      "Epoch: 072, Loss: 1.0726, Train: 1.1305, Val: 1.1333\n",
      "Epoch: 073, Loss: 1.0643, Train: 1.1388, Val: 1.1417\n",
      "Epoch: 074, Loss: 1.0625, Train: 1.0541, Val: 1.0564\n",
      "Epoch: 075, Loss: 1.0688, Train: 1.2061, Val: 1.2094\n",
      "Epoch: 076, Loss: 1.0701, Train: 1.0473, Val: 1.0485\n",
      "Epoch: 077, Loss: 1.0820, Train: 1.0372, Val: 1.0391\n",
      "Epoch: 078, Loss: 1.0651, Train: 1.1086, Val: 1.1117\n",
      "Epoch: 079, Loss: 1.0772, Train: 1.0423, Val: 1.0450\n",
      "Epoch: 080, Loss: 1.0579, Train: 1.0757, Val: 1.0789\n",
      "Epoch: 081, Loss: 1.0609, Train: 1.3423, Val: 1.3462\n",
      "Epoch: 082, Loss: 1.0827, Train: 1.1807, Val: 1.1803\n",
      "Epoch: 083, Loss: 1.1028, Train: 1.2067, Val: 1.2061\n",
      "Epoch: 084, Loss: 1.0784, Train: 1.1170, Val: 1.1175\n",
      "Epoch: 085, Loss: 1.1088, Train: 1.1354, Val: 1.1360\n",
      "Epoch: 086, Loss: 1.0790, Train: 1.1215, Val: 1.1230\n",
      "Epoch: 087, Loss: 1.0800, Train: 1.1122, Val: 1.1151\n",
      "Epoch: 088, Loss: 1.0644, Train: 1.1836, Val: 1.1869\n",
      "Epoch: 089, Loss: 1.0788, Train: 1.1173, Val: 1.1207\n",
      "Epoch: 090, Loss: 1.0522, Train: 1.0322, Val: 1.0344\n",
      "Epoch: 091, Loss: 1.0582, Train: 1.0473, Val: 1.0485\n",
      "Epoch: 092, Loss: 1.0538, Train: 1.0425, Val: 1.0440\n",
      "Epoch: 093, Loss: 1.0589, Train: 1.0645, Val: 1.0648\n",
      "Epoch: 094, Loss: 1.0469, Train: 1.1020, Val: 1.1015\n",
      "Epoch: 095, Loss: 1.0498, Train: 1.0670, Val: 1.0671\n",
      "Epoch: 096, Loss: 1.0451, Train: 1.0421, Val: 1.0429\n",
      "Epoch: 097, Loss: 1.0459, Train: 1.0779, Val: 1.0783\n",
      "Epoch: 098, Loss: 1.0421, Train: 1.0956, Val: 1.0958\n",
      "Epoch: 099, Loss: 1.0439, Train: 1.0527, Val: 1.0535\n",
      "Epoch: 100, Loss: 1.0380, Train: 1.0426, Val: 1.0439\n",
      "Epoch: 101, Loss: 1.0368, Train: 1.0576, Val: 1.0583\n",
      "Epoch: 102, Loss: 1.0357, Train: 1.0549, Val: 1.0556\n",
      "Epoch: 103, Loss: 1.0357, Train: 1.0349, Val: 1.0362\n",
      "Epoch: 104, Loss: 1.0337, Train: 1.0285, Val: 1.0300\n",
      "Epoch: 105, Loss: 1.0318, Train: 1.0416, Val: 1.0427\n",
      "Epoch: 106, Loss: 1.0309, Train: 1.0265, Val: 1.0281\n",
      "Epoch: 107, Loss: 1.0285, Train: 1.0153, Val: 1.0175\n",
      "Epoch: 108, Loss: 1.0292, Train: 1.0245, Val: 1.0262\n",
      "Epoch: 109, Loss: 1.0269, Train: 1.0419, Val: 1.0431\n",
      "Epoch: 110, Loss: 1.0261, Train: 1.0309, Val: 1.0323\n",
      "Epoch: 111, Loss: 1.0247, Train: 1.0260, Val: 1.0277\n",
      "Epoch: 112, Loss: 1.0237, Train: 1.0270, Val: 1.0287\n",
      "Epoch: 113, Loss: 1.0227, Train: 1.0121, Val: 1.0146\n",
      "Epoch: 114, Loss: 1.0213, Train: 1.0202, Val: 1.0236\n",
      "Epoch: 115, Loss: 1.0211, Train: 1.0106, Val: 1.0137\n",
      "Epoch: 116, Loss: 1.0192, Train: 1.0103, Val: 1.0131\n",
      "Epoch: 117, Loss: 1.0184, Train: 1.0099, Val: 1.0129\n",
      "Epoch: 118, Loss: 1.0178, Train: 1.0092, Val: 1.0121\n",
      "Epoch: 119, Loss: 1.0163, Train: 1.0114, Val: 1.0140\n",
      "Epoch: 120, Loss: 1.0159, Train: 1.0098, Val: 1.0126\n",
      "Epoch: 121, Loss: 1.0146, Train: 1.0102, Val: 1.0130\n",
      "Epoch: 122, Loss: 1.0135, Train: 1.0092, Val: 1.0120\n",
      "Epoch: 123, Loss: 1.0129, Train: 1.0079, Val: 1.0114\n",
      "Epoch: 124, Loss: 1.0118, Train: 1.0067, Val: 1.0100\n",
      "Epoch: 125, Loss: 1.0107, Train: 1.0089, Val: 1.0117\n",
      "Epoch: 126, Loss: 1.0099, Train: 1.0069, Val: 1.0099\n",
      "Epoch: 127, Loss: 1.0089, Train: 1.0078, Val: 1.0107\n",
      "Epoch: 128, Loss: 1.0079, Train: 1.0111, Val: 1.0137\n",
      "Epoch: 129, Loss: 1.0071, Train: 1.0045, Val: 1.0080\n",
      "Epoch: 130, Loss: 1.0063, Train: 1.0039, Val: 1.0072\n",
      "Epoch: 131, Loss: 1.0053, Train: 1.0040, Val: 1.0076\n",
      "Epoch: 132, Loss: 1.0043, Train: 1.0074, Val: 1.0105\n",
      "Epoch: 133, Loss: 1.0034, Train: 1.0084, Val: 1.0115\n",
      "Epoch: 134, Loss: 1.0025, Train: 1.0035, Val: 1.0072\n",
      "Epoch: 135, Loss: 1.0018, Train: 1.0117, Val: 1.0145\n",
      "Epoch: 136, Loss: 1.0011, Train: 1.0028, Val: 1.0066\n",
      "Epoch: 137, Loss: 1.0005, Train: 1.0228, Val: 1.0252\n",
      "Epoch: 138, Loss: 1.0004, Train: 1.0507, Val: 1.0556\n",
      "Epoch: 139, Loss: 1.0006, Train: 1.0013, Val: 1.0049\n",
      "Epoch: 140, Loss: 1.0038, Train: 1.1123, Val: 1.1173\n",
      "Epoch: 141, Loss: 1.0012, Train: 1.0038, Val: 1.0070\n",
      "Epoch: 142, Loss: 1.0013, Train: 1.0052, Val: 1.0094\n",
      "Epoch: 143, Loss: 0.9958, Train: 1.0050, Val: 1.0093\n",
      "Epoch: 144, Loss: 0.9949, Train: 1.0248, Val: 1.0271\n",
      "Epoch: 145, Loss: 0.9986, Train: 1.0476, Val: 1.0528\n",
      "Epoch: 146, Loss: 1.0009, Train: 1.0247, Val: 1.0273\n",
      "Epoch: 147, Loss: 1.0083, Train: 1.0107, Val: 1.0155\n",
      "Epoch: 148, Loss: 0.9943, Train: 1.0877, Val: 1.0931\n",
      "Epoch: 149, Loss: 1.0044, Train: 1.2321, Val: 1.2312\n",
      "Epoch: 150, Loss: 1.0223, Train: 1.0261, Val: 1.0318\n",
      "Epoch: 151, Loss: 0.9956, Train: 1.3347, Val: 1.3381\n",
      "Epoch: 152, Loss: 1.0122, Train: 1.1809, Val: 1.1811\n",
      "Epoch: 153, Loss: 1.0140, Train: 1.1527, Val: 1.1535\n",
      "Epoch: 154, Loss: 1.0016, Train: 1.2116, Val: 1.2140\n",
      "Epoch: 155, Loss: 1.0116, Train: 1.2296, Val: 1.2325\n",
      "Epoch: 156, Loss: 0.9912, Train: 1.2520, Val: 1.2553\n",
      "Epoch: 157, Loss: 1.0045, Train: 1.4002, Val: 1.4038\n",
      "Epoch: 158, Loss: 0.9975, Train: 1.2345, Val: 1.2374\n",
      "Epoch: 159, Loss: 0.9894, Train: 1.1728, Val: 1.1760\n",
      "Epoch: 160, Loss: 0.9958, Train: 1.1771, Val: 1.1810\n",
      "Epoch: 161, Loss: 0.9904, Train: 1.2453, Val: 1.2502\n",
      "Epoch: 162, Loss: 0.9864, Train: 1.2022, Val: 1.2082\n",
      "Epoch: 163, Loss: 0.9877, Train: 1.1434, Val: 1.1495\n",
      "Epoch: 164, Loss: 0.9866, Train: 1.1650, Val: 1.1708\n",
      "Epoch: 165, Loss: 0.9838, Train: 1.0074, Val: 1.0121\n",
      "Epoch: 166, Loss: 0.9841, Train: 1.0011, Val: 1.0055\n",
      "Epoch: 167, Loss: 0.9827, Train: 1.0386, Val: 1.0447\n",
      "Epoch: 168, Loss: 0.9803, Train: 0.9986, Val: 1.0043\n",
      "Epoch: 169, Loss: 0.9786, Train: 0.9954, Val: 0.9998\n",
      "Epoch: 170, Loss: 0.9785, Train: 0.9953, Val: 1.0007\n",
      "Epoch: 171, Loss: 0.9775, Train: 1.0196, Val: 1.0229\n",
      "Epoch: 172, Loss: 0.9768, Train: 1.0089, Val: 1.0128\n",
      "Epoch: 173, Loss: 0.9741, Train: 1.0030, Val: 1.0080\n",
      "Epoch: 174, Loss: 0.9746, Train: 1.0316, Val: 1.0347\n",
      "Epoch: 175, Loss: 0.9752, Train: 0.9904, Val: 0.9956\n",
      "Epoch: 176, Loss: 0.9722, Train: 1.0402, Val: 1.0435\n",
      "Epoch: 177, Loss: 0.9717, Train: 1.0038, Val: 1.0082\n",
      "Epoch: 178, Loss: 0.9687, Train: 0.9928, Val: 0.9978\n",
      "Epoch: 179, Loss: 0.9692, Train: 1.0570, Val: 1.0598\n",
      "Epoch: 180, Loss: 0.9698, Train: 0.9928, Val: 0.9984\n",
      "Epoch: 181, Loss: 0.9683, Train: 1.0456, Val: 1.0488\n",
      "Epoch: 182, Loss: 0.9732, Train: 0.9952, Val: 1.0008\n",
      "Epoch: 183, Loss: 0.9664, Train: 0.9921, Val: 0.9978\n",
      "Epoch: 184, Loss: 0.9686, Train: 1.0752, Val: 1.0780\n",
      "Epoch: 185, Loss: 0.9731, Train: 0.9952, Val: 1.0015\n",
      "Epoch: 186, Loss: 0.9696, Train: 0.9945, Val: 1.0004\n",
      "Epoch: 187, Loss: 0.9798, Train: 1.1628, Val: 1.1639\n",
      "Epoch: 188, Loss: 0.9792, Train: 1.0177, Val: 1.0221\n",
      "Epoch: 189, Loss: 0.9801, Train: 1.0295, Val: 1.0362\n",
      "Epoch: 190, Loss: 0.9720, Train: 1.0122, Val: 1.0176\n",
      "Epoch: 191, Loss: 0.9734, Train: 1.0043, Val: 1.0094\n",
      "Epoch: 192, Loss: 0.9782, Train: 1.0458, Val: 1.0491\n",
      "Epoch: 193, Loss: 0.9763, Train: 1.0048, Val: 1.0092\n",
      "Epoch: 194, Loss: 0.9765, Train: 1.1412, Val: 1.1426\n",
      "Epoch: 195, Loss: 0.9744, Train: 1.1372, Val: 1.1388\n",
      "Epoch: 196, Loss: 0.9636, Train: 1.0451, Val: 1.0490\n",
      "Epoch: 197, Loss: 0.9685, Train: 1.1340, Val: 1.1359\n",
      "Epoch: 198, Loss: 0.9630, Train: 1.1130, Val: 1.1149\n",
      "Epoch: 199, Loss: 0.9655, Train: 1.1310, Val: 1.1322\n",
      "Epoch: 200, Loss: 0.9610, Train: 1.0069, Val: 1.0111\n",
      "Epoch: 201, Loss: 0.9615, Train: 1.0907, Val: 1.0930\n",
      "Epoch: 202, Loss: 0.9573, Train: 1.0705, Val: 1.0733\n",
      "Epoch: 203, Loss: 0.9557, Train: 1.0159, Val: 1.0201\n",
      "Epoch: 204, Loss: 0.9546, Train: 1.0555, Val: 1.0588\n",
      "Epoch: 205, Loss: 0.9536, Train: 1.0096, Val: 1.0144\n",
      "Epoch: 206, Loss: 0.9546, Train: 1.1259, Val: 1.1275\n",
      "Epoch: 207, Loss: 0.9606, Train: 1.0017, Val: 1.0070\n",
      "Epoch: 208, Loss: 0.9565, Train: 1.0422, Val: 1.0459\n",
      "Epoch: 209, Loss: 0.9505, Train: 1.1219, Val: 1.1240\n",
      "Epoch: 210, Loss: 0.9543, Train: 0.9867, Val: 0.9941\n",
      "Epoch: 211, Loss: 0.9619, Train: 1.2931, Val: 1.2923\n",
      "Epoch: 212, Loss: 0.9900, Train: 1.1656, Val: 1.1666\n",
      "Epoch: 213, Loss: 0.9681, Train: 1.0143, Val: 1.0210\n",
      "Epoch: 214, Loss: 0.9801, Train: 1.0000, Val: 1.0057\n",
      "Epoch: 215, Loss: 0.9794, Train: 1.0146, Val: 1.0193\n",
      "Epoch: 216, Loss: 0.9624, Train: 1.0075, Val: 1.0136\n",
      "Epoch: 217, Loss: 0.9833, Train: 0.9985, Val: 1.0037\n",
      "Epoch: 218, Loss: 0.9521, Train: 1.0072, Val: 1.0124\n",
      "Epoch: 219, Loss: 0.9690, Train: 1.0028, Val: 1.0088\n",
      "Epoch: 220, Loss: 0.9537, Train: 1.0955, Val: 1.0983\n",
      "Epoch: 221, Loss: 0.9565, Train: 1.1132, Val: 1.1158\n",
      "Epoch: 222, Loss: 0.9552, Train: 1.0032, Val: 1.0091\n",
      "Epoch: 223, Loss: 0.9493, Train: 1.0066, Val: 1.0119\n",
      "Epoch: 224, Loss: 0.9476, Train: 1.0285, Val: 1.0322\n",
      "Epoch: 225, Loss: 0.9494, Train: 1.0194, Val: 1.0234\n",
      "Epoch: 226, Loss: 0.9450, Train: 0.9897, Val: 0.9958\n",
      "Epoch: 227, Loss: 0.9439, Train: 1.0378, Val: 1.0419\n",
      "Epoch: 228, Loss: 0.9425, Train: 1.0830, Val: 1.0857\n",
      "Epoch: 229, Loss: 0.9396, Train: 1.0474, Val: 1.0508\n",
      "Epoch: 230, Loss: 0.9396, Train: 1.0199, Val: 1.0239\n",
      "Epoch: 231, Loss: 0.9383, Train: 0.9840, Val: 0.9896\n",
      "Epoch: 232, Loss: 0.9370, Train: 0.9888, Val: 0.9945\n",
      "Epoch: 233, Loss: 0.9351, Train: 1.0110, Val: 1.0158\n",
      "Epoch: 234, Loss: 0.9351, Train: 0.9981, Val: 1.0038\n",
      "Epoch: 235, Loss: 0.9330, Train: 1.0119, Val: 1.0176\n",
      "Epoch: 236, Loss: 0.9335, Train: 1.0159, Val: 1.0208\n",
      "Epoch: 237, Loss: 0.9320, Train: 1.0096, Val: 1.0152\n",
      "Epoch: 238, Loss: 0.9312, Train: 1.0200, Val: 1.0248\n",
      "Epoch: 239, Loss: 0.9303, Train: 0.9795, Val: 0.9859\n",
      "Epoch: 240, Loss: 0.9293, Train: 1.0085, Val: 1.0140\n",
      "Epoch: 241, Loss: 0.9325, Train: 0.9946, Val: 0.9998\n",
      "Epoch: 242, Loss: 0.9342, Train: 1.0391, Val: 1.0431\n",
      "Epoch: 243, Loss: 0.9282, Train: 1.0137, Val: 1.0194\n",
      "Epoch: 244, Loss: 0.9297, Train: 1.0047, Val: 1.0094\n",
      "Epoch: 245, Loss: 0.9386, Train: 0.9884, Val: 0.9940\n",
      "Epoch: 246, Loss: 0.9277, Train: 1.0555, Val: 1.0606\n",
      "Epoch: 247, Loss: 0.9429, Train: 1.0054, Val: 1.0117\n",
      "Epoch: 248, Loss: 0.9542, Train: 1.0610, Val: 1.0667\n",
      "Epoch: 249, Loss: 0.9560, Train: 1.4243, Val: 1.4218\n",
      "Epoch: 250, Loss: 0.9774, Train: 1.3569, Val: 1.3558\n",
      "Epoch: 251, Loss: 0.9940, Train: 0.9889, Val: 0.9948\n",
      "Epoch: 252, Loss: 0.9671, Train: 1.3002, Val: 1.3043\n",
      "Epoch: 253, Loss: 0.9863, Train: 1.0973, Val: 1.1015\n",
      "Epoch: 254, Loss: 0.9655, Train: 1.1597, Val: 1.1626\n",
      "Epoch: 255, Loss: 0.9722, Train: 1.0803, Val: 1.0842\n",
      "Epoch: 256, Loss: 0.9645, Train: 1.0114, Val: 1.0167\n",
      "Epoch: 257, Loss: 0.9508, Train: 1.0632, Val: 1.0688\n",
      "Epoch: 258, Loss: 0.9511, Train: 1.0500, Val: 1.0557\n",
      "Epoch: 259, Loss: 0.9590, Train: 1.0210, Val: 1.0249\n",
      "Epoch: 260, Loss: 0.9488, Train: 1.0129, Val: 1.0168\n",
      "Epoch: 261, Loss: 0.9462, Train: 0.9790, Val: 0.9841\n",
      "Epoch: 262, Loss: 0.9446, Train: 1.0032, Val: 1.0078\n",
      "Epoch: 263, Loss: 0.9406, Train: 1.0145, Val: 1.0187\n",
      "Epoch: 264, Loss: 0.9418, Train: 0.9937, Val: 0.9985\n",
      "Epoch: 265, Loss: 0.9376, Train: 1.0038, Val: 1.0080\n",
      "Epoch: 266, Loss: 0.9363, Train: 1.0251, Val: 1.0289\n",
      "Epoch: 267, Loss: 0.9355, Train: 0.9908, Val: 0.9962\n",
      "Epoch: 268, Loss: 0.9313, Train: 0.9842, Val: 0.9906\n",
      "Epoch: 269, Loss: 0.9321, Train: 0.9748, Val: 0.9812\n",
      "Epoch: 270, Loss: 0.9295, Train: 0.9771, Val: 0.9831\n",
      "Epoch: 271, Loss: 0.9281, Train: 0.9769, Val: 0.9828\n",
      "Epoch: 272, Loss: 0.9273, Train: 0.9674, Val: 0.9743\n",
      "Epoch: 273, Loss: 0.9251, Train: 0.9981, Val: 1.0058\n",
      "Epoch: 274, Loss: 0.9256, Train: 0.9657, Val: 0.9722\n",
      "Epoch: 275, Loss: 0.9231, Train: 0.9899, Val: 0.9951\n",
      "Epoch: 276, Loss: 0.9228, Train: 0.9740, Val: 0.9800\n",
      "Epoch: 277, Loss: 0.9208, Train: 0.9677, Val: 0.9742\n",
      "Epoch: 278, Loss: 0.9204, Train: 0.9682, Val: 0.9749\n",
      "Epoch: 279, Loss: 0.9192, Train: 0.9680, Val: 0.9750\n",
      "Epoch: 280, Loss: 0.9185, Train: 0.9718, Val: 0.9786\n",
      "Epoch: 281, Loss: 0.9175, Train: 0.9784, Val: 0.9853\n",
      "Epoch: 282, Loss: 0.9166, Train: 1.0227, Val: 1.0278\n",
      "Epoch: 283, Loss: 0.9168, Train: 0.9738, Val: 0.9811\n",
      "Epoch: 284, Loss: 0.9154, Train: 0.9739, Val: 0.9804\n",
      "Epoch: 285, Loss: 0.9152, Train: 0.9644, Val: 0.9723\n",
      "Epoch: 286, Loss: 0.9157, Train: 1.0397, Val: 1.0439\n",
      "Epoch: 287, Loss: 0.9176, Train: 0.9778, Val: 0.9858\n",
      "Epoch: 288, Loss: 0.9184, Train: 1.1403, Val: 1.1424\n",
      "Epoch: 289, Loss: 0.9250, Train: 0.9834, Val: 0.9903\n",
      "Epoch: 290, Loss: 0.9202, Train: 1.0699, Val: 1.0738\n",
      "Epoch: 291, Loss: 0.9136, Train: 1.0188, Val: 1.0246\n",
      "Epoch: 292, Loss: 0.9110, Train: 0.9785, Val: 0.9853\n",
      "Epoch: 293, Loss: 0.9124, Train: 1.1456, Val: 1.1477\n",
      "Epoch: 294, Loss: 0.9155, Train: 1.0078, Val: 1.0138\n",
      "Epoch: 295, Loss: 0.9118, Train: 0.9700, Val: 0.9768\n",
      "Epoch: 296, Loss: 0.9091, Train: 1.0365, Val: 1.0415\n",
      "Epoch: 297, Loss: 0.9108, Train: 1.0754, Val: 1.0797\n",
      "Epoch: 298, Loss: 0.9108, Train: 0.9729, Val: 0.9798\n",
      "Epoch: 299, Loss: 0.9130, Train: 1.1273, Val: 1.1312\n",
      "Epoch: 300, Loss: 0.9129, Train: 1.0372, Val: 1.0420\n",
      "Epoch: 301, Loss: 0.9117, Train: 0.9789, Val: 0.9857\n",
      "Epoch: 302, Loss: 0.9085, Train: 0.9835, Val: 0.9905\n",
      "Epoch: 303, Loss: 0.9103, Train: 1.0539, Val: 1.0575\n",
      "Epoch: 304, Loss: 0.9148, Train: 1.1074, Val: 1.1107\n",
      "Epoch: 305, Loss: 0.9099, Train: 1.1220, Val: 1.1291\n",
      "Epoch: 306, Loss: 0.9131, Train: 1.1760, Val: 1.1825\n",
      "Epoch: 307, Loss: 0.9086, Train: 0.9688, Val: 0.9775\n",
      "Epoch: 308, Loss: 0.9118, Train: 1.0977, Val: 1.1046\n",
      "Epoch: 309, Loss: 0.9099, Train: 1.1164, Val: 1.1241\n",
      "Epoch: 310, Loss: 0.9104, Train: 1.3392, Val: 1.3385\n",
      "Epoch: 311, Loss: 0.9336, Train: 0.9620, Val: 0.9705\n",
      "Epoch: 312, Loss: 0.9155, Train: 1.2841, Val: 1.2879\n",
      "Epoch: 313, Loss: 0.9248, Train: 0.9746, Val: 0.9834\n",
      "Epoch: 314, Loss: 0.9153, Train: 0.9910, Val: 0.9984\n",
      "Epoch: 315, Loss: 0.9372, Train: 0.9849, Val: 0.9926\n",
      "Epoch: 316, Loss: 0.9217, Train: 1.1281, Val: 1.1340\n",
      "Epoch: 317, Loss: 0.9302, Train: 1.1174, Val: 1.1199\n",
      "Epoch: 318, Loss: 0.9381, Train: 1.3687, Val: 1.3673\n",
      "Epoch: 319, Loss: 0.9546, Train: 1.3007, Val: 1.3004\n",
      "Epoch: 320, Loss: 0.9438, Train: 1.1540, Val: 1.1561\n",
      "Epoch: 321, Loss: 0.9448, Train: 1.2054, Val: 1.2068\n",
      "Epoch: 322, Loss: 0.9294, Train: 1.2752, Val: 1.2757\n",
      "Epoch: 323, Loss: 0.9235, Train: 1.0656, Val: 1.0698\n",
      "Epoch: 324, Loss: 0.9250, Train: 0.9744, Val: 0.9815\n",
      "Epoch: 325, Loss: 0.9212, Train: 0.9843, Val: 0.9912\n",
      "Epoch: 326, Loss: 0.9163, Train: 0.9657, Val: 0.9729\n",
      "Epoch: 327, Loss: 0.9158, Train: 0.9765, Val: 0.9836\n",
      "Epoch: 328, Loss: 0.9160, Train: 0.9732, Val: 0.9814\n",
      "Epoch: 329, Loss: 0.9127, Train: 1.0671, Val: 1.0757\n",
      "Epoch: 330, Loss: 0.9112, Train: 1.0981, Val: 1.1056\n",
      "Epoch: 331, Loss: 0.9078, Train: 1.0886, Val: 1.0958\n",
      "Epoch: 332, Loss: 0.9071, Train: 1.0455, Val: 1.0534\n",
      "Epoch: 333, Loss: 0.9064, Train: 0.9921, Val: 1.0005\n",
      "Epoch: 334, Loss: 0.9040, Train: 0.9817, Val: 0.9902\n",
      "Epoch: 335, Loss: 0.9027, Train: 0.9521, Val: 0.9599\n",
      "Epoch: 336, Loss: 0.9012, Train: 0.9617, Val: 0.9694\n",
      "Epoch: 337, Loss: 0.9014, Train: 1.0279, Val: 1.0361\n",
      "Epoch: 338, Loss: 0.8993, Train: 1.0645, Val: 1.0725\n",
      "Epoch: 339, Loss: 0.8978, Train: 1.0877, Val: 1.0953\n",
      "Epoch: 340, Loss: 0.8973, Train: 0.9762, Val: 0.9850\n",
      "Epoch: 341, Loss: 0.8959, Train: 0.9518, Val: 0.9603\n",
      "Epoch: 342, Loss: 0.8949, Train: 0.9676, Val: 0.9760\n",
      "Epoch: 343, Loss: 0.8950, Train: 0.9610, Val: 0.9686\n",
      "Epoch: 344, Loss: 0.8945, Train: 0.9586, Val: 0.9673\n",
      "Epoch: 345, Loss: 0.8941, Train: 0.9482, Val: 0.9563\n",
      "Epoch: 346, Loss: 0.8916, Train: 0.9487, Val: 0.9572\n",
      "Epoch: 347, Loss: 0.8904, Train: 0.9548, Val: 0.9635\n",
      "Epoch: 348, Loss: 0.8906, Train: 0.9609, Val: 0.9698\n",
      "Epoch: 349, Loss: 0.8914, Train: 1.0080, Val: 1.0164\n",
      "Epoch: 350, Loss: 0.8948, Train: 0.9627, Val: 0.9704\n",
      "Epoch: 351, Loss: 0.8914, Train: 0.9478, Val: 0.9560\n",
      "Epoch: 352, Loss: 0.8890, Train: 0.9612, Val: 0.9688\n",
      "Epoch: 353, Loss: 0.8882, Train: 0.9540, Val: 0.9627\n",
      "Epoch: 354, Loss: 0.8906, Train: 0.9771, Val: 0.9837\n",
      "Epoch: 355, Loss: 0.9023, Train: 0.9983, Val: 1.0071\n",
      "Epoch: 356, Loss: 0.9107, Train: 1.2461, Val: 1.2467\n",
      "Epoch: 357, Loss: 0.9374, Train: 0.9711, Val: 0.9777\n",
      "Epoch: 358, Loss: 0.9300, Train: 1.1243, Val: 1.1288\n",
      "Epoch: 359, Loss: 0.8981, Train: 1.1261, Val: 1.1294\n",
      "Epoch: 360, Loss: 0.9098, Train: 0.9771, Val: 0.9824\n",
      "Epoch: 361, Loss: 0.9226, Train: 1.2535, Val: 1.2543\n",
      "Epoch: 362, Loss: 0.8970, Train: 0.9895, Val: 0.9966\n",
      "Epoch: 363, Loss: 0.9010, Train: 1.1213, Val: 1.1292\n",
      "Epoch: 364, Loss: 0.9135, Train: 0.9659, Val: 0.9740\n",
      "Epoch: 365, Loss: 0.8981, Train: 1.1065, Val: 1.1103\n",
      "Epoch: 366, Loss: 0.9052, Train: 0.9539, Val: 0.9617\n",
      "Epoch: 367, Loss: 0.8962, Train: 1.0927, Val: 1.0987\n",
      "Epoch: 368, Loss: 0.9028, Train: 0.9751, Val: 0.9825\n",
      "Epoch: 369, Loss: 0.8958, Train: 0.9996, Val: 1.0069\n",
      "Epoch: 370, Loss: 0.8991, Train: 1.1135, Val: 1.1193\n",
      "Epoch: 371, Loss: 0.9099, Train: 0.9681, Val: 0.9758\n",
      "Epoch: 372, Loss: 0.8937, Train: 1.0709, Val: 1.0762\n",
      "Epoch: 373, Loss: 0.9126, Train: 0.9775, Val: 0.9851\n",
      "Epoch: 374, Loss: 0.8934, Train: 1.0526, Val: 1.0593\n",
      "Epoch: 375, Loss: 0.9020, Train: 0.9485, Val: 0.9554\n",
      "Epoch: 376, Loss: 0.8874, Train: 0.9858, Val: 0.9928\n",
      "Epoch: 377, Loss: 0.8995, Train: 0.9583, Val: 0.9679\n",
      "Epoch: 378, Loss: 0.8859, Train: 1.0343, Val: 1.0422\n",
      "Epoch: 379, Loss: 0.8946, Train: 0.9563, Val: 0.9641\n",
      "Epoch: 380, Loss: 0.8852, Train: 0.9938, Val: 1.0002\n",
      "Epoch: 381, Loss: 0.8884, Train: 0.9548, Val: 0.9639\n",
      "Epoch: 382, Loss: 0.8853, Train: 1.0004, Val: 1.0088\n",
      "Epoch: 383, Loss: 0.8849, Train: 0.9626, Val: 0.9709\n",
      "Epoch: 384, Loss: 0.8837, Train: 0.9520, Val: 0.9616\n",
      "Epoch: 385, Loss: 0.8823, Train: 0.9819, Val: 0.9917\n",
      "Epoch: 386, Loss: 0.8819, Train: 0.9832, Val: 0.9925\n",
      "Epoch: 387, Loss: 0.8798, Train: 0.9484, Val: 0.9567\n",
      "Epoch: 388, Loss: 0.8802, Train: 0.9399, Val: 0.9487\n",
      "Epoch: 389, Loss: 0.8783, Train: 1.0534, Val: 1.0622\n",
      "Epoch: 390, Loss: 0.8778, Train: 1.2056, Val: 1.2067\n",
      "Epoch: 391, Loss: 0.8809, Train: 1.2511, Val: 1.2507\n",
      "Epoch: 392, Loss: 0.8895, Train: 1.0103, Val: 1.0190\n",
      "Epoch: 393, Loss: 0.8849, Train: 1.2695, Val: 1.2742\n",
      "Epoch: 394, Loss: 0.8872, Train: 1.1561, Val: 1.1638\n",
      "Epoch: 395, Loss: 0.8855, Train: 0.9739, Val: 0.9832\n",
      "Epoch: 396, Loss: 0.8822, Train: 1.0075, Val: 1.0163\n",
      "Epoch: 397, Loss: 0.8849, Train: 0.9500, Val: 0.9579\n",
      "Epoch: 398, Loss: 0.8793, Train: 0.9557, Val: 0.9633\n",
      "Epoch: 399, Loss: 0.8804, Train: 0.9641, Val: 0.9734\n",
      "Epoch: 400, Loss: 0.8756, Train: 1.0496, Val: 1.0573\n",
      "Epoch: 401, Loss: 0.8788, Train: 0.9808, Val: 0.9904\n",
      "Epoch: 402, Loss: 0.8747, Train: 0.9446, Val: 0.9542\n",
      "Epoch: 403, Loss: 0.8768, Train: 0.9540, Val: 0.9608\n",
      "Epoch: 404, Loss: 0.8768, Train: 0.9563, Val: 0.9637\n",
      "Epoch: 405, Loss: 0.8736, Train: 0.9475, Val: 0.9567\n",
      "Epoch: 406, Loss: 0.8758, Train: 0.9906, Val: 0.9994\n",
      "Epoch: 407, Loss: 0.8748, Train: 0.9800, Val: 0.9891\n",
      "Epoch: 408, Loss: 0.8750, Train: 0.9915, Val: 0.9980\n",
      "Epoch: 409, Loss: 0.8729, Train: 0.9712, Val: 0.9774\n",
      "Epoch: 410, Loss: 0.8716, Train: 1.0027, Val: 1.0117\n",
      "Epoch: 411, Loss: 0.8712, Train: 0.9437, Val: 0.9536\n",
      "Epoch: 412, Loss: 0.8710, Train: 0.9391, Val: 0.9476\n",
      "Epoch: 413, Loss: 0.8702, Train: 0.9538, Val: 0.9630\n",
      "Epoch: 414, Loss: 0.8685, Train: 0.9358, Val: 0.9445\n",
      "Epoch: 415, Loss: 0.8691, Train: 0.9699, Val: 0.9763\n",
      "Epoch: 416, Loss: 0.8674, Train: 0.9367, Val: 0.9457\n",
      "Epoch: 417, Loss: 0.8671, Train: 0.9707, Val: 0.9804\n",
      "Epoch: 418, Loss: 0.8679, Train: 0.9360, Val: 0.9441\n",
      "Epoch: 419, Loss: 0.8661, Train: 0.9725, Val: 0.9788\n",
      "Epoch: 420, Loss: 0.8669, Train: 0.9377, Val: 0.9466\n",
      "Epoch: 421, Loss: 0.8679, Train: 0.9660, Val: 0.9758\n",
      "Epoch: 422, Loss: 0.8689, Train: 1.0329, Val: 1.0375\n",
      "Epoch: 423, Loss: 0.8741, Train: 0.9398, Val: 0.9481\n",
      "Epoch: 424, Loss: 0.8692, Train: 0.9381, Val: 0.9470\n",
      "Epoch: 425, Loss: 0.8678, Train: 0.9563, Val: 0.9635\n",
      "Epoch: 426, Loss: 0.8641, Train: 0.9387, Val: 0.9466\n",
      "Epoch: 427, Loss: 0.8647, Train: 0.9359, Val: 0.9441\n",
      "Epoch: 428, Loss: 0.8665, Train: 0.9774, Val: 0.9871\n",
      "Epoch: 429, Loss: 0.8680, Train: 1.0486, Val: 1.0527\n",
      "Epoch: 430, Loss: 0.8738, Train: 0.9371, Val: 0.9465\n",
      "Epoch: 431, Loss: 0.8681, Train: 0.9328, Val: 0.9422\n",
      "Epoch: 432, Loss: 0.8642, Train: 0.9950, Val: 1.0008\n",
      "Epoch: 433, Loss: 0.8656, Train: 0.9383, Val: 0.9475\n",
      "Epoch: 434, Loss: 0.8697, Train: 0.9945, Val: 1.0001\n",
      "Epoch: 435, Loss: 0.8668, Train: 1.0259, Val: 1.0352\n",
      "Epoch: 436, Loss: 0.8629, Train: 0.9527, Val: 0.9625\n",
      "Epoch: 437, Loss: 0.8648, Train: 1.0572, Val: 1.0612\n",
      "Epoch: 438, Loss: 0.8664, Train: 1.0738, Val: 1.0827\n",
      "Epoch: 439, Loss: 0.8668, Train: 0.9607, Val: 0.9710\n",
      "Epoch: 440, Loss: 0.8626, Train: 0.9420, Val: 0.9507\n",
      "Epoch: 441, Loss: 0.8644, Train: 0.9603, Val: 0.9705\n",
      "Epoch: 442, Loss: 0.8670, Train: 0.9774, Val: 0.9836\n",
      "Epoch: 443, Loss: 0.8653, Train: 0.9302, Val: 0.9389\n",
      "Epoch: 444, Loss: 0.8636, Train: 1.0289, Val: 1.0388\n",
      "Epoch: 445, Loss: 0.8688, Train: 0.9335, Val: 0.9422\n",
      "Epoch: 446, Loss: 0.8621, Train: 0.9406, Val: 0.9487\n",
      "Epoch: 447, Loss: 0.8618, Train: 0.9348, Val: 0.9449\n",
      "Epoch: 448, Loss: 0.8604, Train: 0.9391, Val: 0.9487\n",
      "Epoch: 449, Loss: 0.8594, Train: 0.9401, Val: 0.9495\n",
      "Epoch: 450, Loss: 0.8590, Train: 0.9301, Val: 0.9392\n",
      "Epoch: 451, Loss: 0.8593, Train: 0.9529, Val: 0.9627\n",
      "Epoch: 452, Loss: 0.8585, Train: 0.9315, Val: 0.9411\n",
      "Epoch: 453, Loss: 0.8612, Train: 0.9530, Val: 0.9611\n",
      "Epoch: 454, Loss: 0.8685, Train: 0.9647, Val: 0.9729\n",
      "Epoch: 455, Loss: 0.8786, Train: 1.0393, Val: 1.0454\n",
      "Epoch: 456, Loss: 0.8981, Train: 0.9526, Val: 0.9607\n",
      "Epoch: 457, Loss: 0.8808, Train: 1.2987, Val: 1.2994\n",
      "Epoch: 458, Loss: 0.8782, Train: 1.0032, Val: 1.0093\n",
      "Epoch: 459, Loss: 0.8793, Train: 0.9532, Val: 0.9608\n",
      "Epoch: 460, Loss: 0.8898, Train: 1.2549, Val: 1.2563\n",
      "Epoch: 461, Loss: 0.8874, Train: 1.2013, Val: 1.2081\n",
      "Epoch: 462, Loss: 0.8793, Train: 1.1055, Val: 1.1128\n",
      "Epoch: 463, Loss: 0.8783, Train: 1.0565, Val: 1.0609\n",
      "Epoch: 464, Loss: 0.8830, Train: 0.9551, Val: 0.9626\n",
      "Epoch: 465, Loss: 0.8680, Train: 0.9599, Val: 0.9691\n",
      "Epoch: 466, Loss: 0.8705, Train: 0.9476, Val: 0.9568\n",
      "Epoch: 467, Loss: 0.8673, Train: 0.9533, Val: 0.9621\n",
      "Epoch: 468, Loss: 0.8710, Train: 0.9343, Val: 0.9430\n",
      "Epoch: 469, Loss: 0.8665, Train: 1.2077, Val: 1.2086\n",
      "Epoch: 470, Loss: 0.8656, Train: 1.0581, Val: 1.0628\n",
      "Epoch: 471, Loss: 0.8645, Train: 1.1266, Val: 1.1347\n",
      "Epoch: 472, Loss: 0.8631, Train: 1.1629, Val: 1.1705\n",
      "Epoch: 473, Loss: 0.8612, Train: 0.9642, Val: 0.9737\n",
      "Epoch: 474, Loss: 0.8623, Train: 0.9356, Val: 0.9449\n",
      "Epoch: 475, Loss: 0.8600, Train: 0.9728, Val: 0.9831\n",
      "Epoch: 476, Loss: 0.8602, Train: 0.9298, Val: 0.9397\n",
      "Epoch: 477, Loss: 0.8579, Train: 0.9392, Val: 0.9487\n",
      "Epoch: 478, Loss: 0.8586, Train: 0.9594, Val: 0.9690\n",
      "Epoch: 479, Loss: 0.8570, Train: 0.9545, Val: 0.9645\n",
      "Epoch: 480, Loss: 0.8573, Train: 0.9271, Val: 0.9364\n",
      "Epoch: 481, Loss: 0.8559, Train: 0.9340, Val: 0.9437\n",
      "Epoch: 482, Loss: 0.8547, Train: 0.9362, Val: 0.9457\n",
      "Epoch: 483, Loss: 0.8551, Train: 0.9425, Val: 0.9502\n",
      "Epoch: 484, Loss: 0.8536, Train: 0.9272, Val: 0.9370\n",
      "Epoch: 485, Loss: 0.8541, Train: 0.9990, Val: 1.0092\n",
      "Epoch: 486, Loss: 0.8533, Train: 0.9287, Val: 0.9381\n",
      "Epoch: 487, Loss: 0.8533, Train: 1.0503, Val: 1.0549\n",
      "Epoch: 488, Loss: 0.8532, Train: 0.9277, Val: 0.9366\n",
      "Epoch: 489, Loss: 0.8517, Train: 0.9786, Val: 0.9889\n",
      "Epoch: 490, Loss: 0.8519, Train: 0.9245, Val: 0.9340\n",
      "Epoch: 491, Loss: 0.8512, Train: 0.9266, Val: 0.9363\n",
      "Epoch: 492, Loss: 0.8511, Train: 1.0023, Val: 1.0081\n",
      "Epoch: 493, Loss: 0.8516, Train: 1.0039, Val: 1.0140\n",
      "Epoch: 494, Loss: 0.8519, Train: 1.1442, Val: 1.1467\n",
      "Epoch: 495, Loss: 0.8528, Train: 1.0816, Val: 1.0903\n",
      "Epoch: 496, Loss: 0.8544, Train: 1.1410, Val: 1.1435\n",
      "Epoch: 497, Loss: 0.8562, Train: 1.1115, Val: 1.1198\n",
      "Epoch: 498, Loss: 0.8549, Train: 0.9979, Val: 1.0039\n",
      "Epoch: 499, Loss: 0.8525, Train: 0.9560, Val: 0.9632\n",
      "Epoch: 500, Loss: 0.8508, Train: 1.0036, Val: 1.0139\n",
      "Test RMSE: 1.0242\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.292947       3.583673\n",
      "std      1727.484387     741.673176       0.650369       1.116938\n",
      "min         0.000000       0.000000       0.647071       1.000000\n",
      "25%      1500.000000     259.000000       2.866636       3.000000\n",
      "50%      3066.000000     693.000000       3.319508       4.000000\n",
      "75%      4472.000000    1292.000000       3.745315       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1095.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.36738802214393\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  1\n",
      "Kernel:  spd\n",
      "Model Name:  KPGraphSAGE\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.6686, Train: 3.5971, Val: 3.5999\n",
      "Epoch: 002, Loss: 9.8964, Train: 3.4081, Val: 3.4109\n",
      "Epoch: 003, Loss: 4.9907, Train: 3.1614, Val: 3.1642\n",
      "Epoch: 004, Loss: 1.6031, Train: 2.8162, Val: 2.8189\n",
      "Epoch: 005, Loss: 3.1348, Train: 2.8347, Val: 2.8372\n",
      "Epoch: 006, Loss: 1.6059, Train: 2.8573, Val: 2.8598\n",
      "Epoch: 007, Loss: 1.4256, Train: 2.8113, Val: 2.8138\n",
      "Epoch: 008, Loss: 1.3107, Train: 2.7335, Val: 2.7361\n",
      "Epoch: 009, Loss: 1.2397, Train: 2.6409, Val: 2.6436\n",
      "Epoch: 010, Loss: 1.2618, Train: 2.5869, Val: 2.5897\n",
      "Epoch: 011, Loss: 1.2741, Train: 2.5720, Val: 2.5748\n",
      "Epoch: 012, Loss: 1.2452, Train: 2.5451, Val: 2.5480\n",
      "Epoch: 013, Loss: 1.2589, Train: 2.4671, Val: 2.4699\n",
      "Epoch: 014, Loss: 1.2170, Train: 2.3943, Val: 2.3971\n",
      "Epoch: 015, Loss: 1.2458, Train: 2.3683, Val: 2.3711\n",
      "Epoch: 016, Loss: 1.2186, Train: 2.3545, Val: 2.3574\n",
      "Epoch: 017, Loss: 1.2110, Train: 2.3107, Val: 2.3137\n",
      "Epoch: 018, Loss: 1.2018, Train: 2.2383, Val: 2.2414\n",
      "Epoch: 019, Loss: 1.1983, Train: 2.1872, Val: 2.1902\n",
      "Epoch: 020, Loss: 1.1979, Train: 2.1752, Val: 2.1782\n",
      "Epoch: 021, Loss: 1.1787, Train: 2.1572, Val: 2.1601\n",
      "Epoch: 022, Loss: 1.1818, Train: 2.1057, Val: 2.1085\n",
      "Epoch: 023, Loss: 1.1739, Train: 2.0405, Val: 2.0431\n",
      "Epoch: 024, Loss: 1.1702, Train: 2.0009, Val: 2.0036\n",
      "Epoch: 025, Loss: 1.1647, Train: 1.9900, Val: 1.9928\n",
      "Epoch: 026, Loss: 1.1590, Train: 1.9719, Val: 1.9747\n",
      "Epoch: 027, Loss: 1.1597, Train: 1.9220, Val: 1.9248\n",
      "Epoch: 028, Loss: 1.1504, Train: 1.8620, Val: 1.8647\n",
      "Epoch: 029, Loss: 1.1498, Train: 1.8331, Val: 1.8357\n",
      "Epoch: 030, Loss: 1.1485, Train: 1.8295, Val: 1.8321\n",
      "Epoch: 031, Loss: 1.1443, Train: 1.8067, Val: 1.8094\n",
      "Epoch: 032, Loss: 1.1427, Train: 1.7634, Val: 1.7660\n",
      "Epoch: 033, Loss: 1.1393, Train: 1.7100, Val: 1.7126\n",
      "Epoch: 034, Loss: 1.1396, Train: 1.6821, Val: 1.6847\n",
      "Epoch: 035, Loss: 1.1362, Train: 1.6663, Val: 1.6689\n",
      "Epoch: 036, Loss: 1.1349, Train: 1.6280, Val: 1.6306\n",
      "Epoch: 037, Loss: 1.1335, Train: 1.5722, Val: 1.5749\n",
      "Epoch: 038, Loss: 1.1325, Train: 1.5450, Val: 1.5478\n",
      "Epoch: 039, Loss: 1.1299, Train: 1.5308, Val: 1.5336\n",
      "Epoch: 040, Loss: 1.1283, Train: 1.5059, Val: 1.5087\n",
      "Epoch: 041, Loss: 1.1272, Train: 1.4666, Val: 1.4694\n",
      "Epoch: 042, Loss: 1.1255, Train: 1.4346, Val: 1.4373\n",
      "Epoch: 043, Loss: 1.1243, Train: 1.4181, Val: 1.4209\n",
      "Epoch: 044, Loss: 1.1222, Train: 1.4002, Val: 1.4032\n",
      "Epoch: 045, Loss: 1.1210, Train: 1.3679, Val: 1.3709\n",
      "Epoch: 046, Loss: 1.1195, Train: 1.3338, Val: 1.3368\n",
      "Epoch: 047, Loss: 1.1182, Train: 1.3131, Val: 1.3161\n",
      "Epoch: 048, Loss: 1.1165, Train: 1.2958, Val: 1.2988\n",
      "Epoch: 049, Loss: 1.1156, Train: 1.2706, Val: 1.2736\n",
      "Epoch: 050, Loss: 1.1140, Train: 1.2437, Val: 1.2466\n",
      "Epoch: 051, Loss: 1.1128, Train: 1.2285, Val: 1.2315\n",
      "Epoch: 052, Loss: 1.1114, Train: 1.2197, Val: 1.2227\n",
      "Epoch: 053, Loss: 1.1102, Train: 1.2048, Val: 1.2079\n",
      "Epoch: 054, Loss: 1.1087, Train: 1.1885, Val: 1.1915\n",
      "Epoch: 055, Loss: 1.1073, Train: 1.1827, Val: 1.1857\n",
      "Epoch: 056, Loss: 1.1056, Train: 1.1831, Val: 1.1861\n",
      "Epoch: 057, Loss: 1.1043, Train: 1.1719, Val: 1.1749\n",
      "Epoch: 058, Loss: 1.1028, Train: 1.1537, Val: 1.1567\n",
      "Epoch: 059, Loss: 1.1014, Train: 1.1445, Val: 1.1475\n",
      "Epoch: 060, Loss: 1.0999, Train: 1.1382, Val: 1.1412\n",
      "Epoch: 061, Loss: 1.0985, Train: 1.1258, Val: 1.1288\n",
      "Epoch: 062, Loss: 1.0971, Train: 1.1148, Val: 1.1178\n",
      "Epoch: 063, Loss: 1.0956, Train: 1.1095, Val: 1.1125\n",
      "Epoch: 064, Loss: 1.0942, Train: 1.1057, Val: 1.1087\n",
      "Epoch: 065, Loss: 1.0927, Train: 1.0998, Val: 1.1029\n",
      "Epoch: 066, Loss: 1.0912, Train: 1.0937, Val: 1.0967\n",
      "Epoch: 067, Loss: 1.0897, Train: 1.0873, Val: 1.0903\n",
      "Epoch: 068, Loss: 1.0882, Train: 1.0821, Val: 1.0851\n",
      "Epoch: 069, Loss: 1.0867, Train: 1.0784, Val: 1.0814\n",
      "Epoch: 070, Loss: 1.0852, Train: 1.0746, Val: 1.0776\n",
      "Epoch: 071, Loss: 1.0837, Train: 1.0702, Val: 1.0733\n",
      "Epoch: 072, Loss: 1.0821, Train: 1.0710, Val: 1.0742\n",
      "Epoch: 073, Loss: 1.0804, Train: 1.0666, Val: 1.0697\n",
      "Epoch: 074, Loss: 1.0788, Train: 1.0600, Val: 1.0630\n",
      "Epoch: 075, Loss: 1.0771, Train: 1.0595, Val: 1.0626\n",
      "Epoch: 076, Loss: 1.0754, Train: 1.0568, Val: 1.0599\n",
      "Epoch: 077, Loss: 1.0737, Train: 1.0566, Val: 1.0597\n",
      "Epoch: 078, Loss: 1.0718, Train: 1.0567, Val: 1.0599\n",
      "Epoch: 079, Loss: 1.0701, Train: 1.0544, Val: 1.0577\n",
      "Epoch: 080, Loss: 1.0685, Train: 1.0524, Val: 1.0556\n",
      "Epoch: 081, Loss: 1.0665, Train: 1.0517, Val: 1.0548\n",
      "Epoch: 082, Loss: 1.0649, Train: 1.0516, Val: 1.0545\n",
      "Epoch: 083, Loss: 1.0641, Train: 1.0416, Val: 1.0450\n",
      "Epoch: 084, Loss: 1.0670, Train: 1.0557, Val: 1.0592\n",
      "Epoch: 085, Loss: 1.0622, Train: 1.0524, Val: 1.0552\n",
      "Epoch: 086, Loss: 1.0650, Train: 1.0337, Val: 1.0366\n",
      "Epoch: 087, Loss: 1.0588, Train: 1.0356, Val: 1.0388\n",
      "Epoch: 088, Loss: 1.0585, Train: 1.0363, Val: 1.0394\n",
      "Epoch: 089, Loss: 1.0534, Train: 1.0373, Val: 1.0404\n",
      "Epoch: 090, Loss: 1.0543, Train: 1.0386, Val: 1.0422\n",
      "Epoch: 091, Loss: 1.0504, Train: 1.0265, Val: 1.0298\n",
      "Epoch: 092, Loss: 1.0501, Train: 1.0282, Val: 1.0316\n",
      "Epoch: 093, Loss: 1.0458, Train: 1.0417, Val: 1.0454\n",
      "Epoch: 094, Loss: 1.0461, Train: 1.0233, Val: 1.0271\n",
      "Epoch: 095, Loss: 1.0441, Train: 1.0234, Val: 1.0273\n",
      "Epoch: 096, Loss: 1.0418, Train: 1.0286, Val: 1.0320\n",
      "Epoch: 097, Loss: 1.0386, Train: 1.0261, Val: 1.0297\n",
      "Epoch: 098, Loss: 1.0371, Train: 1.0213, Val: 1.0254\n",
      "Epoch: 099, Loss: 1.0351, Train: 1.0228, Val: 1.0266\n",
      "Epoch: 100, Loss: 1.0330, Train: 1.0235, Val: 1.0275\n",
      "Epoch: 101, Loss: 1.0300, Train: 1.0171, Val: 1.0211\n",
      "Epoch: 102, Loss: 1.0280, Train: 1.0179, Val: 1.0220\n",
      "Epoch: 103, Loss: 1.0251, Train: 1.0209, Val: 1.0250\n",
      "Epoch: 104, Loss: 1.0237, Train: 1.0172, Val: 1.0215\n",
      "Epoch: 105, Loss: 1.0211, Train: 1.0152, Val: 1.0192\n",
      "Epoch: 106, Loss: 1.0200, Train: 1.0204, Val: 1.0246\n",
      "Epoch: 107, Loss: 1.0201, Train: 1.0925, Val: 1.0956\n",
      "Epoch: 108, Loss: 1.0280, Train: 1.0256, Val: 1.0305\n",
      "Epoch: 109, Loss: 1.0351, Train: 1.0517, Val: 1.0551\n",
      "Epoch: 110, Loss: 1.0388, Train: 1.0645, Val: 1.0680\n",
      "Epoch: 111, Loss: 1.0212, Train: 1.0676, Val: 1.0710\n",
      "Epoch: 112, Loss: 1.0185, Train: 1.0331, Val: 1.0369\n",
      "Epoch: 113, Loss: 1.0194, Train: 1.0197, Val: 1.0248\n",
      "Epoch: 114, Loss: 1.0157, Train: 1.1052, Val: 1.1085\n",
      "Epoch: 115, Loss: 1.0155, Train: 1.0948, Val: 1.0984\n",
      "Epoch: 116, Loss: 1.0147, Train: 1.0270, Val: 1.0313\n",
      "Epoch: 117, Loss: 1.0059, Train: 1.0288, Val: 1.0326\n",
      "Epoch: 118, Loss: 1.0118, Train: 1.0169, Val: 1.0212\n",
      "Epoch: 119, Loss: 1.0020, Train: 1.0273, Val: 1.0315\n",
      "Epoch: 120, Loss: 1.0068, Train: 1.0387, Val: 1.0425\n",
      "Epoch: 121, Loss: 1.0024, Train: 1.0076, Val: 1.0125\n",
      "Epoch: 122, Loss: 1.0034, Train: 1.0165, Val: 1.0208\n",
      "Epoch: 123, Loss: 0.9977, Train: 1.0204, Val: 1.0248\n",
      "Epoch: 124, Loss: 0.9974, Train: 1.0286, Val: 1.0327\n",
      "Epoch: 125, Loss: 0.9945, Train: 1.0067, Val: 1.0112\n",
      "Epoch: 126, Loss: 0.9930, Train: 1.0035, Val: 1.0086\n",
      "Epoch: 127, Loss: 0.9922, Train: 1.0228, Val: 1.0271\n",
      "Epoch: 128, Loss: 0.9920, Train: 0.9997, Val: 1.0048\n",
      "Epoch: 129, Loss: 0.9888, Train: 1.0027, Val: 1.0076\n",
      "Epoch: 130, Loss: 0.9885, Train: 1.0138, Val: 1.0183\n",
      "Epoch: 131, Loss: 0.9864, Train: 0.9974, Val: 1.0027\n",
      "Epoch: 132, Loss: 0.9860, Train: 1.0033, Val: 1.0081\n",
      "Epoch: 133, Loss: 0.9844, Train: 0.9968, Val: 1.0015\n",
      "Epoch: 134, Loss: 0.9809, Train: 0.9935, Val: 0.9985\n",
      "Epoch: 135, Loss: 0.9803, Train: 0.9985, Val: 1.0037\n",
      "Epoch: 136, Loss: 0.9800, Train: 0.9993, Val: 1.0045\n",
      "Epoch: 137, Loss: 0.9781, Train: 1.0181, Val: 1.0224\n",
      "Epoch: 138, Loss: 0.9809, Train: 1.0087, Val: 1.0130\n",
      "Epoch: 139, Loss: 0.9811, Train: 1.0749, Val: 1.0782\n",
      "Epoch: 140, Loss: 0.9854, Train: 0.9899, Val: 0.9950\n",
      "Epoch: 141, Loss: 0.9746, Train: 1.0077, Val: 1.0121\n",
      "Epoch: 142, Loss: 0.9773, Train: 1.1164, Val: 1.1192\n",
      "Epoch: 143, Loss: 0.9890, Train: 1.0031, Val: 1.0082\n",
      "Epoch: 144, Loss: 0.9726, Train: 1.0433, Val: 1.0476\n",
      "Epoch: 145, Loss: 0.9937, Train: 1.2331, Val: 1.2343\n",
      "Epoch: 146, Loss: 1.0056, Train: 1.2905, Val: 1.2913\n",
      "Epoch: 147, Loss: 0.9947, Train: 1.0843, Val: 1.0879\n",
      "Epoch: 148, Loss: 0.9851, Train: 1.0608, Val: 1.0636\n",
      "Epoch: 149, Loss: 0.9917, Train: 1.0032, Val: 1.0080\n",
      "Epoch: 150, Loss: 0.9817, Train: 1.0758, Val: 1.0794\n",
      "Epoch: 151, Loss: 0.9800, Train: 1.1663, Val: 1.1687\n",
      "Epoch: 152, Loss: 0.9822, Train: 1.0542, Val: 1.0578\n",
      "Epoch: 153, Loss: 0.9757, Train: 1.0021, Val: 1.0075\n",
      "Epoch: 154, Loss: 0.9724, Train: 1.0173, Val: 1.0224\n",
      "Epoch: 155, Loss: 0.9780, Train: 1.0323, Val: 1.0352\n",
      "Epoch: 156, Loss: 0.9673, Train: 1.0659, Val: 1.0686\n",
      "Epoch: 157, Loss: 0.9730, Train: 1.0332, Val: 1.0371\n",
      "Epoch: 158, Loss: 0.9688, Train: 0.9897, Val: 0.9950\n",
      "Epoch: 159, Loss: 0.9639, Train: 1.0182, Val: 1.0224\n",
      "Epoch: 160, Loss: 0.9674, Train: 1.0072, Val: 1.0112\n",
      "Epoch: 161, Loss: 0.9617, Train: 0.9981, Val: 1.0030\n",
      "Epoch: 162, Loss: 0.9632, Train: 0.9971, Val: 1.0021\n",
      "Epoch: 163, Loss: 0.9606, Train: 0.9893, Val: 0.9947\n",
      "Epoch: 164, Loss: 0.9584, Train: 1.0008, Val: 1.0063\n",
      "Epoch: 165, Loss: 0.9599, Train: 0.9883, Val: 0.9936\n",
      "Epoch: 166, Loss: 0.9562, Train: 0.9885, Val: 0.9937\n",
      "Epoch: 167, Loss: 0.9569, Train: 0.9859, Val: 0.9914\n",
      "Epoch: 168, Loss: 0.9549, Train: 0.9820, Val: 0.9882\n",
      "Epoch: 169, Loss: 0.9535, Train: 0.9856, Val: 0.9920\n",
      "Epoch: 170, Loss: 0.9532, Train: 0.9787, Val: 0.9848\n",
      "Epoch: 171, Loss: 0.9513, Train: 0.9776, Val: 0.9839\n",
      "Epoch: 172, Loss: 0.9515, Train: 0.9795, Val: 0.9856\n",
      "Epoch: 173, Loss: 0.9499, Train: 0.9781, Val: 0.9851\n",
      "Epoch: 174, Loss: 0.9489, Train: 0.9761, Val: 0.9832\n",
      "Epoch: 175, Loss: 0.9480, Train: 0.9756, Val: 0.9826\n",
      "Epoch: 176, Loss: 0.9470, Train: 0.9789, Val: 0.9854\n",
      "Epoch: 177, Loss: 0.9460, Train: 0.9751, Val: 0.9819\n",
      "Epoch: 178, Loss: 0.9454, Train: 0.9771, Val: 0.9834\n",
      "Epoch: 179, Loss: 0.9449, Train: 0.9737, Val: 0.9808\n",
      "Epoch: 180, Loss: 0.9442, Train: 0.9790, Val: 0.9854\n",
      "Epoch: 181, Loss: 0.9442, Train: 0.9726, Val: 0.9802\n",
      "Epoch: 182, Loss: 0.9452, Train: 0.9878, Val: 0.9937\n",
      "Epoch: 183, Loss: 0.9507, Train: 0.9935, Val: 1.0016\n",
      "Epoch: 184, Loss: 0.9702, Train: 1.0749, Val: 1.0779\n",
      "Epoch: 185, Loss: 1.0012, Train: 0.9991, Val: 1.0067\n",
      "Epoch: 186, Loss: 1.0120, Train: 1.0339, Val: 1.0396\n",
      "Epoch: 187, Loss: 0.9596, Train: 0.9995, Val: 1.0053\n",
      "Epoch: 188, Loss: 0.9639, Train: 0.9886, Val: 0.9956\n",
      "Epoch: 189, Loss: 0.9839, Train: 1.0321, Val: 1.0380\n",
      "Epoch: 190, Loss: 0.9610, Train: 1.0235, Val: 1.0300\n",
      "Epoch: 191, Loss: 0.9549, Train: 1.0373, Val: 1.0450\n",
      "Epoch: 192, Loss: 0.9656, Train: 0.9980, Val: 1.0051\n",
      "Epoch: 193, Loss: 0.9549, Train: 1.0215, Val: 1.0278\n",
      "Epoch: 194, Loss: 0.9552, Train: 1.0078, Val: 1.0156\n",
      "Epoch: 195, Loss: 0.9539, Train: 1.0323, Val: 1.0400\n",
      "Epoch: 196, Loss: 0.9544, Train: 0.9844, Val: 0.9914\n",
      "Epoch: 197, Loss: 0.9447, Train: 1.0395, Val: 1.0451\n",
      "Epoch: 198, Loss: 0.9493, Train: 0.9909, Val: 0.9983\n",
      "Epoch: 199, Loss: 0.9438, Train: 1.0050, Val: 1.0129\n",
      "Epoch: 200, Loss: 0.9442, Train: 0.9766, Val: 0.9842\n",
      "Epoch: 201, Loss: 0.9431, Train: 1.0066, Val: 1.0132\n",
      "Epoch: 202, Loss: 0.9411, Train: 0.9830, Val: 0.9905\n",
      "Epoch: 203, Loss: 0.9400, Train: 0.9856, Val: 0.9935\n",
      "Epoch: 204, Loss: 0.9399, Train: 0.9708, Val: 0.9784\n",
      "Epoch: 205, Loss: 0.9370, Train: 0.9914, Val: 0.9986\n",
      "Epoch: 206, Loss: 0.9370, Train: 0.9864, Val: 0.9938\n",
      "Epoch: 207, Loss: 0.9367, Train: 0.9684, Val: 0.9759\n",
      "Epoch: 208, Loss: 0.9343, Train: 0.9714, Val: 0.9788\n",
      "Epoch: 209, Loss: 0.9345, Train: 0.9744, Val: 0.9820\n",
      "Epoch: 210, Loss: 0.9327, Train: 0.9775, Val: 0.9850\n",
      "Epoch: 211, Loss: 0.9326, Train: 0.9689, Val: 0.9763\n",
      "Epoch: 212, Loss: 0.9316, Train: 0.9658, Val: 0.9735\n",
      "Epoch: 213, Loss: 0.9308, Train: 0.9668, Val: 0.9744\n",
      "Epoch: 214, Loss: 0.9299, Train: 0.9751, Val: 0.9822\n",
      "Epoch: 215, Loss: 0.9294, Train: 0.9726, Val: 0.9801\n",
      "Epoch: 216, Loss: 0.9282, Train: 0.9649, Val: 0.9729\n",
      "Epoch: 217, Loss: 0.9279, Train: 0.9708, Val: 0.9780\n",
      "Epoch: 218, Loss: 0.9270, Train: 0.9766, Val: 0.9837\n",
      "Epoch: 219, Loss: 0.9263, Train: 0.9649, Val: 0.9732\n",
      "Epoch: 220, Loss: 0.9259, Train: 0.9645, Val: 0.9726\n",
      "Epoch: 221, Loss: 0.9249, Train: 0.9664, Val: 0.9743\n",
      "Epoch: 222, Loss: 0.9246, Train: 0.9634, Val: 0.9716\n",
      "Epoch: 223, Loss: 0.9234, Train: 0.9655, Val: 0.9735\n",
      "Epoch: 224, Loss: 0.9234, Train: 0.9652, Val: 0.9733\n",
      "Epoch: 225, Loss: 0.9228, Train: 0.9653, Val: 0.9732\n",
      "Epoch: 226, Loss: 0.9226, Train: 0.9648, Val: 0.9731\n",
      "Epoch: 227, Loss: 0.9223, Train: 0.9673, Val: 0.9755\n",
      "Epoch: 228, Loss: 0.9223, Train: 0.9728, Val: 0.9809\n",
      "Epoch: 229, Loss: 0.9220, Train: 0.9675, Val: 0.9757\n",
      "Epoch: 230, Loss: 0.9212, Train: 0.9707, Val: 0.9786\n",
      "Epoch: 231, Loss: 0.9204, Train: 0.9641, Val: 0.9722\n",
      "Epoch: 232, Loss: 0.9198, Train: 0.9852, Val: 0.9922\n",
      "Epoch: 233, Loss: 0.9198, Train: 0.9623, Val: 0.9705\n",
      "Epoch: 234, Loss: 0.9203, Train: 0.9946, Val: 1.0010\n",
      "Epoch: 235, Loss: 0.9229, Train: 0.9643, Val: 0.9724\n",
      "Epoch: 236, Loss: 0.9251, Train: 0.9951, Val: 1.0015\n",
      "Epoch: 237, Loss: 0.9223, Train: 0.9674, Val: 0.9750\n",
      "Epoch: 238, Loss: 0.9187, Train: 1.0142, Val: 1.0209\n",
      "Epoch: 239, Loss: 0.9215, Train: 1.0032, Val: 1.0089\n",
      "Epoch: 240, Loss: 0.9261, Train: 1.0009, Val: 1.0081\n",
      "Epoch: 241, Loss: 0.9236, Train: 0.9906, Val: 0.9966\n",
      "Epoch: 242, Loss: 0.9173, Train: 0.9627, Val: 0.9706\n",
      "Epoch: 243, Loss: 0.9181, Train: 1.0465, Val: 1.0522\n",
      "Epoch: 244, Loss: 0.9232, Train: 0.9726, Val: 0.9797\n",
      "Epoch: 245, Loss: 0.9161, Train: 0.9734, Val: 0.9801\n",
      "Epoch: 246, Loss: 0.9149, Train: 0.9991, Val: 1.0059\n",
      "Epoch: 247, Loss: 0.9183, Train: 0.9677, Val: 0.9751\n",
      "Epoch: 248, Loss: 0.9149, Train: 0.9927, Val: 0.9990\n",
      "Epoch: 249, Loss: 0.9135, Train: 0.9705, Val: 0.9782\n",
      "Epoch: 250, Loss: 0.9162, Train: 0.9809, Val: 0.9876\n",
      "Epoch: 251, Loss: 0.9143, Train: 0.9619, Val: 0.9699\n",
      "Epoch: 252, Loss: 0.9112, Train: 0.9839, Val: 0.9911\n",
      "Epoch: 253, Loss: 0.9121, Train: 0.9630, Val: 0.9705\n",
      "Epoch: 254, Loss: 0.9126, Train: 1.0174, Val: 1.0237\n",
      "Epoch: 255, Loss: 0.9123, Train: 0.9633, Val: 0.9714\n",
      "Epoch: 256, Loss: 0.9106, Train: 0.9935, Val: 1.0010\n",
      "Epoch: 257, Loss: 0.9128, Train: 0.9886, Val: 0.9950\n",
      "Epoch: 258, Loss: 0.9204, Train: 1.0182, Val: 1.0254\n",
      "Epoch: 259, Loss: 0.9388, Train: 1.0912, Val: 1.0942\n",
      "Epoch: 260, Loss: 0.9573, Train: 0.9749, Val: 0.9839\n",
      "Epoch: 261, Loss: 0.9318, Train: 1.0956, Val: 1.1001\n",
      "Epoch: 262, Loss: 0.9234, Train: 1.0617, Val: 1.0651\n",
      "Epoch: 263, Loss: 0.9297, Train: 0.9711, Val: 0.9782\n",
      "Epoch: 264, Loss: 0.9245, Train: 1.0576, Val: 1.0635\n",
      "Epoch: 265, Loss: 0.9363, Train: 1.0005, Val: 1.0070\n",
      "Epoch: 266, Loss: 0.9169, Train: 0.9908, Val: 0.9980\n",
      "Epoch: 267, Loss: 0.9278, Train: 0.9683, Val: 0.9757\n",
      "Epoch: 268, Loss: 0.9179, Train: 0.9857, Val: 0.9938\n",
      "Epoch: 269, Loss: 0.9186, Train: 0.9782, Val: 0.9875\n",
      "Epoch: 270, Loss: 0.9168, Train: 0.9765, Val: 0.9831\n",
      "Epoch: 271, Loss: 0.9142, Train: 0.9803, Val: 0.9867\n",
      "Epoch: 272, Loss: 0.9169, Train: 0.9836, Val: 0.9931\n",
      "Epoch: 273, Loss: 0.9111, Train: 0.9762, Val: 0.9850\n",
      "Epoch: 274, Loss: 0.9123, Train: 0.9671, Val: 0.9741\n",
      "Epoch: 275, Loss: 0.9086, Train: 0.9691, Val: 0.9765\n",
      "Epoch: 276, Loss: 0.9110, Train: 0.9554, Val: 0.9641\n",
      "Epoch: 277, Loss: 0.9077, Train: 0.9702, Val: 0.9796\n",
      "Epoch: 278, Loss: 0.9102, Train: 0.9628, Val: 0.9716\n",
      "Epoch: 279, Loss: 0.9081, Train: 0.9577, Val: 0.9658\n",
      "Epoch: 280, Loss: 0.9084, Train: 0.9607, Val: 0.9691\n",
      "Epoch: 281, Loss: 0.9026, Train: 0.9658, Val: 0.9740\n",
      "Epoch: 282, Loss: 0.9032, Train: 0.9644, Val: 0.9728\n",
      "Epoch: 283, Loss: 0.9029, Train: 0.9610, Val: 0.9692\n",
      "Epoch: 284, Loss: 0.9047, Train: 0.9585, Val: 0.9667\n",
      "Epoch: 285, Loss: 0.9013, Train: 0.9840, Val: 0.9922\n",
      "Epoch: 286, Loss: 0.9019, Train: 0.9553, Val: 0.9643\n",
      "Epoch: 287, Loss: 0.9013, Train: 0.9640, Val: 0.9717\n",
      "Epoch: 288, Loss: 0.9034, Train: 0.9910, Val: 0.9992\n",
      "Epoch: 289, Loss: 0.9052, Train: 0.9595, Val: 0.9677\n",
      "Epoch: 290, Loss: 0.9058, Train: 0.9567, Val: 0.9655\n",
      "Epoch: 291, Loss: 0.9043, Train: 0.9755, Val: 0.9835\n",
      "Epoch: 292, Loss: 0.9051, Train: 0.9637, Val: 0.9723\n",
      "Epoch: 293, Loss: 0.9023, Train: 1.0130, Val: 1.0196\n",
      "Epoch: 294, Loss: 0.9011, Train: 0.9780, Val: 0.9852\n",
      "Epoch: 295, Loss: 0.8954, Train: 0.9667, Val: 0.9750\n",
      "Epoch: 296, Loss: 0.8991, Train: 1.0052, Val: 1.0122\n",
      "Epoch: 297, Loss: 0.9012, Train: 0.9859, Val: 0.9922\n",
      "Epoch: 298, Loss: 0.8969, Train: 0.9589, Val: 0.9667\n",
      "Epoch: 299, Loss: 0.8969, Train: 0.9834, Val: 0.9912\n",
      "Epoch: 300, Loss: 0.8976, Train: 0.9843, Val: 0.9905\n",
      "Epoch: 301, Loss: 0.8975, Train: 0.9673, Val: 0.9757\n",
      "Epoch: 302, Loss: 0.8992, Train: 1.0205, Val: 1.0274\n",
      "Epoch: 303, Loss: 0.9039, Train: 0.9843, Val: 0.9907\n",
      "Epoch: 304, Loss: 0.8981, Train: 0.9794, Val: 0.9867\n",
      "Epoch: 305, Loss: 0.8982, Train: 1.0074, Val: 1.0148\n",
      "Epoch: 306, Loss: 0.9007, Train: 0.9968, Val: 1.0031\n",
      "Epoch: 307, Loss: 0.8955, Train: 0.9888, Val: 0.9957\n",
      "Epoch: 308, Loss: 0.8957, Train: 0.9837, Val: 0.9922\n",
      "Epoch: 309, Loss: 0.8987, Train: 1.0035, Val: 1.0100\n",
      "Epoch: 310, Loss: 0.9007, Train: 1.0042, Val: 1.0102\n",
      "Epoch: 311, Loss: 0.9068, Train: 0.9576, Val: 0.9658\n",
      "Epoch: 312, Loss: 0.9046, Train: 0.9906, Val: 0.9988\n",
      "Epoch: 313, Loss: 0.9068, Train: 0.9844, Val: 0.9907\n",
      "Epoch: 314, Loss: 0.8954, Train: 0.9697, Val: 0.9770\n",
      "Epoch: 315, Loss: 0.9071, Train: 0.9733, Val: 0.9820\n",
      "Epoch: 316, Loss: 0.9177, Train: 1.0270, Val: 1.0334\n",
      "Epoch: 317, Loss: 0.9070, Train: 0.9999, Val: 1.0052\n",
      "Epoch: 318, Loss: 0.9058, Train: 0.9907, Val: 0.9995\n",
      "Epoch: 319, Loss: 0.9027, Train: 0.9996, Val: 1.0063\n",
      "Epoch: 320, Loss: 0.9024, Train: 1.0666, Val: 1.0709\n",
      "Epoch: 321, Loss: 0.9040, Train: 0.9724, Val: 0.9792\n",
      "Epoch: 322, Loss: 0.8935, Train: 0.9585, Val: 0.9670\n",
      "Epoch: 323, Loss: 0.8999, Train: 0.9712, Val: 0.9783\n",
      "Epoch: 324, Loss: 0.8908, Train: 1.0219, Val: 1.0276\n",
      "Epoch: 325, Loss: 0.8962, Train: 0.9740, Val: 0.9806\n",
      "Epoch: 326, Loss: 0.8887, Train: 0.9510, Val: 0.9593\n",
      "Epoch: 327, Loss: 0.8925, Train: 0.9464, Val: 0.9555\n",
      "Epoch: 328, Loss: 0.8874, Train: 0.9538, Val: 0.9621\n",
      "Epoch: 329, Loss: 0.8898, Train: 0.9732, Val: 0.9810\n",
      "Epoch: 330, Loss: 0.8859, Train: 0.9503, Val: 0.9583\n",
      "Epoch: 331, Loss: 0.8874, Train: 0.9536, Val: 0.9621\n",
      "Epoch: 332, Loss: 0.8836, Train: 0.9495, Val: 0.9581\n",
      "Epoch: 333, Loss: 0.8856, Train: 0.9547, Val: 0.9629\n",
      "Epoch: 334, Loss: 0.8822, Train: 0.9453, Val: 0.9539\n",
      "Epoch: 335, Loss: 0.8838, Train: 0.9434, Val: 0.9524\n",
      "Epoch: 336, Loss: 0.8809, Train: 0.9510, Val: 0.9593\n",
      "Epoch: 337, Loss: 0.8826, Train: 0.9436, Val: 0.9524\n",
      "Epoch: 338, Loss: 0.8800, Train: 0.9499, Val: 0.9584\n",
      "Epoch: 339, Loss: 0.8820, Train: 0.9477, Val: 0.9572\n",
      "Epoch: 340, Loss: 0.8813, Train: 0.9517, Val: 0.9600\n",
      "Epoch: 341, Loss: 0.8854, Train: 0.9665, Val: 0.9761\n",
      "Epoch: 342, Loss: 0.8912, Train: 0.9908, Val: 0.9974\n",
      "Epoch: 343, Loss: 0.8999, Train: 0.9669, Val: 0.9762\n",
      "Epoch: 344, Loss: 0.8941, Train: 0.9507, Val: 0.9597\n",
      "Epoch: 345, Loss: 0.8887, Train: 0.9725, Val: 0.9809\n",
      "Epoch: 346, Loss: 0.8916, Train: 0.9804, Val: 0.9887\n",
      "Epoch: 347, Loss: 0.8910, Train: 0.9498, Val: 0.9577\n",
      "Epoch: 348, Loss: 0.8785, Train: 0.9877, Val: 0.9958\n",
      "Epoch: 349, Loss: 0.8894, Train: 0.9662, Val: 0.9749\n",
      "Epoch: 350, Loss: 0.8857, Train: 0.9637, Val: 0.9716\n",
      "Epoch: 351, Loss: 0.8864, Train: 0.9714, Val: 0.9795\n",
      "Epoch: 352, Loss: 0.8832, Train: 0.9734, Val: 0.9823\n",
      "Epoch: 353, Loss: 0.8846, Train: 0.9675, Val: 0.9751\n",
      "Epoch: 354, Loss: 0.8846, Train: 0.9518, Val: 0.9601\n",
      "Epoch: 376, Loss: 0.8806, Train: 0.9560, Val: 0.9651\n",
      "Epoch: 377, Loss: 0.8882, Train: 0.9631, Val: 0.9724\n",
      "Epoch: 378, Loss: 0.8881, Train: 0.9482, Val: 0.9569\n",
      "Epoch: 379, Loss: 0.8828, Train: 0.9381, Val: 0.9474\n",
      "Epoch: 380, Loss: 0.8804, Train: 0.9703, Val: 0.9783\n",
      "Epoch: 381, Loss: 0.8759, Train: 0.9412, Val: 0.9504\n",
      "Epoch: 382, Loss: 0.8699, Train: 0.9388, Val: 0.9478\n",
      "Epoch: 383, Loss: 0.8733, Train: 0.9482, Val: 0.9571\n",
      "Epoch: 384, Loss: 0.8751, Train: 0.9476, Val: 0.9573\n",
      "Epoch: 385, Loss: 0.8741, Train: 0.9471, Val: 0.9557\n",
      "Epoch: 386, Loss: 0.8746, Train: 0.9514, Val: 0.9611\n",
      "Epoch: 387, Loss: 0.8699, Train: 0.9406, Val: 0.9510\n",
      "Epoch: 388, Loss: 0.8672, Train: 0.9398, Val: 0.9488\n",
      "Epoch: 389, Loss: 0.8694, Train: 0.9450, Val: 0.9555\n",
      "Epoch: 390, Loss: 0.8691, Train: 0.9445, Val: 0.9553\n",
      "Epoch: 391, Loss: 0.8678, Train: 0.9389, Val: 0.9487\n",
      "Epoch: 392, Loss: 0.8699, Train: 0.9395, Val: 0.9507\n",
      "Epoch: 393, Loss: 0.8692, Train: 0.9374, Val: 0.9482\n",
      "Epoch: 394, Loss: 0.8662, Train: 0.9352, Val: 0.9452\n",
      "Epoch: 395, Loss: 0.8663, Train: 0.9381, Val: 0.9494\n",
      "Epoch: 396, Loss: 0.8682, Train: 0.9342, Val: 0.9446\n",
      "Epoch: 397, Loss: 0.8666, Train: 0.9392, Val: 0.9500\n",
      "Epoch: 398, Loss: 0.8661, Train: 0.9456, Val: 0.9565\n",
      "Epoch: 399, Loss: 0.8672, Train: 0.9349, Val: 0.9450\n",
      "Epoch: 400, Loss: 0.8663, Train: 0.9361, Val: 0.9469\n",
      "Epoch: 401, Loss: 0.8627, Train: 0.9300, Val: 0.9408\n",
      "Epoch: 402, Loss: 0.8623, Train: 0.9324, Val: 0.9427\n",
      "Epoch: 403, Loss: 0.8630, Train: 0.9382, Val: 0.9494\n",
      "Epoch: 404, Loss: 0.8625, Train: 0.9303, Val: 0.9412\n",
      "Epoch: 405, Loss: 0.8621, Train: 0.9318, Val: 0.9422\n",
      "Epoch: 406, Loss: 0.8627, Train: 0.9393, Val: 0.9510\n",
      "Epoch: 407, Loss: 0.8646, Train: 0.9333, Val: 0.9437\n",
      "Epoch: 408, Loss: 0.8660, Train: 0.9404, Val: 0.9512\n",
      "Epoch: 409, Loss: 0.8696, Train: 0.9526, Val: 0.9635\n",
      "Epoch: 410, Loss: 0.8789, Train: 1.0236, Val: 1.0303\n",
      "Epoch: 411, Loss: 0.9141, Train: 1.0158, Val: 1.0228\n",
      "Epoch: 412, Loss: 0.9538, Train: 0.9987, Val: 1.0095\n",
      "Epoch: 413, Loss: 0.9410, Train: 1.1410, Val: 1.1435\n",
      "Epoch: 414, Loss: 0.9110, Train: 1.0863, Val: 1.0905\n",
      "Epoch: 415, Loss: 0.9079, Train: 0.9943, Val: 1.0020\n",
      "Epoch: 416, Loss: 0.9101, Train: 1.0929, Val: 1.0974\n",
      "Epoch: 417, Loss: 0.8979, Train: 1.0424, Val: 1.0491\n",
      "Epoch: 418, Loss: 0.9097, Train: 1.0015, Val: 1.0083\n",
      "Epoch: 419, Loss: 0.8842, Train: 0.9603, Val: 0.9684\n",
      "Epoch: 420, Loss: 0.8939, Train: 0.9549, Val: 0.9632\n",
      "Epoch: 421, Loss: 0.8834, Train: 0.9632, Val: 0.9713\n",
      "Epoch: 422, Loss: 0.8875, Train: 0.9702, Val: 0.9798\n",
      "Epoch: 423, Loss: 0.8781, Train: 0.9526, Val: 0.9622\n",
      "Epoch: 424, Loss: 0.8791, Train: 0.9648, Val: 0.9735\n",
      "Epoch: 425, Loss: 0.8771, Train: 0.9554, Val: 0.9652\n",
      "Epoch: 426, Loss: 0.8757, Train: 0.9396, Val: 0.9502\n",
      "Epoch: 427, Loss: 0.8736, Train: 0.9432, Val: 0.9534\n",
      "Epoch: 428, Loss: 0.8707, Train: 0.9428, Val: 0.9525\n",
      "Epoch: 429, Loss: 0.8698, Train: 0.9410, Val: 0.9503\n",
      "Epoch: 430, Loss: 0.8680, Train: 0.9354, Val: 0.9452\n",
      "Epoch: 431, Loss: 0.8687, Train: 0.9358, Val: 0.9452\n",
      "Epoch: 432, Loss: 0.8651, Train: 0.9352, Val: 0.9449\n",
      "Epoch: 433, Loss: 0.8662, Train: 0.9361, Val: 0.9464\n",
      "Epoch: 434, Loss: 0.8637, Train: 0.9330, Val: 0.9431\n",
      "Epoch: 435, Loss: 0.8643, Train: 0.9320, Val: 0.9419\n",
      "Epoch: 436, Loss: 0.8625, Train: 0.9337, Val: 0.9434\n",
      "Epoch: 437, Loss: 0.8620, Train: 0.9297, Val: 0.9402\n",
      "Epoch: 438, Loss: 0.8611, Train: 0.9290, Val: 0.9399\n",
      "Epoch: 439, Loss: 0.8602, Train: 0.9315, Val: 0.9417\n",
      "Epoch: 440, Loss: 0.8600, Train: 0.9297, Val: 0.9401\n",
      "Epoch: 441, Loss: 0.8594, Train: 0.9299, Val: 0.9403\n",
      "Epoch: 442, Loss: 0.8585, Train: 0.9295, Val: 0.9400\n",
      "Epoch: 443, Loss: 0.8582, Train: 0.9279, Val: 0.9383\n",
      "Epoch: 444, Loss: 0.8572, Train: 0.9280, Val: 0.9385\n",
      "Epoch: 445, Loss: 0.8574, Train: 0.9292, Val: 0.9402\n",
      "Epoch: 446, Loss: 0.8568, Train: 0.9279, Val: 0.9384\n",
      "Epoch: 447, Loss: 0.8565, Train: 0.9274, Val: 0.9377\n",
      "Epoch: 448, Loss: 0.8561, Train: 0.9291, Val: 0.9394\n",
      "Epoch: 449, Loss: 0.8562, Train: 0.9257, Val: 0.9365\n",
      "Epoch: 450, Loss: 0.8559, Train: 0.9279, Val: 0.9384\n",
      "Epoch: 451, Loss: 0.8558, Train: 0.9267, Val: 0.9372\n",
      "Epoch: 452, Loss: 0.8558, Train: 0.9277, Val: 0.9378\n",
      "Epoch: 453, Loss: 0.8558, Train: 0.9280, Val: 0.9388\n",
      "Epoch: 454, Loss: 0.8557, Train: 0.9284, Val: 0.9385\n",
      "Epoch: 455, Loss: 0.8555, Train: 0.9298, Val: 0.9401\n",
      "Epoch: 456, Loss: 0.8554, Train: 0.9295, Val: 0.9397\n",
      "Epoch: 457, Loss: 0.8555, Train: 0.9273, Val: 0.9379\n",
      "Epoch: 458, Loss: 0.8562, Train: 0.9288, Val: 0.9391\n",
      "Epoch: 459, Loss: 0.8581, Train: 0.9344, Val: 0.9445\n",
      "Epoch: 460, Loss: 0.8624, Train: 0.9345, Val: 0.9443\n",
      "Epoch: 461, Loss: 0.8698, Train: 0.9410, Val: 0.9504\n",
      "Epoch: 462, Loss: 0.8794, Train: 0.9404, Val: 0.9503\n",
      "Epoch: 463, Loss: 0.8778, Train: 0.9397, Val: 0.9495\n",
      "Epoch: 464, Loss: 0.8672, Train: 0.9475, Val: 0.9565\n",
      "Epoch: 465, Loss: 0.8555, Train: 0.9341, Val: 0.9440\n",
      "Epoch: 466, Loss: 0.8579, Train: 0.9340, Val: 0.9439\n",
      "Epoch: 467, Loss: 0.8659, Train: 0.9461, Val: 0.9555\n",
      "Epoch: 468, Loss: 0.8628, Train: 0.9338, Val: 0.9440\n",
      "Epoch: 469, Loss: 0.8540, Train: 0.9336, Val: 0.9429\n",
      "Epoch: 470, Loss: 0.8547, Train: 0.9475, Val: 0.9568\n",
      "Epoch: 471, Loss: 0.8596, Train: 0.9332, Val: 0.9431\n",
      "Epoch: 472, Loss: 0.8548, Train: 0.9286, Val: 0.9383\n",
      "Epoch: 473, Loss: 0.8508, Train: 0.9317, Val: 0.9421\n",
      "Epoch: 474, Loss: 0.8540, Train: 0.9330, Val: 0.9434\n",
      "Epoch: 475, Loss: 0.8552, Train: 0.9249, Val: 0.9356\n",
      "Epoch: 476, Loss: 0.8510, Train: 0.9237, Val: 0.9339\n",
      "Epoch: 477, Loss: 0.8497, Train: 0.9284, Val: 0.9385\n",
      "Epoch: 478, Loss: 0.8524, Train: 0.9249, Val: 0.9355\n",
      "Epoch: 479, Loss: 0.8520, Train: 0.9248, Val: 0.9355\n",
      "Epoch: 480, Loss: 0.8490, Train: 0.9281, Val: 0.9387\n",
      "Epoch: 481, Loss: 0.8494, Train: 0.9329, Val: 0.9430\n",
      "Epoch: 482, Loss: 0.8515, Train: 0.9346, Val: 0.9448\n",
      "Epoch: 483, Loss: 0.8505, Train: 0.9235, Val: 0.9344\n",
      "Epoch: 484, Loss: 0.8487, Train: 0.9245, Val: 0.9349\n",
      "Epoch: 485, Loss: 0.8498, Train: 0.9246, Val: 0.9355\n",
      "Epoch: 486, Loss: 0.8527, Train: 0.9341, Val: 0.9446\n",
      "Epoch: 487, Loss: 0.8581, Train: 0.9536, Val: 0.9637\n",
      "Epoch: 488, Loss: 0.8710, Train: 0.9758, Val: 0.9849\n",
      "Epoch: 489, Loss: 0.8880, Train: 0.9617, Val: 0.9735\n",
      "Epoch: 490, Loss: 0.8905, Train: 0.9593, Val: 0.9686\n",
      "Epoch: 491, Loss: 0.8778, Train: 0.9919, Val: 1.0023\n",
      "Epoch: 492, Loss: 0.8574, Train: 0.9631, Val: 0.9744\n",
      "Epoch: 493, Loss: 0.8719, Train: 0.9464, Val: 0.9554\n",
      "Epoch: 494, Loss: 0.8690, Train: 0.9874, Val: 0.9975\n",
      "Epoch: 495, Loss: 0.8669, Train: 0.9608, Val: 0.9717\n",
      "Epoch: 496, Loss: 0.8611, Train: 0.9389, Val: 0.9482\n",
      "Epoch: 497, Loss: 0.8571, Train: 0.9398, Val: 0.9499\n",
      "Epoch: 498, Loss: 0.8612, Train: 0.9600, Val: 0.9707\n",
      "Epoch: 499, Loss: 0.8561, Train: 0.9397, Val: 0.9501\n",
      "Epoch: 500, Loss: 0.8571, Train: 0.9290, Val: 0.9388\n",
      "Test RMSE: 0.9613\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.653152       3.583673\n",
      "std      1727.484387     741.673176       0.607879       1.116938\n",
      "min         0.000000       0.000000       0.510037       1.000000\n",
      "25%      1500.000000     259.000000       3.277977       3.000000\n",
      "50%      3066.000000     693.000000       3.706572       4.000000\n",
      "75%      4472.000000    1292.000000       4.080982       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1092.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.3036403288039\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  2\n",
      "Kernel:  gd\n",
      "Model Name:  KPGraphSAGE\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.4468, Train: 3.5951, Val: 3.5980\n",
      "Epoch: 002, Loss: 8.5435, Train: 3.3523, Val: 3.3553\n",
      "Epoch: 003, Loss: 2.9983, Train: 3.0211, Val: 3.0241\n",
      "Epoch: 004, Loss: 2.7219, Train: 2.9461, Val: 2.9490\n",
      "Epoch: 005, Loss: 1.3504, Train: 2.8602, Val: 2.8631\n",
      "Epoch: 006, Loss: 1.3068, Train: 2.7894, Val: 2.7923\n",
      "Epoch: 007, Loss: 1.2767, Train: 2.7546, Val: 2.7574\n",
      "Epoch: 008, Loss: 1.2480, Train: 2.7166, Val: 2.7194\n",
      "Epoch: 009, Loss: 1.2527, Train: 2.6540, Val: 2.6568\n",
      "Epoch: 010, Loss: 1.2391, Train: 2.5922, Val: 2.5951\n",
      "Epoch: 011, Loss: 1.2415, Train: 2.5546, Val: 2.5574\n",
      "Epoch: 012, Loss: 1.2326, Train: 2.5065, Val: 2.5093\n",
      "Epoch: 013, Loss: 1.2255, Train: 2.4392, Val: 2.4420\n",
      "Epoch: 014, Loss: 1.2223, Train: 2.3936, Val: 2.3963\n",
      "Epoch: 015, Loss: 1.2131, Train: 2.3556, Val: 2.3584\n",
      "Epoch: 016, Loss: 1.2125, Train: 2.2819, Val: 2.2848\n",
      "Epoch: 017, Loss: 1.2107, Train: 2.2577, Val: 2.2605\n",
      "Epoch: 018, Loss: 1.1983, Train: 2.2382, Val: 2.2410\n",
      "Epoch: 019, Loss: 1.1993, Train: 2.1743, Val: 2.1770\n",
      "Epoch: 020, Loss: 1.1910, Train: 2.1333, Val: 2.1360\n",
      "Epoch: 021, Loss: 1.1869, Train: 2.1305, Val: 2.1332\n",
      "Epoch: 022, Loss: 1.1844, Train: 2.0810, Val: 2.0838\n",
      "Epoch: 023, Loss: 1.1760, Train: 2.0275, Val: 2.0302\n",
      "Epoch: 024, Loss: 1.1753, Train: 2.0256, Val: 2.0282\n",
      "Epoch: 025, Loss: 1.1731, Train: 1.9542, Val: 1.9568\n",
      "Epoch: 026, Loss: 1.1656, Train: 1.9199, Val: 1.9226\n",
      "Epoch: 027, Loss: 1.1621, Train: 1.8985, Val: 1.9012\n",
      "Epoch: 028, Loss: 1.1573, Train: 1.8430, Val: 1.8457\n",
      "Epoch: 029, Loss: 1.1538, Train: 1.8130, Val: 1.8157\n",
      "Epoch: 030, Loss: 1.1499, Train: 1.7911, Val: 1.7939\n",
      "Epoch: 031, Loss: 1.1495, Train: 1.7319, Val: 1.7348\n",
      "Epoch: 032, Loss: 1.1444, Train: 1.7033, Val: 1.7061\n",
      "Epoch: 033, Loss: 1.1423, Train: 1.6734, Val: 1.6762\n",
      "Epoch: 034, Loss: 1.1419, Train: 1.6134, Val: 1.6162\n",
      "Epoch: 035, Loss: 1.1394, Train: 1.5885, Val: 1.5915\n",
      "Epoch: 036, Loss: 1.1360, Train: 1.5608, Val: 1.5638\n",
      "Epoch: 037, Loss: 1.1344, Train: 1.5073, Val: 1.5102\n",
      "Epoch: 038, Loss: 1.1337, Train: 1.4790, Val: 1.4819\n",
      "Epoch: 039, Loss: 1.1304, Train: 1.4441, Val: 1.4470\n",
      "Epoch: 040, Loss: 1.1291, Train: 1.3962, Val: 1.3991\n",
      "Epoch: 041, Loss: 1.1280, Train: 1.3813, Val: 1.3841\n",
      "Epoch: 042, Loss: 1.1265, Train: 1.3535, Val: 1.3563\n",
      "Epoch: 043, Loss: 1.1242, Train: 1.3183, Val: 1.3211\n",
      "Epoch: 044, Loss: 1.1233, Train: 1.3112, Val: 1.3140\n",
      "Epoch: 045, Loss: 1.1220, Train: 1.2736, Val: 1.2762\n",
      "Epoch: 046, Loss: 1.1201, Train: 1.2532, Val: 1.2558\n",
      "Epoch: 047, Loss: 1.1183, Train: 1.2318, Val: 1.2344\n",
      "Epoch: 048, Loss: 1.1173, Train: 1.2017, Val: 1.2042\n",
      "Epoch: 049, Loss: 1.1164, Train: 1.2009, Val: 1.2035\n",
      "Epoch: 050, Loss: 1.1149, Train: 1.1713, Val: 1.1737\n",
      "Epoch: 051, Loss: 1.1137, Train: 1.1731, Val: 1.1756\n",
      "Epoch: 052, Loss: 1.1125, Train: 1.1425, Val: 1.1450\n",
      "Epoch: 053, Loss: 1.1111, Train: 1.1473, Val: 1.1498\n",
      "Epoch: 054, Loss: 1.1099, Train: 1.1195, Val: 1.1219\n",
      "Epoch: 055, Loss: 1.1093, Train: 1.1361, Val: 1.1386\n",
      "Epoch: 056, Loss: 1.1094, Train: 1.0933, Val: 1.0957\n",
      "Epoch: 057, Loss: 1.1122, Train: 1.1505, Val: 1.1532\n",
      "Epoch: 058, Loss: 1.1232, Train: 1.0655, Val: 1.0676\n",
      "Epoch: 059, Loss: 1.1421, Train: 1.1490, Val: 1.1519\n",
      "Epoch: 060, Loss: 1.1542, Train: 1.0624, Val: 1.0646\n",
      "Epoch: 061, Loss: 1.1150, Train: 1.0614, Val: 1.0635\n",
      "Epoch: 062, Loss: 1.1156, Train: 1.1040, Val: 1.1068\n",
      "Epoch: 063, Loss: 1.1320, Train: 1.0599, Val: 1.0622\n",
      "Epoch: 064, Loss: 1.1052, Train: 1.0560, Val: 1.0583\n",
      "Epoch: 065, Loss: 1.1111, Train: 1.1030, Val: 1.1059\n",
      "Epoch: 066, Loss: 1.1421, Train: 1.0887, Val: 1.0901\n",
      "Epoch: 067, Loss: 1.1463, Train: 1.0700, Val: 1.0715\n",
      "Epoch: 068, Loss: 1.1043, Train: 1.0651, Val: 1.0665\n",
      "Epoch: 069, Loss: 1.1349, Train: 1.1807, Val: 1.1812\n",
      "Epoch: 070, Loss: 1.1119, Train: 1.1883, Val: 1.1889\n",
      "Epoch: 071, Loss: 1.1144, Train: 1.0628, Val: 1.0643\n",
      "Epoch: 072, Loss: 1.1145, Train: 1.0660, Val: 1.0673\n",
      "Epoch: 073, Loss: 1.0980, Train: 1.1071, Val: 1.1080\n",
      "Epoch: 074, Loss: 1.1203, Train: 1.0499, Val: 1.0516\n",
      "Epoch: 075, Loss: 1.1044, Train: 1.0491, Val: 1.0508\n",
      "Epoch: 076, Loss: 1.1015, Train: 1.0897, Val: 1.0910\n",
      "Epoch: 077, Loss: 1.1077, Train: 1.0624, Val: 1.0641\n",
      "Epoch: 078, Loss: 1.0939, Train: 1.0463, Val: 1.0485\n",
      "Epoch: 079, Loss: 1.1045, Train: 1.0647, Val: 1.0665\n",
      "Epoch: 080, Loss: 1.0932, Train: 1.0818, Val: 1.0833\n",
      "Epoch: 081, Loss: 1.1000, Train: 1.0490, Val: 1.0510\n",
      "Epoch: 082, Loss: 1.0939, Train: 1.0475, Val: 1.0495\n",
      "Epoch: 083, Loss: 1.0936, Train: 1.0676, Val: 1.0691\n",
      "Epoch: 084, Loss: 1.0945, Train: 1.0504, Val: 1.0521\n",
      "Epoch: 085, Loss: 1.0891, Train: 1.0449, Val: 1.0470\n",
      "Epoch: 086, Loss: 1.0937, Train: 1.0470, Val: 1.0489\n",
      "Epoch: 087, Loss: 1.0874, Train: 1.0516, Val: 1.0534\n",
      "Epoch: 088, Loss: 1.0899, Train: 1.0428, Val: 1.0450\n",
      "Epoch: 089, Loss: 1.0875, Train: 1.0424, Val: 1.0445\n",
      "Epoch: 090, Loss: 1.0856, Train: 1.0550, Val: 1.0568\n",
      "Epoch: 091, Loss: 1.0874, Train: 1.0447, Val: 1.0467\n",
      "Epoch: 092, Loss: 1.0832, Train: 1.0413, Val: 1.0435\n",
      "Epoch: 093, Loss: 1.0849, Train: 1.0456, Val: 1.0475\n",
      "Epoch: 094, Loss: 1.0820, Train: 1.0450, Val: 1.0470\n",
      "Epoch: 095, Loss: 1.0814, Train: 1.0419, Val: 1.0442\n",
      "Epoch: 096, Loss: 1.0816, Train: 1.0404, Val: 1.0425\n",
      "Epoch: 097, Loss: 1.0785, Train: 1.0430, Val: 1.0449\n",
      "Epoch: 098, Loss: 1.0788, Train: 1.0405, Val: 1.0428\n",
      "Epoch: 099, Loss: 1.0772, Train: 1.0396, Val: 1.0419\n",
      "Epoch: 100, Loss: 1.0747, Train: 1.0374, Val: 1.0397\n",
      "Epoch: 101, Loss: 1.0749, Train: 1.0490, Val: 1.0518\n",
      "Epoch: 102, Loss: 1.0724, Train: 1.0477, Val: 1.0504\n",
      "Epoch: 103, Loss: 1.0700, Train: 1.0392, Val: 1.0417\n",
      "Epoch: 104, Loss: 1.0698, Train: 1.0808, Val: 1.0841\n",
      "Epoch: 105, Loss: 1.0690, Train: 1.0507, Val: 1.0537\n",
      "Epoch: 106, Loss: 1.0651, Train: 1.0517, Val: 1.0547\n",
      "Epoch: 107, Loss: 1.0627, Train: 1.0954, Val: 1.0988\n",
      "Epoch: 108, Loss: 1.0639, Train: 1.0355, Val: 1.0383\n",
      "Epoch: 109, Loss: 1.0661, Train: 1.2260, Val: 1.2296\n",
      "Epoch: 110, Loss: 1.0696, Train: 1.1762, Val: 1.1797\n",
      "Epoch: 111, Loss: 1.0639, Train: 1.2131, Val: 1.2168\n",
      "Epoch: 112, Loss: 1.0579, Train: 1.1538, Val: 1.1574\n",
      "Epoch: 113, Loss: 1.0557, Train: 1.0685, Val: 1.0720\n",
      "Epoch: 114, Loss: 1.0546, Train: 1.0914, Val: 1.0953\n",
      "Epoch: 115, Loss: 1.0529, Train: 1.1012, Val: 1.1052\n",
      "Epoch: 116, Loss: 1.0475, Train: 1.1169, Val: 1.1211\n",
      "Epoch: 117, Loss: 1.0456, Train: 1.1443, Val: 1.1486\n",
      "Epoch: 118, Loss: 1.0463, Train: 1.1043, Val: 1.1083\n",
      "Epoch: 119, Loss: 1.0438, Train: 1.1208, Val: 1.1249\n",
      "Epoch: 120, Loss: 1.0402, Train: 1.0671, Val: 1.0710\n",
      "Epoch: 121, Loss: 1.0374, Train: 1.1010, Val: 1.1050\n",
      "Epoch: 122, Loss: 1.0349, Train: 1.1132, Val: 1.1173\n",
      "Epoch: 123, Loss: 1.0326, Train: 1.0815, Val: 1.0856\n",
      "Epoch: 124, Loss: 1.0319, Train: 1.1615, Val: 1.1653\n",
      "Epoch: 125, Loss: 1.0335, Train: 1.0342, Val: 1.0378\n",
      "Epoch: 126, Loss: 1.0372, Train: 1.1373, Val: 1.1414\n",
      "Epoch: 127, Loss: 1.0428, Train: 1.0257, Val: 1.0289\n",
      "Epoch: 128, Loss: 1.0434, Train: 1.0697, Val: 1.0740\n",
      "Epoch: 129, Loss: 1.0336, Train: 1.0543, Val: 1.0584\n",
      "Epoch: 130, Loss: 1.0210, Train: 1.0477, Val: 1.0518\n",
      "Epoch: 131, Loss: 1.0254, Train: 1.1494, Val: 1.1542\n",
      "Epoch: 132, Loss: 1.0333, Train: 1.0208, Val: 1.0249\n",
      "Epoch: 133, Loss: 1.0257, Train: 1.0413, Val: 1.0460\n",
      "Epoch: 134, Loss: 1.0158, Train: 1.0690, Val: 1.0741\n",
      "Epoch: 135, Loss: 1.0177, Train: 1.0395, Val: 1.0444\n",
      "Epoch: 136, Loss: 1.0207, Train: 1.1258, Val: 1.1314\n",
      "Epoch: 137, Loss: 1.0149, Train: 1.0256, Val: 1.0307\n",
      "Epoch: 138, Loss: 1.0117, Train: 1.0400, Val: 1.0452\n",
      "Epoch: 139, Loss: 1.0139, Train: 1.0419, Val: 1.0471\n",
      "Epoch: 140, Loss: 1.0118, Train: 1.0434, Val: 1.0487\n",
      "Epoch: 141, Loss: 1.0063, Train: 1.0152, Val: 1.0203\n",
      "Epoch: 142, Loss: 1.0062, Train: 1.0401, Val: 1.0456\n",
      "Epoch: 143, Loss: 1.0074, Train: 1.0066, Val: 1.0112\n",
      "Epoch: 144, Loss: 1.0038, Train: 1.0400, Val: 1.0457\n",
      "Epoch: 145, Loss: 1.0053, Train: 1.0242, Val: 1.0277\n",
      "Epoch: 146, Loss: 1.0177, Train: 1.0609, Val: 1.0667\n",
      "Epoch: 147, Loss: 1.0122, Train: 1.0001, Val: 1.0045\n",
      "Epoch: 148, Loss: 1.0082, Train: 1.0582, Val: 1.0643\n",
      "Epoch: 149, Loss: 1.0070, Train: 1.0068, Val: 1.0121\n",
      "Epoch: 150, Loss: 1.0014, Train: 1.0190, Val: 1.0226\n",
      "Epoch: 151, Loss: 0.9997, Train: 1.0064, Val: 1.0120\n",
      "Epoch: 152, Loss: 1.0025, Train: 1.0135, Val: 1.0175\n",
      "Epoch: 153, Loss: 1.0047, Train: 1.0173, Val: 1.0234\n",
      "Epoch: 154, Loss: 0.9997, Train: 1.0053, Val: 1.0101\n",
      "Epoch: 155, Loss: 0.9925, Train: 1.0003, Val: 1.0054\n",
      "Epoch: 156, Loss: 0.9953, Train: 1.0175, Val: 1.0216\n",
      "Epoch: 157, Loss: 0.9948, Train: 1.0220, Val: 1.0260\n",
      "Epoch: 158, Loss: 0.9896, Train: 1.0381, Val: 1.0423\n",
      "Epoch: 159, Loss: 0.9877, Train: 1.0500, Val: 1.0532\n",
      "Epoch: 160, Loss: 0.9915, Train: 1.0525, Val: 1.0560\n",
      "Epoch: 161, Loss: 0.9835, Train: 1.1592, Val: 1.1604\n",
      "Epoch: 162, Loss: 0.9842, Train: 1.0490, Val: 1.0523\n",
      "Epoch: 163, Loss: 0.9883, Train: 1.0329, Val: 1.0377\n",
      "Epoch: 164, Loss: 0.9869, Train: 1.1013, Val: 1.1033\n",
      "Epoch: 165, Loss: 1.0003, Train: 1.2153, Val: 1.2162\n",
      "Epoch: 166, Loss: 1.0378, Train: 1.2711, Val: 1.2710\n",
      "Epoch: 167, Loss: 1.0748, Train: 1.1573, Val: 1.1591\n",
      "Epoch: 168, Loss: 1.0665, Train: 1.1928, Val: 1.1942\n",
      "Epoch: 169, Loss: 1.0474, Train: 1.3311, Val: 1.3311\n",
      "Epoch: 170, Loss: 1.0591, Train: 1.2032, Val: 1.2043\n",
      "Epoch: 171, Loss: 1.0339, Train: 1.2050, Val: 1.2062\n",
      "Epoch: 172, Loss: 1.0261, Train: 1.2573, Val: 1.2578\n",
      "Epoch: 173, Loss: 1.0326, Train: 1.1962, Val: 1.1973\n",
      "Epoch: 174, Loss: 1.0154, Train: 1.1186, Val: 1.1208\n",
      "Epoch: 175, Loss: 1.0230, Train: 1.1440, Val: 1.1456\n",
      "Epoch: 176, Loss: 1.0155, Train: 1.2153, Val: 1.2160\n",
      "Epoch: 177, Loss: 1.0198, Train: 1.1589, Val: 1.1606\n",
      "Epoch: 178, Loss: 1.0161, Train: 1.1972, Val: 1.1982\n",
      "Epoch: 179, Loss: 1.0083, Train: 1.2163, Val: 1.2170\n",
      "Epoch: 180, Loss: 1.0103, Train: 1.1364, Val: 1.1381\n",
      "Epoch: 181, Loss: 1.0104, Train: 1.2017, Val: 1.2024\n",
      "Epoch: 182, Loss: 1.0092, Train: 1.2265, Val: 1.2271\n",
      "Epoch: 183, Loss: 1.0090, Train: 1.1824, Val: 1.1835\n",
      "Epoch: 184, Loss: 1.0019, Train: 1.1667, Val: 1.1681\n",
      "Epoch: 185, Loss: 1.0016, Train: 1.2105, Val: 1.2117\n",
      "Epoch: 186, Loss: 1.0023, Train: 1.2098, Val: 1.2111\n",
      "Epoch: 187, Loss: 1.0001, Train: 1.1789, Val: 1.1805\n",
      "Epoch: 188, Loss: 0.9993, Train: 1.1657, Val: 1.1673\n",
      "Epoch: 189, Loss: 0.9963, Train: 1.1213, Val: 1.1233\n",
      "Epoch: 190, Loss: 0.9957, Train: 1.0881, Val: 1.0905\n",
      "Epoch: 191, Loss: 0.9968, Train: 1.1490, Val: 1.1503\n",
      "Epoch: 192, Loss: 0.9961, Train: 1.1168, Val: 1.1188\n",
      "Epoch: 193, Loss: 0.9944, Train: 1.0788, Val: 1.0816\n",
      "Epoch: 194, Loss: 0.9924, Train: 1.0880, Val: 1.0906\n",
      "Epoch: 195, Loss: 0.9917, Train: 1.0765, Val: 1.0795\n",
      "Epoch: 196, Loss: 0.9912, Train: 1.0643, Val: 1.0675\n",
      "Epoch: 197, Loss: 0.9911, Train: 1.0491, Val: 1.0528\n",
      "Epoch: 198, Loss: 0.9899, Train: 1.0346, Val: 1.0386\n",
      "Epoch: 199, Loss: 0.9879, Train: 1.0082, Val: 1.0133\n",
      "Epoch: 200, Loss: 0.9880, Train: 1.0226, Val: 1.0273\n",
      "Epoch: 201, Loss: 0.9864, Train: 1.0471, Val: 1.0510\n",
      "Epoch: 202, Loss: 0.9870, Train: 1.0079, Val: 1.0135\n",
      "Epoch: 203, Loss: 0.9864, Train: 1.0223, Val: 1.0270\n",
      "Epoch: 204, Loss: 0.9860, Train: 0.9995, Val: 1.0057\n",
      "Epoch: 205, Loss: 0.9855, Train: 1.0242, Val: 1.0288\n",
      "Epoch: 206, Loss: 0.9861, Train: 1.0195, Val: 1.0248\n",
      "Epoch: 207, Loss: 0.9865, Train: 1.0246, Val: 1.0291\n",
      "Epoch: 208, Loss: 0.9866, Train: 0.9960, Val: 1.0024\n",
      "Epoch: 209, Loss: 0.9863, Train: 1.0526, Val: 1.0562\n",
      "Epoch: 210, Loss: 0.9875, Train: 1.0202, Val: 1.0254\n",
      "Epoch: 211, Loss: 0.9848, Train: 1.0092, Val: 1.0142\n",
      "Epoch: 212, Loss: 0.9818, Train: 1.0261, Val: 1.0306\n",
      "Epoch: 213, Loss: 0.9789, Train: 1.0336, Val: 1.0381\n",
      "Epoch: 214, Loss: 0.9780, Train: 1.0141, Val: 1.0191\n",
      "Epoch: 215, Loss: 0.9794, Train: 1.0642, Val: 1.0680\n",
      "Epoch: 216, Loss: 0.9801, Train: 1.0228, Val: 1.0272\n",
      "Epoch: 217, Loss: 0.9769, Train: 0.9975, Val: 1.0034\n",
      "Epoch: 218, Loss: 0.9766, Train: 1.0926, Val: 1.0954\n",
      "Epoch: 219, Loss: 0.9791, Train: 1.0189, Val: 1.0236\n",
      "Epoch: 220, Loss: 0.9740, Train: 0.9892, Val: 0.9961\n",
      "Epoch: 221, Loss: 0.9757, Train: 1.1232, Val: 1.1256\n",
      "Epoch: 222, Loss: 0.9799, Train: 1.0080, Val: 1.0131\n",
      "Epoch: 223, Loss: 0.9725, Train: 1.0230, Val: 1.0307\n",
      "Epoch: 224, Loss: 0.9843, Train: 1.1554, Val: 1.1577\n",
      "Epoch: 225, Loss: 0.9827, Train: 1.1548, Val: 1.1573\n",
      "Epoch: 226, Loss: 0.9868, Train: 1.0633, Val: 1.0672\n",
      "Epoch: 227, Loss: 0.9789, Train: 0.9912, Val: 0.9978\n",
      "Epoch: 228, Loss: 0.9764, Train: 1.0097, Val: 1.0165\n",
      "Epoch: 229, Loss: 0.9805, Train: 0.9999, Val: 1.0050\n",
      "Epoch: 230, Loss: 0.9751, Train: 1.0446, Val: 1.0480\n",
      "Epoch: 231, Loss: 0.9760, Train: 1.1206, Val: 1.1230\n",
      "Epoch: 232, Loss: 0.9742, Train: 1.0950, Val: 1.0974\n",
      "Epoch: 233, Loss: 0.9742, Train: 1.0537, Val: 1.0572\n",
      "Epoch: 234, Loss: 0.9718, Train: 1.0666, Val: 1.0691\n",
      "Epoch: 235, Loss: 0.9699, Train: 1.0329, Val: 1.0368\n",
      "Epoch: 236, Loss: 0.9734, Train: 1.1736, Val: 1.1745\n",
      "Epoch: 237, Loss: 0.9834, Train: 1.0228, Val: 1.0288\n",
      "Epoch: 238, Loss: 0.9780, Train: 1.0191, Val: 1.0241\n",
      "Epoch: 239, Loss: 0.9679, Train: 1.0469, Val: 1.0514\n",
      "Epoch: 240, Loss: 0.9668, Train: 1.0286, Val: 1.0348\n",
      "Epoch: 241, Loss: 0.9701, Train: 0.9850, Val: 0.9918\n",
      "Epoch: 242, Loss: 0.9677, Train: 0.9853, Val: 0.9922\n",
      "Epoch: 243, Loss: 0.9635, Train: 1.0025, Val: 1.0098\n",
      "Epoch: 244, Loss: 0.9689, Train: 0.9909, Val: 0.9987\n",
      "Epoch: 245, Loss: 0.9679, Train: 0.9846, Val: 0.9921\n",
      "Epoch: 246, Loss: 0.9748, Train: 1.0261, Val: 1.0330\n",
      "Epoch: 247, Loss: 0.9796, Train: 0.9839, Val: 0.9907\n",
      "Epoch: 248, Loss: 0.9832, Train: 0.9831, Val: 0.9913\n",
      "Epoch: 249, Loss: 0.9729, Train: 1.0455, Val: 1.0539\n",
      "Epoch: 250, Loss: 0.9690, Train: 0.9851, Val: 0.9936\n",
      "Epoch: 251, Loss: 0.9547, Train: 0.9939, Val: 1.0029\n",
      "Epoch: 252, Loss: 0.9566, Train: 1.0547, Val: 1.0633\n",
      "Epoch: 253, Loss: 0.9645, Train: 0.9979, Val: 1.0061\n",
      "Epoch: 254, Loss: 0.9614, Train: 0.9825, Val: 0.9909\n",
      "Epoch: 255, Loss: 0.9598, Train: 1.0017, Val: 1.0102\n",
      "Epoch: 256, Loss: 0.9497, Train: 1.0118, Val: 1.0208\n",
      "Epoch: 257, Loss: 0.9495, Train: 0.9870, Val: 0.9958\n",
      "Epoch: 258, Loss: 0.9545, Train: 0.9952, Val: 1.0038\n",
      "Epoch: 259, Loss: 0.9535, Train: 1.0284, Val: 1.0370\n",
      "Epoch: 260, Loss: 0.9536, Train: 1.0006, Val: 1.0075\n",
      "Epoch: 261, Loss: 0.9474, Train: 0.9921, Val: 1.0008\n",
      "Epoch: 262, Loss: 0.9434, Train: 1.0263, Val: 1.0347\n",
      "Epoch: 263, Loss: 0.9493, Train: 0.9842, Val: 0.9931\n",
      "Epoch: 264, Loss: 0.9494, Train: 0.9883, Val: 0.9971\n",
      "Epoch: 265, Loss: 0.9483, Train: 1.0113, Val: 1.0192\n",
      "Epoch: 266, Loss: 0.9435, Train: 0.9853, Val: 0.9946\n",
      "Epoch: 267, Loss: 0.9384, Train: 0.9863, Val: 0.9954\n",
      "Epoch: 268, Loss: 0.9426, Train: 1.0391, Val: 1.0493\n",
      "Epoch: 269, Loss: 0.9489, Train: 0.9719, Val: 0.9802\n",
      "Epoch: 270, Loss: 0.9515, Train: 0.9874, Val: 0.9969\n",
      "Epoch: 271, Loss: 0.9429, Train: 1.0005, Val: 1.0097\n",
      "Epoch: 272, Loss: 0.9354, Train: 1.0251, Val: 1.0352\n",
      "Epoch: 273, Loss: 0.9376, Train: 1.0414, Val: 1.0517\n",
      "Epoch: 274, Loss: 0.9410, Train: 1.0094, Val: 1.0190\n",
      "Epoch: 275, Loss: 0.9440, Train: 0.9737, Val: 0.9824\n",
      "Epoch: 276, Loss: 0.9404, Train: 1.0862, Val: 1.0960\n",
      "Epoch: 277, Loss: 0.9333, Train: 0.9857, Val: 0.9949\n",
      "Epoch: 278, Loss: 0.9322, Train: 1.0400, Val: 1.0504\n",
      "Epoch: 279, Loss: 0.9401, Train: 1.0223, Val: 1.0306\n",
      "Epoch: 280, Loss: 0.9463, Train: 1.0865, Val: 1.0967\n",
      "Epoch: 281, Loss: 0.9319, Train: 1.0534, Val: 1.0639\n",
      "Epoch: 282, Loss: 0.9320, Train: 1.0252, Val: 1.0336\n",
      "Epoch: 283, Loss: 0.9356, Train: 1.0400, Val: 1.0500\n",
      "Epoch: 284, Loss: 0.9264, Train: 1.1645, Val: 1.1749\n",
      "Epoch: 285, Loss: 0.9345, Train: 1.0305, Val: 1.0382\n",
      "Epoch: 286, Loss: 0.9330, Train: 1.0077, Val: 1.0156\n",
      "Epoch: 287, Loss: 0.9262, Train: 1.0550, Val: 1.0650\n",
      "Epoch: 288, Loss: 0.9291, Train: 1.0843, Val: 1.0936\n",
      "Epoch: 289, Loss: 0.9236, Train: 1.0604, Val: 1.0689\n",
      "Epoch: 290, Loss: 0.9273, Train: 1.0164, Val: 1.0258\n",
      "Epoch: 291, Loss: 0.9209, Train: 1.0156, Val: 1.0247\n",
      "Epoch: 292, Loss: 0.9231, Train: 1.0070, Val: 1.0141\n",
      "Epoch: 293, Loss: 0.9196, Train: 1.0073, Val: 1.0160\n",
      "Epoch: 294, Loss: 0.9194, Train: 1.0136, Val: 1.0242\n",
      "Epoch: 295, Loss: 0.9193, Train: 1.0004, Val: 1.0090\n",
      "Epoch: 296, Loss: 0.9166, Train: 0.9916, Val: 0.9994\n",
      "Epoch: 297, Loss: 0.9208, Train: 1.0022, Val: 1.0118\n",
      "Epoch: 298, Loss: 0.9177, Train: 0.9821, Val: 0.9924\n",
      "Epoch: 299, Loss: 0.9218, Train: 1.0396, Val: 1.0477\n",
      "Epoch: 300, Loss: 0.9234, Train: 0.9624, Val: 0.9701\n",
      "Epoch: 301, Loss: 0.9282, Train: 1.0599, Val: 1.0700\n",
      "Epoch: 302, Loss: 0.9177, Train: 1.1118, Val: 1.1214\n",
      "Epoch: 303, Loss: 0.9124, Train: 0.9710, Val: 0.9804\n",
      "Epoch: 304, Loss: 0.9127, Train: 1.0208, Val: 1.0303\n",
      "Epoch: 305, Loss: 0.9158, Train: 1.0047, Val: 1.0143\n",
      "Epoch: 306, Loss: 0.9087, Train: 1.0734, Val: 1.0835\n",
      "Epoch: 307, Loss: 0.9107, Train: 1.1065, Val: 1.1163\n",
      "Epoch: 308, Loss: 0.9093, Train: 0.9696, Val: 0.9770\n",
      "Epoch: 309, Loss: 0.9095, Train: 0.9877, Val: 0.9979\n",
      "Epoch: 310, Loss: 0.9090, Train: 1.0345, Val: 1.0449\n",
      "Epoch: 311, Loss: 0.9095, Train: 1.1306, Val: 1.1409\n",
      "Epoch: 312, Loss: 0.9204, Train: 1.0075, Val: 1.0149\n",
      "Epoch: 313, Loss: 0.9512, Train: 1.0139, Val: 1.0228\n",
      "Epoch: 314, Loss: 1.0111, Train: 1.0496, Val: 1.0562\n",
      "Epoch: 315, Loss: 0.9890, Train: 1.0617, Val: 1.0657\n",
      "Epoch: 316, Loss: 0.9677, Train: 1.0351, Val: 1.0432\n",
      "Epoch: 317, Loss: 0.9325, Train: 1.1182, Val: 1.1266\n",
      "Epoch: 318, Loss: 0.9759, Train: 1.0461, Val: 1.0548\n",
      "Epoch: 319, Loss: 0.9640, Train: 0.9923, Val: 1.0003\n",
      "Epoch: 320, Loss: 0.9299, Train: 1.0387, Val: 1.0482\n",
      "Epoch: 321, Loss: 0.9500, Train: 0.9727, Val: 0.9805\n",
      "Epoch: 322, Loss: 0.9234, Train: 1.0536, Val: 1.0613\n",
      "Epoch: 323, Loss: 0.9319, Train: 1.0417, Val: 1.0507\n",
      "Epoch: 324, Loss: 0.9171, Train: 0.9924, Val: 1.0001\n",
      "Epoch: 325, Loss: 0.9243, Train: 0.9950, Val: 1.0020\n",
      "Epoch: 326, Loss: 0.9243, Train: 0.9651, Val: 0.9735\n",
      "Epoch: 327, Loss: 0.9289, Train: 0.9931, Val: 1.0015\n",
      "Epoch: 328, Loss: 0.9160, Train: 1.0252, Val: 1.0346\n",
      "Epoch: 329, Loss: 0.9091, Train: 0.9952, Val: 1.0038\n",
      "Epoch: 330, Loss: 0.9076, Train: 0.9709, Val: 0.9779\n",
      "Epoch: 331, Loss: 0.9112, Train: 0.9555, Val: 0.9632\n",
      "Epoch: 332, Loss: 0.9041, Train: 0.9566, Val: 0.9645\n",
      "Epoch: 333, Loss: 0.9030, Train: 0.9622, Val: 0.9694\n",
      "Epoch: 334, Loss: 0.9013, Train: 0.9749, Val: 0.9834\n",
      "Epoch: 335, Loss: 0.9033, Train: 0.9862, Val: 0.9933\n",
      "Epoch: 336, Loss: 0.9080, Train: 1.0001, Val: 1.0098\n",
      "Epoch: 337, Loss: 0.9031, Train: 0.9651, Val: 0.9739\n",
      "Epoch: 338, Loss: 0.9016, Train: 0.9691, Val: 0.9767\n",
      "Epoch: 339, Loss: 0.8996, Train: 0.9522, Val: 0.9615\n",
      "Epoch: 340, Loss: 0.9005, Train: 0.9599, Val: 0.9690\n",
      "Epoch: 341, Loss: 0.8931, Train: 0.9631, Val: 0.9727\n",
      "Epoch: 342, Loss: 0.8911, Train: 0.9648, Val: 0.9749\n",
      "Epoch: 343, Loss: 0.8926, Train: 0.9604, Val: 0.9692\n",
      "Epoch: 344, Loss: 0.8910, Train: 0.9502, Val: 0.9598\n",
      "Epoch: 345, Loss: 0.8914, Train: 0.9491, Val: 0.9594\n",
      "Epoch: 346, Loss: 0.8883, Train: 0.9548, Val: 0.9637\n",
      "Epoch: 347, Loss: 0.8869, Train: 0.9509, Val: 0.9598\n",
      "Epoch: 348, Loss: 0.8850, Train: 0.9493, Val: 0.9580\n",
      "Epoch: 349, Loss: 0.8853, Train: 0.9563, Val: 0.9649\n",
      "Epoch: 350, Loss: 0.8860, Train: 0.9424, Val: 0.9511\n",
      "Epoch: 351, Loss: 0.8843, Train: 0.9527, Val: 0.9609\n",
      "Epoch: 352, Loss: 0.8819, Train: 0.9482, Val: 0.9574\n",
      "Epoch: 353, Loss: 0.8801, Train: 0.9424, Val: 0.9516\n",
      "Epoch: 354, Loss: 0.8799, Train: 0.9578, Val: 0.9657\n",
      "Epoch: 355, Loss: 0.8813, Train: 0.9415, Val: 0.9502\n",
      "Epoch: 356, Loss: 0.8813, Train: 0.9529, Val: 0.9616\n",
      "Epoch: 357, Loss: 0.8799, Train: 0.9395, Val: 0.9488\n",
      "Epoch: 358, Loss: 0.8776, Train: 0.9458, Val: 0.9545\n",
      "Epoch: 359, Loss: 0.8753, Train: 0.9461, Val: 0.9547\n",
      "Epoch: 360, Loss: 0.8750, Train: 0.9415, Val: 0.9507\n",
      "Epoch: 361, Loss: 0.8753, Train: 0.9593, Val: 0.9673\n",
      "Epoch: 362, Loss: 0.8765, Train: 0.9539, Val: 0.9633\n",
      "Epoch: 363, Loss: 0.8761, Train: 0.9602, Val: 0.9681\n",
      "Epoch: 364, Loss: 0.8774, Train: 0.9687, Val: 0.9770\n",
      "Epoch: 365, Loss: 0.8805, Train: 0.9819, Val: 0.9887\n",
      "Epoch: 366, Loss: 0.8867, Train: 0.9928, Val: 1.0011\n",
      "Epoch: 367, Loss: 0.9074, Train: 1.1048, Val: 1.1086\n",
      "Epoch: 368, Loss: 0.9200, Train: 0.9791, Val: 0.9871\n",
      "Epoch: 369, Loss: 0.9181, Train: 0.9531, Val: 0.9620\n",
      "Epoch: 370, Loss: 0.8819, Train: 0.9735, Val: 0.9808\n",
      "Epoch: 371, Loss: 0.8763, Train: 0.9896, Val: 0.9980\n",
      "Epoch: 372, Loss: 0.8904, Train: 0.9617, Val: 0.9719\n",
      "Epoch: 373, Loss: 0.8876, Train: 0.9749, Val: 0.9822\n",
      "Epoch: 374, Loss: 0.8798, Train: 0.9783, Val: 0.9873\n",
      "Epoch: 375, Loss: 0.8702, Train: 0.9700, Val: 0.9807\n",
      "Epoch: 376, Loss: 0.8816, Train: 0.9824, Val: 0.9900\n",
      "Epoch: 377, Loss: 0.8837, Train: 0.9779, Val: 0.9856\n",
      "Epoch: 378, Loss: 0.8793, Train: 0.9528, Val: 0.9621\n",
      "Epoch: 379, Loss: 0.8755, Train: 0.9926, Val: 1.0027\n",
      "Epoch: 380, Loss: 0.8773, Train: 0.9961, Val: 1.0031\n",
      "Epoch: 381, Loss: 0.8803, Train: 0.9892, Val: 0.9956\n",
      "Epoch: 382, Loss: 0.8765, Train: 0.9945, Val: 1.0041\n",
      "Epoch: 383, Loss: 0.8747, Train: 0.9788, Val: 0.9886\n",
      "Epoch: 384, Loss: 0.8781, Train: 1.0084, Val: 1.0146\n",
      "Epoch: 385, Loss: 0.8744, Train: 1.0051, Val: 1.0113\n",
      "Epoch: 386, Loss: 0.8757, Train: 0.9670, Val: 0.9752\n",
      "Epoch: 387, Loss: 0.8704, Train: 0.9895, Val: 0.9983\n",
      "Epoch: 388, Loss: 0.8721, Train: 0.9845, Val: 0.9930\n",
      "Epoch: 389, Loss: 0.8666, Train: 0.9839, Val: 0.9908\n",
      "Epoch: 390, Loss: 0.8654, Train: 1.0044, Val: 1.0107\n",
      "Epoch: 391, Loss: 0.8688, Train: 0.9775, Val: 0.9858\n",
      "Epoch: 392, Loss: 0.8640, Train: 0.9587, Val: 0.9673\n",
      "Epoch: 393, Loss: 0.8621, Train: 0.9629, Val: 0.9704\n",
      "Epoch: 394, Loss: 0.8644, Train: 0.9690, Val: 0.9765\n",
      "Epoch: 395, Loss: 0.8620, Train: 0.9865, Val: 0.9933\n",
      "Epoch: 396, Loss: 0.8622, Train: 0.9579, Val: 0.9659\n",
      "Epoch: 397, Loss: 0.8597, Train: 0.9445, Val: 0.9534\n",
      "Epoch: 398, Loss: 0.8592, Train: 0.9462, Val: 0.9548\n",
      "Epoch: 399, Loss: 0.8592, Train: 0.9598, Val: 0.9677\n",
      "Epoch: 400, Loss: 0.8581, Train: 0.9888, Val: 0.9954\n",
      "Epoch: 401, Loss: 0.8577, Train: 0.9479, Val: 0.9564\n",
      "Epoch: 402, Loss: 0.8563, Train: 0.9398, Val: 0.9491\n",
      "Epoch: 403, Loss: 0.8561, Train: 0.9584, Val: 0.9662\n",
      "Epoch: 404, Loss: 0.8553, Train: 0.9650, Val: 0.9730\n",
      "Epoch: 405, Loss: 0.8565, Train: 0.9721, Val: 0.9793\n",
      "Epoch: 406, Loss: 0.8574, Train: 0.9386, Val: 0.9486\n",
      "Epoch: 407, Loss: 0.8576, Train: 0.9629, Val: 0.9702\n",
      "Epoch: 408, Loss: 0.8562, Train: 0.9515, Val: 0.9602\n",
      "Epoch: 409, Loss: 0.8533, Train: 0.9428, Val: 0.9521\n",
      "Epoch: 410, Loss: 0.8516, Train: 0.9448, Val: 0.9536\n",
      "Epoch: 411, Loss: 0.8536, Train: 0.9505, Val: 0.9602\n",
      "Epoch: 412, Loss: 0.8553, Train: 0.9720, Val: 0.9796\n",
      "Epoch: 413, Loss: 0.8562, Train: 0.9519, Val: 0.9618\n",
      "Epoch: 414, Loss: 0.8549, Train: 0.9314, Val: 0.9410\n",
      "Epoch: 415, Loss: 0.8533, Train: 0.9501, Val: 0.9591\n",
      "Epoch: 416, Loss: 0.8511, Train: 0.9426, Val: 0.9537\n",
      "Epoch: 417, Loss: 0.8531, Train: 0.9517, Val: 0.9604\n",
      "Epoch: 418, Loss: 0.8561, Train: 0.9437, Val: 0.9547\n",
      "Epoch: 419, Loss: 0.8534, Train: 0.9315, Val: 0.9419\n",
      "Epoch: 420, Loss: 0.8494, Train: 0.9269, Val: 0.9374\n",
      "Epoch: 421, Loss: 0.8500, Train: 0.9387, Val: 0.9496\n",
      "Epoch: 422, Loss: 0.8526, Train: 0.9351, Val: 0.9455\n",
      "Epoch: 423, Loss: 0.8527, Train: 0.9328, Val: 0.9442\n",
      "Epoch: 424, Loss: 0.8505, Train: 0.9459, Val: 0.9572\n",
      "Epoch: 425, Loss: 0.8500, Train: 0.9353, Val: 0.9465\n",
      "Epoch: 426, Loss: 0.8536, Train: 0.9748, Val: 0.9858\n",
      "Epoch: 427, Loss: 0.8571, Train: 0.9377, Val: 0.9476\n",
      "Epoch: 428, Loss: 0.8552, Train: 0.9608, Val: 0.9726\n",
      "Epoch: 429, Loss: 0.8509, Train: 0.9436, Val: 0.9536\n",
      "Epoch: 430, Loss: 0.8505, Train: 0.9429, Val: 0.9545\n",
      "Epoch: 431, Loss: 0.8519, Train: 0.9591, Val: 0.9702\n",
      "Epoch: 432, Loss: 0.8522, Train: 0.9363, Val: 0.9478\n",
      "Epoch: 433, Loss: 0.8468, Train: 0.9253, Val: 0.9363\n",
      "Epoch: 434, Loss: 0.8449, Train: 0.9694, Val: 0.9807\n",
      "Epoch: 435, Loss: 0.8470, Train: 0.9302, Val: 0.9409\n",
      "Epoch: 436, Loss: 0.8434, Train: 0.9360, Val: 0.9474\n",
      "Epoch: 437, Loss: 0.8434, Train: 0.9428, Val: 0.9539\n",
      "Epoch: 438, Loss: 0.8443, Train: 0.9590, Val: 0.9702\n",
      "Epoch: 439, Loss: 0.8466, Train: 0.9351, Val: 0.9450\n",
      "Epoch: 440, Loss: 0.8540, Train: 0.9924, Val: 1.0037\n",
      "Epoch: 441, Loss: 0.8666, Train: 0.9624, Val: 0.9699\n",
      "Epoch: 442, Loss: 0.8737, Train: 1.0657, Val: 1.0705\n",
      "Epoch: 443, Loss: 0.8664, Train: 0.9406, Val: 0.9499\n",
      "Epoch: 444, Loss: 0.8526, Train: 0.9439, Val: 0.9532\n",
      "Epoch: 445, Loss: 0.8614, Train: 0.9547, Val: 0.9622\n",
      "Epoch: 446, Loss: 0.8550, Train: 1.0092, Val: 1.0154\n",
      "Epoch: 447, Loss: 0.8542, Train: 0.9758, Val: 0.9827\n",
      "Epoch: 448, Loss: 0.8521, Train: 0.9533, Val: 0.9637\n",
      "Epoch: 449, Loss: 0.8539, Train: 0.9376, Val: 0.9465\n",
      "Epoch: 450, Loss: 0.8539, Train: 1.0322, Val: 1.0372\n",
      "Epoch: 451, Loss: 0.8523, Train: 0.9456, Val: 0.9536\n",
      "Epoch: 452, Loss: 0.8516, Train: 0.9613, Val: 0.9705\n",
      "Epoch: 453, Loss: 0.8619, Train: 0.9580, Val: 0.9656\n",
      "Epoch: 454, Loss: 0.8577, Train: 0.9452, Val: 0.9530\n",
      "Epoch: 455, Loss: 0.8708, Train: 0.9637, Val: 0.9713\n",
      "Epoch: 456, Loss: 0.8661, Train: 0.9552, Val: 0.9642\n",
      "Epoch: 457, Loss: 0.8625, Train: 0.9417, Val: 0.9515\n",
      "Epoch: 458, Loss: 0.8526, Train: 0.9480, Val: 0.9571\n",
      "Epoch: 459, Loss: 0.8436, Train: 0.9588, Val: 0.9698\n",
      "Epoch: 460, Loss: 0.8494, Train: 0.9740, Val: 0.9843\n",
      "Epoch: 461, Loss: 0.8513, Train: 0.9856, Val: 0.9947\n",
      "Epoch: 462, Loss: 0.8549, Train: 0.9498, Val: 0.9597\n",
      "Epoch: 463, Loss: 0.8502, Train: 0.9783, Val: 0.9882\n",
      "Epoch: 464, Loss: 0.8474, Train: 0.9403, Val: 0.9508\n",
      "Epoch: 465, Loss: 0.8427, Train: 0.9605, Val: 0.9701\n",
      "Epoch: 466, Loss: 0.8442, Train: 0.9955, Val: 1.0052\n",
      "Epoch: 467, Loss: 0.8430, Train: 0.9597, Val: 0.9694\n",
      "Epoch: 468, Loss: 0.8413, Train: 0.9668, Val: 0.9756\n",
      "Epoch: 469, Loss: 0.8408, Train: 0.9879, Val: 0.9979\n",
      "Epoch: 470, Loss: 0.8380, Train: 0.9739, Val: 0.9841\n",
      "Epoch: 471, Loss: 0.8377, Train: 0.9621, Val: 0.9715\n",
      "Epoch: 472, Loss: 0.8394, Train: 0.9615, Val: 0.9715\n",
      "Epoch: 473, Loss: 0.8386, Train: 0.9829, Val: 0.9928\n",
      "Epoch: 474, Loss: 0.8368, Train: 0.9653, Val: 0.9761\n",
      "Epoch: 475, Loss: 0.8356, Train: 0.9643, Val: 0.9745\n",
      "Epoch: 476, Loss: 0.8354, Train: 0.9671, Val: 0.9776\n",
      "Epoch: 477, Loss: 0.8333, Train: 0.9702, Val: 0.9812\n",
      "Epoch: 478, Loss: 0.8348, Train: 0.9787, Val: 0.9895\n",
      "Epoch: 479, Loss: 0.8347, Train: 0.9568, Val: 0.9672\n",
      "Epoch: 480, Loss: 0.8355, Train: 0.9674, Val: 0.9787\n",
      "Epoch: 481, Loss: 0.8343, Train: 0.9703, Val: 0.9809\n",
      "Epoch: 482, Loss: 0.8348, Train: 0.9811, Val: 0.9923\n",
      "Epoch: 483, Loss: 0.8340, Train: 0.9568, Val: 0.9671\n",
      "Epoch: 484, Loss: 0.8349, Train: 1.0146, Val: 1.0258\n",
      "Epoch: 485, Loss: 0.8376, Train: 0.9752, Val: 0.9848\n",
      "Epoch: 486, Loss: 0.8426, Train: 0.9405, Val: 0.9515\n",
      "Epoch: 487, Loss: 0.8425, Train: 0.9535, Val: 0.9638\n",
      "Epoch: 488, Loss: 0.8367, Train: 0.9625, Val: 0.9740\n",
      "Epoch: 489, Loss: 0.8372, Train: 0.9774, Val: 0.9884\n",
      "Epoch: 490, Loss: 0.8427, Train: 0.9516, Val: 0.9605\n",
      "Epoch: 491, Loss: 0.8404, Train: 1.0221, Val: 1.0322\n",
      "Epoch: 492, Loss: 0.8390, Train: 0.9465, Val: 0.9566\n",
      "Epoch: 493, Loss: 0.8362, Train: 0.9357, Val: 0.9457\n",
      "Epoch: 494, Loss: 0.8336, Train: 1.0196, Val: 1.0300\n",
      "Epoch: 495, Loss: 0.8377, Train: 0.9688, Val: 0.9782\n",
      "Epoch: 496, Loss: 0.8385, Train: 0.9376, Val: 0.9479\n",
      "Epoch: 497, Loss: 0.8358, Train: 1.0150, Val: 1.0258\n",
      "Epoch: 498, Loss: 0.8363, Train: 0.9595, Val: 0.9698\n",
      "Epoch: 499, Loss: 0.8423, Train: 0.9377, Val: 0.9484\n",
      "Epoch: 500, Loss: 0.8394, Train: 1.0179, Val: 1.0292\n",
      "Test RMSE: 1.0434\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.192428       3.583673\n",
      "std      1727.484387     741.673176       0.588368       1.116938\n",
      "min         0.000000       0.000000       0.776823       1.000000\n",
      "25%      1500.000000     259.000000       2.820768       3.000000\n",
      "50%      3066.000000     693.000000       3.237381       4.000000\n",
      "75%      4472.000000    1292.000000       3.605916       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1043.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.03355141754739\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  2\n",
      "Kernel:  spd\n",
      "Model Name:  KPGraphSAGE\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 14.0819, Train: 3.6561, Val: 3.6590\n",
      "Epoch: 002, Loss: 10.0199, Train: 3.5161, Val: 3.5189\n",
      "Epoch: 003, Loss: 5.8787, Train: 3.3330, Val: 3.3357\n",
      "Epoch: 004, Loss: 1.5822, Train: 3.1090, Val: 3.1116\n",
      "Epoch: 005, Loss: 1.9985, Train: 3.0262, Val: 3.0287\n",
      "Epoch: 006, Loss: 1.3765, Train: 3.0071, Val: 3.0098\n",
      "Epoch: 007, Loss: 1.3911, Train: 2.9496, Val: 2.9522\n",
      "Epoch: 008, Loss: 1.2634, Train: 2.8787, Val: 2.8814\n",
      "Epoch: 009, Loss: 1.3363, Train: 2.8686, Val: 2.8712\n",
      "Epoch: 010, Loss: 1.2485, Train: 2.8313, Val: 2.8340\n",
      "Epoch: 011, Loss: 1.2872, Train: 2.7302, Val: 2.7329\n",
      "Epoch: 012, Loss: 1.2345, Train: 2.6468, Val: 2.6495\n",
      "Epoch: 013, Loss: 1.2806, Train: 2.6337, Val: 2.6364\n",
      "Epoch: 014, Loss: 1.2270, Train: 2.6063, Val: 2.6090\n",
      "Epoch: 015, Loss: 1.2539, Train: 2.5217, Val: 2.5245\n",
      "Epoch: 016, Loss: 1.2137, Train: 2.4478, Val: 2.4506\n",
      "Epoch: 017, Loss: 1.2396, Train: 2.4501, Val: 2.4529\n",
      "Epoch: 018, Loss: 1.2054, Train: 2.4347, Val: 2.4375\n",
      "Epoch: 019, Loss: 1.2138, Train: 2.3647, Val: 2.3675\n",
      "Epoch: 020, Loss: 1.1946, Train: 2.3193, Val: 2.3221\n",
      "Epoch: 021, Loss: 1.1925, Train: 2.3018, Val: 2.3046\n",
      "Epoch: 022, Loss: 1.1839, Train: 2.2609, Val: 2.2637\n",
      "Epoch: 023, Loss: 1.1826, Train: 2.1954, Val: 2.1983\n",
      "Epoch: 024, Loss: 1.1722, Train: 2.1459, Val: 2.1488\n",
      "Epoch: 025, Loss: 1.1722, Train: 2.1199, Val: 2.1229\n",
      "Epoch: 026, Loss: 1.1629, Train: 2.0762, Val: 2.0791\n",
      "Epoch: 027, Loss: 1.1617, Train: 2.0155, Val: 2.0184\n",
      "Epoch: 028, Loss: 1.1541, Train: 1.9716, Val: 1.9746\n",
      "Epoch: 029, Loss: 1.1507, Train: 1.9376, Val: 1.9406\n",
      "Epoch: 030, Loss: 1.1469, Train: 1.8836, Val: 1.8865\n",
      "Epoch: 031, Loss: 1.1425, Train: 1.8163, Val: 1.8193\n",
      "Epoch: 032, Loss: 1.1410, Train: 1.7684, Val: 1.7714\n",
      "Epoch: 033, Loss: 1.1359, Train: 1.7325, Val: 1.7356\n",
      "Epoch: 034, Loss: 1.1360, Train: 1.6818, Val: 1.6848\n",
      "Epoch: 035, Loss: 1.1325, Train: 1.6259, Val: 1.6288\n",
      "Epoch: 036, Loss: 1.1315, Train: 1.5929, Val: 1.5958\n",
      "Epoch: 037, Loss: 1.1294, Train: 1.5645, Val: 1.5674\n",
      "Epoch: 038, Loss: 1.1276, Train: 1.5155, Val: 1.5184\n",
      "Epoch: 039, Loss: 1.1263, Train: 1.4710, Val: 1.4738\n",
      "Epoch: 040, Loss: 1.1238, Train: 1.4446, Val: 1.4474\n",
      "Epoch: 041, Loss: 1.1225, Train: 1.4067, Val: 1.4095\n",
      "Epoch: 042, Loss: 1.1203, Train: 1.3617, Val: 1.3645\n",
      "Epoch: 043, Loss: 1.1184, Train: 1.3376, Val: 1.3404\n",
      "Epoch: 044, Loss: 1.1163, Train: 1.3134, Val: 1.3161\n",
      "Epoch: 045, Loss: 1.1143, Train: 1.2751, Val: 1.2779\n",
      "Epoch: 046, Loss: 1.1129, Train: 1.2487, Val: 1.2515\n",
      "Epoch: 047, Loss: 1.1103, Train: 1.2309, Val: 1.2337\n",
      "Epoch: 048, Loss: 1.1092, Train: 1.2014, Val: 1.2042\n",
      "Epoch: 049, Loss: 1.1068, Train: 1.1829, Val: 1.1856\n",
      "Epoch: 050, Loss: 1.1051, Train: 1.1673, Val: 1.1699\n",
      "Epoch: 051, Loss: 1.1029, Train: 1.1396, Val: 1.1421\n",
      "Epoch: 052, Loss: 1.1013, Train: 1.1301, Val: 1.1325\n",
      "Epoch: 053, Loss: 1.0988, Train: 1.1159, Val: 1.1183\n",
      "Epoch: 054, Loss: 1.0970, Train: 1.0995, Val: 1.1018\n",
      "Epoch: 055, Loss: 1.0947, Train: 1.0968, Val: 1.0992\n",
      "Epoch: 056, Loss: 1.0930, Train: 1.0807, Val: 1.0830\n",
      "Epoch: 057, Loss: 1.0906, Train: 1.0806, Val: 1.0830\n",
      "Epoch: 058, Loss: 1.0882, Train: 1.0695, Val: 1.0718\n",
      "Epoch: 059, Loss: 1.0858, Train: 1.0696, Val: 1.0721\n",
      "Epoch: 060, Loss: 1.0829, Train: 1.0586, Val: 1.0609\n",
      "Epoch: 061, Loss: 1.0803, Train: 1.0622, Val: 1.0646\n",
      "Epoch: 062, Loss: 1.0778, Train: 1.0486, Val: 1.0509\n",
      "Epoch: 063, Loss: 1.0757, Train: 1.0660, Val: 1.0688\n",
      "Epoch: 064, Loss: 1.0766, Train: 1.0500, Val: 1.0521\n",
      "Epoch: 065, Loss: 1.0860, Train: 1.1303, Val: 1.1336\n",
      "Epoch: 066, Loss: 1.1239, Train: 1.0922, Val: 1.0937\n",
      "Epoch: 067, Loss: 1.1418, Train: 1.1170, Val: 1.1205\n",
      "Epoch: 068, Loss: 1.1005, Train: 1.0626, Val: 1.0660\n",
      "Epoch: 069, Loss: 1.0686, Train: 1.0558, Val: 1.0583\n",
      "Epoch: 070, Loss: 1.1094, Train: 1.0828, Val: 1.0865\n",
      "Epoch: 071, Loss: 1.0771, Train: 1.0852, Val: 1.0889\n",
      "Epoch: 072, Loss: 1.0787, Train: 1.0303, Val: 1.0339\n",
      "Epoch: 073, Loss: 1.0736, Train: 1.0317, Val: 1.0354\n",
      "Epoch: 074, Loss: 1.0666, Train: 1.0506, Val: 1.0545\n",
      "Epoch: 075, Loss: 1.0670, Train: 1.0398, Val: 1.0433\n",
      "Epoch: 076, Loss: 1.0548, Train: 1.0333, Val: 1.0364\n",
      "Epoch: 077, Loss: 1.0607, Train: 1.0271, Val: 1.0305\n",
      "Epoch: 078, Loss: 1.0486, Train: 1.0494, Val: 1.0532\n",
      "Epoch: 079, Loss: 1.0574, Train: 1.0276, Val: 1.0315\n",
      "Epoch: 080, Loss: 1.0428, Train: 1.0263, Val: 1.0297\n",
      "Epoch: 081, Loss: 1.0483, Train: 1.0259, Val: 1.0294\n",
      "Epoch: 082, Loss: 1.0403, Train: 1.0317, Val: 1.0355\n",
      "Epoch: 083, Loss: 1.0426, Train: 1.0282, Val: 1.0316\n",
      "Epoch: 084, Loss: 1.0376, Train: 1.0266, Val: 1.0299\n",
      "Epoch: 085, Loss: 1.0367, Train: 1.0235, Val: 1.0277\n",
      "Epoch: 086, Loss: 1.0348, Train: 1.0210, Val: 1.0252\n",
      "Epoch: 087, Loss: 1.0307, Train: 1.0284, Val: 1.0318\n",
      "Epoch: 088, Loss: 1.0313, Train: 1.0177, Val: 1.0215\n",
      "Epoch: 089, Loss: 1.0270, Train: 1.0182, Val: 1.0223\n",
      "Epoch: 090, Loss: 1.0260, Train: 1.0146, Val: 1.0187\n",
      "Epoch: 091, Loss: 1.0244, Train: 1.0130, Val: 1.0172\n",
      "Epoch: 092, Loss: 1.0220, Train: 1.0162, Val: 1.0206\n",
      "Epoch: 093, Loss: 1.0213, Train: 1.0146, Val: 1.0186\n",
      "Epoch: 094, Loss: 1.0181, Train: 1.0139, Val: 1.0176\n",
      "Epoch: 095, Loss: 1.0185, Train: 1.0154, Val: 1.0192\n",
      "Epoch: 096, Loss: 1.0149, Train: 1.0193, Val: 1.0233\n",
      "Epoch: 097, Loss: 1.0152, Train: 1.0191, Val: 1.0227\n",
      "Epoch: 098, Loss: 1.0137, Train: 1.0167, Val: 1.0204\n",
      "Epoch: 099, Loss: 1.0118, Train: 1.0177, Val: 1.0215\n",
      "Epoch: 100, Loss: 1.0100, Train: 1.0414, Val: 1.0444\n",
      "Epoch: 101, Loss: 1.0099, Train: 1.0355, Val: 1.0388\n",
      "Epoch: 102, Loss: 1.0076, Train: 1.0227, Val: 1.0261\n",
      "Epoch: 103, Loss: 1.0068, Train: 1.0470, Val: 1.0500\n",
      "Epoch: 104, Loss: 1.0049, Train: 1.0415, Val: 1.0448\n",
      "Epoch: 105, Loss: 1.0041, Train: 1.0514, Val: 1.0541\n",
      "Epoch: 106, Loss: 1.0040, Train: 1.0380, Val: 1.0411\n",
      "Epoch: 107, Loss: 1.0065, Train: 1.0619, Val: 1.0643\n",
      "Epoch: 108, Loss: 1.0132, Train: 1.0869, Val: 1.0892\n",
      "Epoch: 109, Loss: 1.0035, Train: 1.0338, Val: 1.0368\n",
      "Epoch: 110, Loss: 1.0019, Train: 1.1066, Val: 1.1081\n",
      "Epoch: 111, Loss: 1.0070, Train: 1.0260, Val: 1.0294\n",
      "Epoch: 112, Loss: 1.0058, Train: 1.1070, Val: 1.1086\n",
      "Epoch: 113, Loss: 1.0063, Train: 1.0592, Val: 1.0616\n",
      "Epoch: 114, Loss: 0.9988, Train: 1.0369, Val: 1.0403\n",
      "Epoch: 115, Loss: 1.0042, Train: 1.0799, Val: 1.0822\n",
      "Epoch: 116, Loss: 0.9998, Train: 1.0341, Val: 1.0376\n",
      "Epoch: 117, Loss: 0.9962, Train: 1.0075, Val: 1.0122\n",
      "Epoch: 118, Loss: 1.0002, Train: 1.0903, Val: 1.0927\n",
      "Epoch: 119, Loss: 0.9958, Train: 1.0735, Val: 1.0757\n",
      "Epoch: 120, Loss: 0.9935, Train: 1.0293, Val: 1.0327\n",
      "Epoch: 121, Loss: 0.9945, Train: 1.0608, Val: 1.0644\n",
      "Epoch: 122, Loss: 0.9913, Train: 1.0500, Val: 1.0534\n",
      "Epoch: 123, Loss: 0.9902, Train: 1.0038, Val: 1.0083\n",
      "Epoch: 124, Loss: 0.9918, Train: 1.0748, Val: 1.0779\n",
      "Epoch: 125, Loss: 0.9900, Train: 1.0262, Val: 1.0306\n",
      "Epoch: 126, Loss: 0.9986, Train: 1.1367, Val: 1.1380\n",
      "Epoch: 127, Loss: 1.0167, Train: 1.0107, Val: 1.0156\n",
      "Epoch: 128, Loss: 1.0616, Train: 1.1735, Val: 1.1747\n",
      "Epoch: 129, Loss: 1.0607, Train: 1.0059, Val: 1.0114\n",
      "Epoch: 130, Loss: 1.0085, Train: 1.0234, Val: 1.0265\n",
      "Epoch: 131, Loss: 0.9974, Train: 1.1791, Val: 1.1795\n",
      "Epoch: 132, Loss: 1.0216, Train: 1.0047, Val: 1.0094\n",
      "Epoch: 133, Loss: 1.0177, Train: 1.0024, Val: 1.0081\n",
      "Epoch: 134, Loss: 1.0050, Train: 1.0506, Val: 1.0534\n",
      "Epoch: 135, Loss: 1.0007, Train: 1.0292, Val: 1.0324\n",
      "Epoch: 136, Loss: 1.0080, Train: 1.0737, Val: 1.0762\n",
      "Epoch: 137, Loss: 1.0069, Train: 1.0535, Val: 1.0559\n",
      "Epoch: 138, Loss: 0.9869, Train: 1.0554, Val: 1.0580\n",
      "Epoch: 139, Loss: 0.9954, Train: 1.0545, Val: 1.0578\n",
      "Epoch: 140, Loss: 0.9928, Train: 1.0182, Val: 1.0227\n",
      "Epoch: 141, Loss: 0.9899, Train: 1.0559, Val: 1.0580\n",
      "Epoch: 142, Loss: 0.9863, Train: 1.1155, Val: 1.1161\n",
      "Epoch: 143, Loss: 0.9926, Train: 1.0628, Val: 1.0646\n",
      "Epoch: 144, Loss: 0.9859, Train: 1.0308, Val: 1.0340\n",
      "Epoch: 145, Loss: 0.9835, Train: 1.0554, Val: 1.0581\n",
      "Epoch: 146, Loss: 0.9834, Train: 1.0452, Val: 1.0481\n",
      "Epoch: 147, Loss: 0.9835, Train: 1.0413, Val: 1.0444\n",
      "Epoch: 148, Loss: 0.9804, Train: 1.0640, Val: 1.0665\n",
      "Epoch: 149, Loss: 0.9800, Train: 1.0397, Val: 1.0427\n",
      "Epoch: 150, Loss: 0.9792, Train: 1.0618, Val: 1.0642\n",
      "Epoch: 151, Loss: 0.9785, Train: 1.0668, Val: 1.0692\n",
      "Epoch: 152, Loss: 0.9769, Train: 1.0307, Val: 1.0341\n",
      "Epoch: 153, Loss: 0.9766, Train: 1.0537, Val: 1.0564\n",
      "Epoch: 154, Loss: 0.9751, Train: 1.0394, Val: 1.0426\n",
      "Epoch: 155, Loss: 0.9740, Train: 1.0187, Val: 1.0227\n",
      "Epoch: 156, Loss: 0.9733, Train: 1.0514, Val: 1.0542\n",
      "Epoch: 157, Loss: 0.9731, Train: 1.0354, Val: 1.0384\n",
      "Epoch: 158, Loss: 0.9724, Train: 1.0545, Val: 1.0572\n",
      "Epoch: 159, Loss: 0.9702, Train: 1.0342, Val: 1.0375\n",
      "Epoch: 160, Loss: 0.9689, Train: 1.0122, Val: 1.0165\n",
      "Epoch: 161, Loss: 0.9700, Train: 1.0478, Val: 1.0513\n",
      "Epoch: 162, Loss: 0.9695, Train: 1.0124, Val: 1.0165\n",
      "Epoch: 163, Loss: 0.9677, Train: 1.0396, Val: 1.0429\n",
      "Epoch: 164, Loss: 0.9668, Train: 1.0219, Val: 1.0260\n",
      "Epoch: 165, Loss: 0.9658, Train: 1.0189, Val: 1.0232\n",
      "Epoch: 166, Loss: 0.9642, Train: 1.0196, Val: 1.0238\n",
      "Epoch: 167, Loss: 0.9637, Train: 1.0201, Val: 1.0245\n",
      "Epoch: 168, Loss: 0.9633, Train: 1.0283, Val: 1.0327\n",
      "Epoch: 169, Loss: 0.9623, Train: 0.9996, Val: 1.0047\n",
      "Epoch: 170, Loss: 0.9637, Train: 1.1046, Val: 1.1080\n",
      "Epoch: 171, Loss: 0.9716, Train: 1.0030, Val: 1.0084\n",
      "Epoch: 172, Loss: 1.0051, Train: 1.1227, Val: 1.1259\n",
      "Epoch: 173, Loss: 1.0268, Train: 1.0219, Val: 1.0296\n",
      "Epoch: 174, Loss: 1.0577, Train: 1.0810, Val: 1.0837\n",
      "Epoch: 175, Loss: 1.0475, Train: 1.0526, Val: 1.0565\n",
      "Epoch: 176, Loss: 1.0188, Train: 1.1141, Val: 1.1192\n",
      "Epoch: 177, Loss: 1.0446, Train: 1.0796, Val: 1.0832\n",
      "Epoch: 178, Loss: 1.0186, Train: 1.0006, Val: 1.0060\n",
      "Epoch: 179, Loss: 0.9995, Train: 1.1733, Val: 1.1817\n",
      "Epoch: 180, Loss: 1.0006, Train: 1.3226, Val: 1.3312\n",
      "Epoch: 181, Loss: 1.0014, Train: 1.3282, Val: 1.3361\n",
      "Epoch: 182, Loss: 0.9906, Train: 1.2371, Val: 1.2444\n",
      "Epoch: 183, Loss: 0.9868, Train: 1.0501, Val: 1.0568\n",
      "Epoch: 184, Loss: 0.9897, Train: 1.0054, Val: 1.0111\n",
      "Epoch: 185, Loss: 0.9866, Train: 1.0302, Val: 1.0367\n",
      "Epoch: 186, Loss: 0.9812, Train: 1.0470, Val: 1.0542\n",
      "Epoch: 187, Loss: 0.9773, Train: 1.0535, Val: 1.0612\n",
      "Epoch: 188, Loss: 0.9760, Train: 1.0771, Val: 1.0850\n",
      "Epoch: 189, Loss: 0.9778, Train: 1.0183, Val: 1.0252\n",
      "Epoch: 190, Loss: 0.9750, Train: 1.0083, Val: 1.0149\n",
      "Epoch: 191, Loss: 0.9716, Train: 1.0068, Val: 1.0121\n",
      "Epoch: 192, Loss: 0.9698, Train: 1.0328, Val: 1.0375\n",
      "Epoch: 193, Loss: 0.9684, Train: 1.0265, Val: 1.0323\n",
      "Epoch: 194, Loss: 0.9664, Train: 1.0423, Val: 1.0479\n",
      "Epoch: 195, Loss: 0.9666, Train: 1.0167, Val: 1.0235\n",
      "Epoch: 196, Loss: 0.9620, Train: 1.0047, Val: 1.0113\n",
      "Epoch: 197, Loss: 0.9601, Train: 0.9921, Val: 0.9991\n",
      "Epoch: 198, Loss: 0.9613, Train: 0.9966, Val: 1.0044\n",
      "Epoch: 199, Loss: 0.9594, Train: 0.9993, Val: 1.0063\n",
      "Epoch: 200, Loss: 0.9586, Train: 0.9980, Val: 1.0054\n",
      "Epoch: 201, Loss: 0.9595, Train: 1.0306, Val: 1.0352\n",
      "Epoch: 202, Loss: 0.9593, Train: 1.0077, Val: 1.0124\n",
      "Epoch: 203, Loss: 0.9612, Train: 1.0654, Val: 1.0693\n",
      "Epoch: 204, Loss: 0.9566, Train: 0.9886, Val: 0.9952\n",
      "Epoch: 205, Loss: 0.9528, Train: 0.9924, Val: 0.9991\n",
      "Epoch: 206, Loss: 0.9517, Train: 0.9923, Val: 0.9985\n",
      "Epoch: 207, Loss: 0.9501, Train: 0.9887, Val: 0.9949\n",
      "Epoch: 208, Loss: 0.9505, Train: 1.0505, Val: 1.0552\n",
      "Epoch: 209, Loss: 0.9521, Train: 0.9882, Val: 0.9940\n",
      "Epoch: 210, Loss: 0.9548, Train: 1.0919, Val: 1.0957\n",
      "Epoch: 211, Loss: 0.9586, Train: 0.9864, Val: 0.9924\n",
      "Epoch: 212, Loss: 0.9631, Train: 1.0742, Val: 1.0784\n",
      "Epoch: 213, Loss: 0.9577, Train: 0.9791, Val: 0.9856\n",
      "Epoch: 214, Loss: 0.9489, Train: 0.9904, Val: 0.9962\n",
      "Epoch: 215, Loss: 0.9443, Train: 1.0062, Val: 1.0117\n",
      "Epoch: 216, Loss: 0.9479, Train: 0.9758, Val: 0.9825\n",
      "Epoch: 217, Loss: 0.9518, Train: 1.0340, Val: 1.0394\n",
      "Epoch: 218, Loss: 0.9483, Train: 0.9877, Val: 0.9934\n",
      "Epoch: 219, Loss: 0.9445, Train: 0.9882, Val: 0.9940\n",
      "Epoch: 220, Loss: 0.9410, Train: 0.9746, Val: 0.9820\n",
      "Epoch: 221, Loss: 0.9421, Train: 0.9785, Val: 0.9849\n",
      "Epoch: 222, Loss: 0.9430, Train: 0.9737, Val: 0.9807\n",
      "Epoch: 223, Loss: 0.9394, Train: 0.9713, Val: 0.9786\n",
      "Epoch: 224, Loss: 0.9375, Train: 0.9829, Val: 0.9889\n",
      "Epoch: 225, Loss: 0.9376, Train: 0.9723, Val: 0.9796\n",
      "Epoch: 226, Loss: 0.9380, Train: 0.9749, Val: 0.9822\n",
      "Epoch: 227, Loss: 0.9379, Train: 0.9763, Val: 0.9829\n",
      "Epoch: 228, Loss: 0.9357, Train: 0.9731, Val: 0.9806\n",
      "Epoch: 229, Loss: 0.9342, Train: 0.9718, Val: 0.9791\n",
      "Epoch: 230, Loss: 0.9353, Train: 0.9845, Val: 0.9916\n",
      "Epoch: 231, Loss: 0.9403, Train: 0.9891, Val: 0.9965\n",
      "Epoch: 232, Loss: 0.9418, Train: 1.0079, Val: 1.0149\n",
      "Epoch: 233, Loss: 0.9528, Train: 0.9672, Val: 0.9746\n",
      "Epoch: 234, Loss: 0.9438, Train: 1.0489, Val: 1.0588\n",
      "Epoch: 235, Loss: 0.9605, Train: 0.9912, Val: 0.9968\n",
      "Epoch: 236, Loss: 0.9548, Train: 0.9864, Val: 0.9921\n",
      "Epoch: 237, Loss: 0.9454, Train: 0.9845, Val: 0.9929\n",
      "Epoch: 238, Loss: 0.9403, Train: 1.0040, Val: 1.0128\n",
      "Epoch: 239, Loss: 0.9471, Train: 1.0533, Val: 1.0617\n",
      "Epoch: 240, Loss: 0.9583, Train: 0.9796, Val: 0.9860\n",
      "Epoch: 241, Loss: 0.9492, Train: 0.9796, Val: 0.9860\n",
      "Epoch: 242, Loss: 0.9440, Train: 0.9728, Val: 0.9803\n",
      "Epoch: 243, Loss: 0.9399, Train: 0.9713, Val: 0.9791\n",
      "Epoch: 244, Loss: 0.9365, Train: 1.0158, Val: 1.0246\n",
      "Epoch: 245, Loss: 0.9431, Train: 0.9721, Val: 0.9801\n",
      "Epoch: 246, Loss: 0.9341, Train: 0.9687, Val: 0.9759\n",
      "Epoch: 247, Loss: 0.9325, Train: 0.9878, Val: 0.9954\n",
      "Epoch: 248, Loss: 0.9386, Train: 0.9911, Val: 0.9960\n",
      "Epoch: 249, Loss: 0.9423, Train: 1.0100, Val: 1.0176\n",
      "Epoch: 250, Loss: 0.9532, Train: 0.9878, Val: 0.9936\n",
      "Epoch: 251, Loss: 0.9571, Train: 1.0118, Val: 1.0176\n",
      "Epoch: 252, Loss: 0.9656, Train: 1.0157, Val: 1.0222\n",
      "Epoch: 253, Loss: 0.9824, Train: 0.9841, Val: 0.9899\n",
      "Epoch: 254, Loss: 0.9414, Train: 0.9796, Val: 0.9879\n",
      "Epoch: 255, Loss: 0.9345, Train: 1.1243, Val: 1.1331\n",
      "Epoch: 256, Loss: 0.9628, Train: 0.9847, Val: 0.9933\n",
      "Epoch: 257, Loss: 0.9365, Train: 0.9714, Val: 0.9794\n",
      "Epoch: 258, Loss: 0.9305, Train: 1.0417, Val: 1.0503\n",
      "Epoch: 259, Loss: 0.9428, Train: 1.0726, Val: 1.0814\n",
      "Epoch: 260, Loss: 0.9342, Train: 1.0480, Val: 1.0576\n",
      "Epoch: 261, Loss: 0.9247, Train: 1.0625, Val: 1.0722\n",
      "Epoch: 262, Loss: 0.9318, Train: 1.0307, Val: 1.0397\n",
      "Epoch: 263, Loss: 0.9309, Train: 0.9932, Val: 1.0018\n",
      "Epoch: 264, Loss: 0.9226, Train: 0.9841, Val: 0.9931\n",
      "Epoch: 265, Loss: 0.9232, Train: 1.0143, Val: 1.0242\n",
      "Epoch: 266, Loss: 0.9243, Train: 1.0632, Val: 1.0733\n",
      "Epoch: 267, Loss: 0.9202, Train: 0.9908, Val: 1.0001\n",
      "Epoch: 268, Loss: 0.9169, Train: 0.9680, Val: 0.9764\n",
      "Epoch: 269, Loss: 0.9189, Train: 0.9822, Val: 0.9913\n",
      "Epoch: 270, Loss: 0.9174, Train: 0.9715, Val: 0.9806\n",
      "Epoch: 271, Loss: 0.9143, Train: 0.9736, Val: 0.9831\n",
      "Epoch: 272, Loss: 0.9146, Train: 0.9949, Val: 1.0049\n",
      "Epoch: 273, Loss: 0.9154, Train: 0.9596, Val: 0.9680\n",
      "Epoch: 274, Loss: 0.9131, Train: 0.9632, Val: 0.9722\n",
      "Epoch: 275, Loss: 0.9096, Train: 0.9895, Val: 0.9998\n",
      "Epoch: 276, Loss: 0.9122, Train: 0.9610, Val: 0.9692\n",
      "Epoch: 277, Loss: 0.9116, Train: 0.9634, Val: 0.9723\n",
      "Epoch: 278, Loss: 0.9087, Train: 0.9665, Val: 0.9760\n",
      "Epoch: 279, Loss: 0.9086, Train: 0.9665, Val: 0.9746\n",
      "Epoch: 280, Loss: 0.9111, Train: 0.9862, Val: 0.9957\n",
      "Epoch: 281, Loss: 0.9128, Train: 0.9878, Val: 0.9961\n",
      "Epoch: 282, Loss: 0.9247, Train: 0.9631, Val: 0.9708\n",
      "Epoch: 283, Loss: 0.9123, Train: 1.0420, Val: 1.0516\n",
      "Epoch: 284, Loss: 0.9256, Train: 0.9872, Val: 0.9962\n",
      "Epoch: 285, Loss: 0.9253, Train: 0.9564, Val: 0.9652\n",
      "Epoch: 286, Loss: 0.9200, Train: 1.0157, Val: 1.0250\n",
      "Epoch: 287, Loss: 0.9205, Train: 1.0555, Val: 1.0655\n",
      "Epoch: 288, Loss: 0.9183, Train: 1.0194, Val: 1.0289\n",
      "Epoch: 289, Loss: 0.9179, Train: 0.9701, Val: 0.9791\n",
      "Epoch: 290, Loss: 0.9126, Train: 0.9710, Val: 0.9779\n",
      "Epoch: 291, Loss: 0.9135, Train: 0.9663, Val: 0.9749\n",
      "Epoch: 292, Loss: 0.9118, Train: 1.0096, Val: 1.0196\n",
      "Epoch: 293, Loss: 0.9071, Train: 1.0139, Val: 1.0238\n",
      "Epoch: 294, Loss: 0.9133, Train: 0.9874, Val: 0.9965\n",
      "Epoch: 295, Loss: 0.9046, Train: 0.9596, Val: 0.9669\n",
      "Epoch: 296, Loss: 0.9098, Train: 0.9583, Val: 0.9657\n",
      "Epoch: 297, Loss: 0.9013, Train: 0.9829, Val: 0.9925\n",
      "Epoch: 298, Loss: 0.9028, Train: 0.9740, Val: 0.9833\n",
      "Epoch: 299, Loss: 0.9026, Train: 0.9766, Val: 0.9858\n",
      "Epoch: 300, Loss: 0.9045, Train: 0.9598, Val: 0.9681\n",
      "Epoch: 301, Loss: 0.9061, Train: 0.9805, Val: 0.9879\n",
      "Epoch: 302, Loss: 0.9064, Train: 0.9534, Val: 0.9625\n",
      "Epoch: 303, Loss: 0.8999, Train: 0.9698, Val: 0.9789\n",
      "Epoch: 304, Loss: 0.8997, Train: 0.9546, Val: 0.9629\n",
      "Epoch: 305, Loss: 0.8939, Train: 0.9510, Val: 0.9598\n",
      "Epoch: 306, Loss: 0.8943, Train: 0.9573, Val: 0.9651\n",
      "Epoch: 307, Loss: 0.8930, Train: 0.9580, Val: 0.9669\n",
      "Epoch: 308, Loss: 0.8937, Train: 0.9766, Val: 0.9860\n",
      "Epoch: 309, Loss: 0.8959, Train: 0.9726, Val: 0.9788\n",
      "Epoch: 310, Loss: 0.9004, Train: 0.9556, Val: 0.9642\n",
      "Epoch: 311, Loss: 0.9022, Train: 0.9581, Val: 0.9661\n",
      "Epoch: 312, Loss: 0.8973, Train: 0.9749, Val: 0.9847\n",
      "Epoch: 313, Loss: 0.8916, Train: 0.9538, Val: 0.9622\n",
      "Epoch: 314, Loss: 0.8875, Train: 0.9584, Val: 0.9657\n",
      "Epoch: 315, Loss: 0.8882, Train: 0.9541, Val: 0.9638\n",
      "Epoch: 316, Loss: 0.8892, Train: 0.9742, Val: 0.9813\n",
      "Epoch: 317, Loss: 0.8916, Train: 0.9539, Val: 0.9629\n",
      "Epoch: 318, Loss: 0.8907, Train: 0.9699, Val: 0.9767\n",
      "Epoch: 319, Loss: 0.8914, Train: 0.9552, Val: 0.9631\n",
      "Epoch: 320, Loss: 0.8878, Train: 0.9446, Val: 0.9541\n",
      "Epoch: 321, Loss: 0.8869, Train: 0.9757, Val: 0.9815\n",
      "Epoch: 322, Loss: 0.8889, Train: 0.9459, Val: 0.9540\n",
      "Epoch: 323, Loss: 0.8863, Train: 0.9525, Val: 0.9596\n",
      "Epoch: 324, Loss: 0.8849, Train: 0.9465, Val: 0.9544\n",
      "Epoch: 325, Loss: 0.8838, Train: 0.9539, Val: 0.9614\n",
      "Epoch: 326, Loss: 0.8856, Train: 0.9518, Val: 0.9591\n",
      "Epoch: 327, Loss: 0.8857, Train: 0.9568, Val: 0.9656\n",
      "Epoch: 328, Loss: 0.8812, Train: 0.9429, Val: 0.9511\n",
      "Epoch: 329, Loss: 0.8781, Train: 0.9635, Val: 0.9696\n",
      "Epoch: 330, Loss: 0.8783, Train: 0.9415, Val: 0.9502\n",
      "Epoch: 331, Loss: 0.8763, Train: 0.9451, Val: 0.9534\n",
      "Epoch: 332, Loss: 0.8762, Train: 0.9520, Val: 0.9595\n",
      "Epoch: 333, Loss: 0.8770, Train: 0.9457, Val: 0.9543\n",
      "Epoch: 334, Loss: 0.8761, Train: 0.9414, Val: 0.9494\n",
      "Epoch: 335, Loss: 0.8745, Train: 0.9422, Val: 0.9509\n",
      "Epoch: 336, Loss: 0.8730, Train: 0.9407, Val: 0.9491\n",
      "Epoch: 337, Loss: 0.8737, Train: 0.9429, Val: 0.9520\n",
      "Epoch: 338, Loss: 0.8771, Train: 0.9780, Val: 0.9842\n",
      "Epoch: 339, Loss: 0.8961, Train: 0.9762, Val: 0.9859\n",
      "Epoch: 340, Loss: 0.8988, Train: 1.0313, Val: 1.0365\n",
      "Epoch: 341, Loss: 0.9209, Train: 0.9841, Val: 0.9925\n",
      "Epoch: 342, Loss: 0.9012, Train: 0.9648, Val: 0.9748\n",
      "Epoch: 343, Loss: 0.8826, Train: 0.9955, Val: 1.0053\n",
      "Epoch: 344, Loss: 0.8913, Train: 0.9654, Val: 0.9734\n",
      "Epoch: 345, Loss: 0.8847, Train: 0.9753, Val: 0.9854\n",
      "Epoch: 346, Loss: 0.8860, Train: 0.9884, Val: 0.9986\n",
      "Epoch: 347, Loss: 0.8861, Train: 0.9777, Val: 0.9869\n",
      "Epoch: 348, Loss: 0.8777, Train: 1.0093, Val: 1.0191\n",
      "Epoch: 349, Loss: 0.8762, Train: 0.9682, Val: 0.9782\n",
      "Epoch: 350, Loss: 0.8779, Train: 0.9798, Val: 0.9892\n",
      "Epoch: 351, Loss: 0.8740, Train: 1.0405, Val: 1.0507\n",
      "Epoch: 352, Loss: 0.8747, Train: 1.1015, Val: 1.1123\n",
      "Epoch: 353, Loss: 0.8691, Train: 0.9906, Val: 1.0014\n",
      "Epoch: 354, Loss: 0.8715, Train: 1.0081, Val: 1.0180\n",
      "Epoch: 355, Loss: 0.8682, Train: 1.0895, Val: 1.1003\n",
      "Epoch: 356, Loss: 0.8691, Train: 1.0561, Val: 1.0673\n",
      "Epoch: 357, Loss: 0.8697, Train: 1.0810, Val: 1.0917\n",
      "Epoch: 358, Loss: 0.8649, Train: 1.0416, Val: 1.0524\n",
      "Epoch: 359, Loss: 0.8676, Train: 1.0631, Val: 1.0729\n",
      "Epoch: 360, Loss: 0.8654, Train: 1.0461, Val: 1.0578\n",
      "Epoch: 361, Loss: 0.8663, Train: 1.0866, Val: 1.0973\n",
      "Epoch: 362, Loss: 0.8657, Train: 0.9750, Val: 0.9854\n",
      "Epoch: 363, Loss: 0.8689, Train: 1.0857, Val: 1.0969\n",
      "Epoch: 364, Loss: 0.8667, Train: 0.9902, Val: 1.0012\n",
      "Epoch: 365, Loss: 0.8644, Train: 1.0194, Val: 1.0293\n",
      "Epoch: 366, Loss: 0.8628, Train: 1.0479, Val: 1.0594\n",
      "Epoch: 367, Loss: 0.8627, Train: 0.9683, Val: 0.9781\n",
      "Epoch: 368, Loss: 0.8664, Train: 1.0244, Val: 1.0352\n",
      "Epoch: 369, Loss: 0.8679, Train: 0.9646, Val: 0.9747\n",
      "Epoch: 370, Loss: 0.8725, Train: 0.9971, Val: 1.0079\n",
      "Epoch: 371, Loss: 0.8812, Train: 0.9803, Val: 0.9911\n",
      "Epoch: 372, Loss: 0.8822, Train: 0.9736, Val: 0.9841\n",
      "Epoch: 373, Loss: 0.8650, Train: 1.0050, Val: 1.0163\n",
      "Epoch: 374, Loss: 0.8647, Train: 1.0024, Val: 1.0136\n",
      "Epoch: 375, Loss: 0.8723, Train: 0.9670, Val: 0.9765\n",
      "Epoch: 376, Loss: 0.8705, Train: 1.0026, Val: 1.0133\n",
      "Epoch: 377, Loss: 0.8580, Train: 1.0253, Val: 1.0362\n",
      "Epoch: 378, Loss: 0.8709, Train: 0.9887, Val: 0.9984\n",
      "Epoch: 379, Loss: 0.8645, Train: 1.0009, Val: 1.0114\n",
      "Epoch: 380, Loss: 0.8606, Train: 1.0128, Val: 1.0230\n",
      "Epoch: 381, Loss: 0.8682, Train: 1.1238, Val: 1.1354\n",
      "Epoch: 382, Loss: 0.8818, Train: 1.1318, Val: 1.1328\n",
      "Epoch: 383, Loss: 0.9633, Train: 0.9739, Val: 0.9804\n",
      "Epoch: 384, Loss: 0.9490, Train: 1.0452, Val: 1.0557\n",
      "Epoch: 385, Loss: 0.8786, Train: 1.1290, Val: 1.1371\n",
      "Epoch: 386, Loss: 0.9373, Train: 1.1010, Val: 1.1108\n",
      "Epoch: 387, Loss: 0.9442, Train: 1.0034, Val: 1.0099\n",
      "Epoch: 388, Loss: 0.9591, Train: 1.2825, Val: 1.2808\n",
      "Epoch: 389, Loss: 0.9735, Train: 1.0719, Val: 1.0736\n",
      "Epoch: 390, Loss: 0.9433, Train: 1.0129, Val: 1.0186\n",
      "Epoch: 391, Loss: 0.9693, Train: 1.0318, Val: 1.0404\n",
      "Epoch: 392, Loss: 0.9142, Train: 1.1020, Val: 1.1103\n",
      "Epoch: 393, Loss: 0.9222, Train: 1.0427, Val: 1.0511\n",
      "Epoch: 394, Loss: 0.9160, Train: 1.1181, Val: 1.1269\n",
      "Epoch: 395, Loss: 0.9210, Train: 1.1251, Val: 1.1344\n",
      "Epoch: 396, Loss: 0.8981, Train: 1.0499, Val: 1.0589\n",
      "Epoch: 397, Loss: 0.9043, Train: 1.1103, Val: 1.1185\n",
      "Epoch: 398, Loss: 0.8961, Train: 1.1727, Val: 1.1796\n",
      "Epoch: 399, Loss: 0.8958, Train: 1.0915, Val: 1.0987\n",
      "Epoch: 400, Loss: 0.8947, Train: 1.0492, Val: 1.0576\n",
      "Epoch: 401, Loss: 0.8814, Train: 1.0284, Val: 1.0369\n",
      "Epoch: 402, Loss: 0.8845, Train: 1.0070, Val: 1.0156\n",
      "Epoch: 403, Loss: 0.8829, Train: 1.0828, Val: 1.0924\n",
      "Epoch: 404, Loss: 0.8774, Train: 1.0827, Val: 1.0933\n",
      "Epoch: 405, Loss: 0.8742, Train: 0.9872, Val: 0.9982\n",
      "Epoch: 406, Loss: 0.8736, Train: 0.9656, Val: 0.9757\n",
      "Epoch: 407, Loss: 0.8716, Train: 0.9563, Val: 0.9657\n",
      "Epoch: 408, Loss: 0.8698, Train: 0.9545, Val: 0.9642\n",
      "Epoch: 409, Loss: 0.8676, Train: 0.9959, Val: 1.0068\n",
      "Epoch: 410, Loss: 0.8655, Train: 0.9886, Val: 0.9997\n",
      "Epoch: 411, Loss: 0.8633, Train: 0.9593, Val: 0.9699\n",
      "Epoch: 412, Loss: 0.8624, Train: 0.9659, Val: 0.9767\n",
      "Epoch: 413, Loss: 0.8611, Train: 0.9542, Val: 0.9652\n",
      "Epoch: 414, Loss: 0.8591, Train: 0.9422, Val: 0.9532\n",
      "Epoch: 415, Loss: 0.8583, Train: 0.9500, Val: 0.9611\n",
      "Epoch: 416, Loss: 0.8569, Train: 0.9494, Val: 0.9606\n",
      "Epoch: 417, Loss: 0.8559, Train: 0.9503, Val: 0.9619\n",
      "Epoch: 418, Loss: 0.8553, Train: 0.9536, Val: 0.9653\n",
      "Epoch: 419, Loss: 0.8532, Train: 0.9365, Val: 0.9478\n",
      "Epoch: 420, Loss: 0.8526, Train: 0.9351, Val: 0.9464\n",
      "Epoch: 421, Loss: 0.8516, Train: 0.9474, Val: 0.9592\n",
      "Epoch: 422, Loss: 0.8515, Train: 0.9333, Val: 0.9447\n",
      "Epoch: 423, Loss: 0.8498, Train: 0.9315, Val: 0.9428\n",
      "Epoch: 424, Loss: 0.8490, Train: 0.9314, Val: 0.9427\n",
      "Epoch: 425, Loss: 0.8484, Train: 0.9270, Val: 0.9380\n",
      "Epoch: 426, Loss: 0.8482, Train: 0.9288, Val: 0.9400\n",
      "Epoch: 427, Loss: 0.8470, Train: 0.9298, Val: 0.9413\n",
      "Epoch: 428, Loss: 0.8462, Train: 0.9269, Val: 0.9383\n",
      "Epoch: 429, Loss: 0.8455, Train: 0.9250, Val: 0.9364\n",
      "Epoch: 430, Loss: 0.8451, Train: 0.9256, Val: 0.9366\n",
      "Epoch: 431, Loss: 0.8446, Train: 0.9244, Val: 0.9357\n",
      "Epoch: 432, Loss: 0.8442, Train: 0.9240, Val: 0.9347\n",
      "Epoch: 433, Loss: 0.8434, Train: 0.9220, Val: 0.9330\n",
      "Epoch: 434, Loss: 0.8428, Train: 0.9208, Val: 0.9316\n",
      "Epoch: 435, Loss: 0.8421, Train: 0.9225, Val: 0.9336\n",
      "Epoch: 436, Loss: 0.8415, Train: 0.9225, Val: 0.9334\n",
      "Epoch: 437, Loss: 0.8411, Train: 0.9205, Val: 0.9310\n",
      "Epoch: 438, Loss: 0.8407, Train: 0.9204, Val: 0.9310\n",
      "Epoch: 439, Loss: 0.8404, Train: 0.9211, Val: 0.9312\n",
      "Epoch: 440, Loss: 0.8403, Train: 0.9192, Val: 0.9299\n",
      "Epoch: 441, Loss: 0.8407, Train: 0.9262, Val: 0.9358\n",
      "Epoch: 442, Loss: 0.8430, Train: 0.9281, Val: 0.9383\n",
      "Epoch: 443, Loss: 0.8477, Train: 0.9564, Val: 0.9640\n",
      "Epoch: 444, Loss: 0.8562, Train: 0.9355, Val: 0.9446\n",
      "Epoch: 445, Loss: 0.8521, Train: 0.9382, Val: 0.9465\n",
      "Epoch: 446, Loss: 0.8424, Train: 0.9335, Val: 0.9423\n",
      "Epoch: 447, Loss: 0.8431, Train: 0.9466, Val: 0.9548\n",
      "Epoch: 448, Loss: 0.8462, Train: 0.9357, Val: 0.9441\n",
      "Epoch: 449, Loss: 0.8396, Train: 0.9340, Val: 0.9423\n",
      "Epoch: 450, Loss: 0.8444, Train: 0.9780, Val: 0.9843\n",
      "Epoch: 451, Loss: 0.8461, Train: 0.9319, Val: 0.9402\n",
      "Epoch: 452, Loss: 0.8389, Train: 0.9416, Val: 0.9497\n",
      "Epoch: 453, Loss: 0.8447, Train: 0.9531, Val: 0.9607\n",
      "Epoch: 454, Loss: 0.8404, Train: 0.9439, Val: 0.9518\n",
      "Epoch: 455, Loss: 0.8375, Train: 0.9362, Val: 0.9448\n",
      "Epoch: 456, Loss: 0.8423, Train: 0.9299, Val: 0.9392\n",
      "Epoch: 457, Loss: 0.8374, Train: 0.9520, Val: 0.9599\n",
      "Epoch: 458, Loss: 0.8394, Train: 0.9308, Val: 0.9396\n",
      "Epoch: 459, Loss: 0.8382, Train: 0.9220, Val: 0.9320\n",
      "Epoch: 460, Loss: 0.8351, Train: 0.9352, Val: 0.9447\n",
      "Epoch: 461, Loss: 0.8376, Train: 0.9290, Val: 0.9379\n",
      "Epoch: 462, Loss: 0.8359, Train: 0.9205, Val: 0.9308\n",
      "Epoch: 463, Loss: 0.8344, Train: 0.9318, Val: 0.9417\n",
      "Epoch: 464, Loss: 0.8359, Train: 0.9173, Val: 0.9273\n",
      "Epoch: 465, Loss: 0.8337, Train: 0.9194, Val: 0.9297\n",
      "Epoch: 466, Loss: 0.8343, Train: 0.9302, Val: 0.9401\n",
      "Epoch: 467, Loss: 0.8349, Train: 0.9162, Val: 0.9265\n",
      "Epoch: 468, Loss: 0.8331, Train: 0.9201, Val: 0.9311\n",
      "Epoch: 469, Loss: 0.8342, Train: 0.9338, Val: 0.9435\n",
      "Epoch: 470, Loss: 0.8374, Train: 0.9226, Val: 0.9330\n",
      "Epoch: 471, Loss: 0.8430, Train: 0.9374, Val: 0.9465\n",
      "Epoch: 472, Loss: 0.8599, Train: 0.9415, Val: 0.9526\n",
      "Epoch: 473, Loss: 0.8696, Train: 0.9971, Val: 1.0032\n",
      "Epoch: 474, Loss: 0.8720, Train: 0.9236, Val: 0.9335\n",
      "Epoch: 475, Loss: 0.8474, Train: 0.9347, Val: 0.9461\n",
      "Epoch: 476, Loss: 0.8570, Train: 0.9365, Val: 0.9454\n",
      "Epoch: 477, Loss: 0.8449, Train: 0.9553, Val: 0.9645\n",
      "Epoch: 478, Loss: 0.8642, Train: 0.9425, Val: 0.9513\n",
      "Epoch: 479, Loss: 0.8482, Train: 0.9338, Val: 0.9443\n",
      "Epoch: 480, Loss: 0.8471, Train: 0.9695, Val: 0.9792\n",
      "Epoch: 481, Loss: 0.8416, Train: 0.9623, Val: 0.9712\n",
      "Epoch: 482, Loss: 0.8457, Train: 0.9296, Val: 0.9392\n",
      "Epoch: 483, Loss: 0.8410, Train: 0.9334, Val: 0.9429\n",
      "Epoch: 484, Loss: 0.8390, Train: 0.9537, Val: 0.9620\n",
      "Epoch: 485, Loss: 0.8386, Train: 0.9484, Val: 0.9573\n",
      "Epoch: 486, Loss: 0.8419, Train: 0.9384, Val: 0.9476\n",
      "Epoch: 487, Loss: 0.8367, Train: 0.9427, Val: 0.9513\n",
      "Epoch: 488, Loss: 0.8358, Train: 0.9441, Val: 0.9524\n",
      "Epoch: 489, Loss: 0.8362, Train: 0.9439, Val: 0.9522\n",
      "Epoch: 490, Loss: 0.8353, Train: 0.9366, Val: 0.9461\n",
      "Epoch: 491, Loss: 0.8343, Train: 0.9331, Val: 0.9419\n",
      "Epoch: 492, Loss: 0.8322, Train: 0.9498, Val: 0.9573\n",
      "Epoch: 493, Loss: 0.8340, Train: 0.9290, Val: 0.9385\n",
      "Epoch: 494, Loss: 0.8320, Train: 0.9307, Val: 0.9409\n",
      "Epoch: 495, Loss: 0.8308, Train: 0.9315, Val: 0.9409\n",
      "Epoch: 496, Loss: 0.8306, Train: 0.9301, Val: 0.9400\n",
      "Epoch: 497, Loss: 0.8307, Train: 0.9266, Val: 0.9368\n",
      "Epoch: 498, Loss: 0.8305, Train: 0.9260, Val: 0.9364\n",
      "Epoch: 499, Loss: 0.8281, Train: 0.9307, Val: 0.9408\n",
      "Epoch: 500, Loss: 0.8289, Train: 0.9227, Val: 0.9325\n",
      "Test RMSE: 0.9499\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.601166       3.583673\n",
      "std      1727.484387     741.673176       0.636083       1.116938\n",
      "min         0.000000       0.000000       0.812773       1.000000\n",
      "25%      1500.000000     259.000000       3.205650       3.000000\n",
      "50%      3066.000000     693.000000       3.669785       4.000000\n",
      "75%      4472.000000    1292.000000       4.059100       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1080.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.30699547055863\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  3\n",
      "Kernel:  gd\n",
      "Model Name:  KPGraphSAGE\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 15.3503, Train: 3.7512, Val: 3.7541\n",
      "Epoch: 002, Loss: 11.9488, Train: 3.6345, Val: 3.6374\n",
      "Epoch: 003, Loss: 5.5885, Train: 3.4044, Val: 3.4073\n",
      "Epoch: 004, Loss: 1.2967, Train: 3.1112, Val: 3.1141\n",
      "Epoch: 005, Loss: 4.8375, Train: 3.0993, Val: 3.1023\n",
      "Epoch: 006, Loss: 1.7332, Train: 3.1485, Val: 3.1515\n",
      "Epoch: 007, Loss: 1.3521, Train: 3.1613, Val: 3.1642\n",
      "Epoch: 008, Loss: 1.5554, Train: 3.1098, Val: 3.1126\n",
      "Epoch: 009, Loss: 1.4056, Train: 3.0063, Val: 3.0091\n",
      "Epoch: 010, Loss: 1.2513, Train: 2.8839, Val: 2.8867\n",
      "Epoch: 011, Loss: 1.3953, Train: 2.8112, Val: 2.8139\n",
      "Epoch: 012, Loss: 1.3975, Train: 2.7902, Val: 2.7930\n",
      "Epoch: 013, Loss: 1.2622, Train: 2.7896, Val: 2.7924\n",
      "Epoch: 014, Loss: 1.2645, Train: 2.7696, Val: 2.7724\n",
      "Epoch: 015, Loss: 1.3210, Train: 2.7083, Val: 2.7111\n",
      "Epoch: 016, Loss: 1.2903, Train: 2.6056, Val: 2.6083\n",
      "Epoch: 017, Loss: 1.2455, Train: 2.4923, Val: 2.4950\n",
      "Epoch: 018, Loss: 1.2808, Train: 2.4218, Val: 2.4244\n",
      "Epoch: 019, Loss: 1.2937, Train: 2.4050, Val: 2.4076\n",
      "Epoch: 020, Loss: 1.2487, Train: 2.4064, Val: 2.4091\n",
      "Epoch: 021, Loss: 1.2504, Train: 2.3807, Val: 2.3833\n",
      "Epoch: 022, Loss: 1.2755, Train: 2.3065, Val: 2.3091\n",
      "Epoch: 023, Loss: 1.2570, Train: 2.1971, Val: 2.1996\n",
      "Epoch: 024, Loss: 1.2364, Train: 2.0928, Val: 2.0954\n",
      "Epoch: 025, Loss: 1.2537, Train: 2.0341, Val: 2.0366\n",
      "Epoch: 026, Loss: 1.2489, Train: 2.0195, Val: 2.0220\n",
      "Epoch: 027, Loss: 1.2312, Train: 2.0004, Val: 2.0029\n",
      "Epoch: 028, Loss: 1.2460, Train: 1.9403, Val: 1.9429\n",
      "Epoch: 029, Loss: 1.2336, Train: 1.8536, Val: 1.8561\n",
      "Epoch: 030, Loss: 1.2270, Train: 1.7782, Val: 1.7806\n",
      "Epoch: 031, Loss: 1.2342, Train: 1.7353, Val: 1.7378\n",
      "Epoch: 032, Loss: 1.2307, Train: 1.7172, Val: 1.7196\n",
      "Epoch: 033, Loss: 1.2248, Train: 1.6973, Val: 1.6997\n",
      "Epoch: 034, Loss: 1.2256, Train: 1.6559, Val: 1.6582\n",
      "Epoch: 035, Loss: 1.2207, Train: 1.5940, Val: 1.5963\n",
      "Epoch: 036, Loss: 1.2110, Train: 1.5360, Val: 1.5382\n",
      "Epoch: 037, Loss: 1.2074, Train: 1.5003, Val: 1.5024\n",
      "Epoch: 038, Loss: 1.2058, Train: 1.4494, Val: 1.4515\n",
      "Epoch: 039, Loss: 1.2043, Train: 1.3907, Val: 1.3927\n",
      "Epoch: 040, Loss: 1.2038, Train: 1.3573, Val: 1.3592\n",
      "Epoch: 041, Loss: 1.2022, Train: 1.3460, Val: 1.3479\n",
      "Epoch: 042, Loss: 1.1996, Train: 1.3289, Val: 1.3307\n",
      "Epoch: 043, Loss: 1.1992, Train: 1.2976, Val: 1.2994\n",
      "Epoch: 044, Loss: 1.1971, Train: 1.2701, Val: 1.2717\n",
      "Epoch: 045, Loss: 1.1967, Train: 1.2561, Val: 1.2577\n",
      "Epoch: 046, Loss: 1.1939, Train: 1.2447, Val: 1.2463\n",
      "Epoch: 047, Loss: 1.1920, Train: 1.2223, Val: 1.2237\n",
      "Epoch: 048, Loss: 1.1892, Train: 1.1948, Val: 1.1961\n",
      "Epoch: 049, Loss: 1.1864, Train: 1.1774, Val: 1.1787\n",
      "Epoch: 050, Loss: 1.1837, Train: 1.1710, Val: 1.1722\n",
      "Epoch: 051, Loss: 1.1797, Train: 1.1631, Val: 1.1643\n",
      "Epoch: 052, Loss: 1.1759, Train: 1.1480, Val: 1.1491\n",
      "Epoch: 053, Loss: 1.1708, Train: 1.1348, Val: 1.1358\n",
      "Epoch: 054, Loss: 1.1661, Train: 1.1276, Val: 1.1286\n",
      "Epoch: 055, Loss: 1.1609, Train: 1.1221, Val: 1.1232\n",
      "Epoch: 056, Loss: 1.1564, Train: 1.1132, Val: 1.1143\n",
      "Epoch: 057, Loss: 1.1522, Train: 1.1035, Val: 1.1046\n",
      "Epoch: 058, Loss: 1.1484, Train: 1.0987, Val: 1.0998\n",
      "Epoch: 059, Loss: 1.1450, Train: 1.0991, Val: 1.1002\n",
      "Epoch: 060, Loss: 1.1409, Train: 1.1010, Val: 1.1022\n",
      "Epoch: 061, Loss: 1.1372, Train: 1.1008, Val: 1.1020\n",
      "Epoch: 062, Loss: 1.1337, Train: 1.0999, Val: 1.1011\n",
      "Epoch: 063, Loss: 1.1310, Train: 1.1007, Val: 1.1020\n",
      "Epoch: 064, Loss: 1.1284, Train: 1.1002, Val: 1.1016\n",
      "Epoch: 065, Loss: 1.1262, Train: 1.0961, Val: 1.0976\n",
      "Epoch: 066, Loss: 1.1240, Train: 1.0930, Val: 1.0944\n",
      "Epoch: 067, Loss: 1.1222, Train: 1.0941, Val: 1.0956\n",
      "Epoch: 068, Loss: 1.1204, Train: 1.0981, Val: 1.0997\n",
      "Epoch: 069, Loss: 1.1189, Train: 1.1015, Val: 1.1032\n",
      "Epoch: 070, Loss: 1.1175, Train: 1.1032, Val: 1.1050\n",
      "Epoch: 071, Loss: 1.1163, Train: 1.1020, Val: 1.1038\n",
      "Epoch: 072, Loss: 1.1149, Train: 1.0981, Val: 1.0999\n",
      "Epoch: 073, Loss: 1.1134, Train: 1.0913, Val: 1.0931\n",
      "Epoch: 074, Loss: 1.1121, Train: 1.0889, Val: 1.0908\n",
      "Epoch: 075, Loss: 1.1107, Train: 1.0907, Val: 1.0927\n",
      "Epoch: 076, Loss: 1.1093, Train: 1.0908, Val: 1.0929\n",
      "Epoch: 077, Loss: 1.1079, Train: 1.0872, Val: 1.0893\n",
      "Epoch: 078, Loss: 1.1067, Train: 1.0835, Val: 1.0857\n",
      "Epoch: 079, Loss: 1.1053, Train: 1.0820, Val: 1.0843\n",
      "Epoch: 080, Loss: 1.1040, Train: 1.0777, Val: 1.0801\n",
      "Epoch: 081, Loss: 1.1027, Train: 1.0742, Val: 1.0766\n",
      "Epoch: 082, Loss: 1.1016, Train: 1.0814, Val: 1.0840\n",
      "Epoch: 083, Loss: 1.1004, Train: 1.0860, Val: 1.0887\n",
      "Epoch: 084, Loss: 1.0993, Train: 1.0791, Val: 1.0817\n",
      "Epoch: 085, Loss: 1.0981, Train: 1.0788, Val: 1.0815\n",
      "Epoch: 086, Loss: 1.0969, Train: 1.0849, Val: 1.0876\n",
      "Epoch: 087, Loss: 1.0958, Train: 1.0829, Val: 1.0856\n",
      "Epoch: 088, Loss: 1.0947, Train: 1.0909, Val: 1.0937\n",
      "Epoch: 089, Loss: 1.0937, Train: 1.0765, Val: 1.0791\n",
      "Epoch: 090, Loss: 1.0929, Train: 1.0878, Val: 1.0906\n",
      "Epoch: 091, Loss: 1.0914, Train: 1.0884, Val: 1.0913\n",
      "Epoch: 092, Loss: 1.0902, Train: 1.0934, Val: 1.0965\n",
      "Epoch: 093, Loss: 1.0891, Train: 1.0994, Val: 1.1025\n",
      "Epoch: 094, Loss: 1.0881, Train: 1.0919, Val: 1.0950\n",
      "Epoch: 095, Loss: 1.0876, Train: 1.1242, Val: 1.1274\n",
      "Epoch: 096, Loss: 1.0894, Train: 1.0498, Val: 1.0523\n",
      "Epoch: 097, Loss: 1.0984, Train: 1.0642, Val: 1.0671\n",
      "Epoch: 098, Loss: 1.0899, Train: 1.1218, Val: 1.1254\n",
      "Epoch: 099, Loss: 1.0906, Train: 1.0881, Val: 1.0916\n",
      "Epoch: 100, Loss: 1.0918, Train: 1.0496, Val: 1.0524\n",
      "Epoch: 101, Loss: 1.0857, Train: 1.0474, Val: 1.0496\n",
      "Epoch: 102, Loss: 1.0872, Train: 1.0448, Val: 1.0469\n",
      "Epoch: 103, Loss: 1.0853, Train: 1.0547, Val: 1.0577\n",
      "Epoch: 104, Loss: 1.0832, Train: 1.0640, Val: 1.0671\n",
      "Epoch: 105, Loss: 1.0830, Train: 1.0411, Val: 1.0434\n",
      "Epoch: 106, Loss: 1.0823, Train: 1.0400, Val: 1.0423\n",
      "Epoch: 107, Loss: 1.0805, Train: 1.0433, Val: 1.0461\n",
      "Epoch: 108, Loss: 1.0801, Train: 1.0420, Val: 1.0447\n",
      "Epoch: 109, Loss: 1.0776, Train: 1.0397, Val: 1.0420\n",
      "Epoch: 110, Loss: 1.0779, Train: 1.0405, Val: 1.0431\n",
      "Epoch: 111, Loss: 1.0756, Train: 1.0458, Val: 1.0487\n",
      "Epoch: 112, Loss: 1.0754, Train: 1.0390, Val: 1.0414\n",
      "Epoch: 113, Loss: 1.0731, Train: 1.0375, Val: 1.0396\n",
      "Epoch: 114, Loss: 1.0724, Train: 1.0423, Val: 1.0450\n",
      "Epoch: 115, Loss: 1.0716, Train: 1.0403, Val: 1.0422\n",
      "Epoch: 116, Loss: 1.0703, Train: 1.0418, Val: 1.0445\n",
      "Epoch: 117, Loss: 1.0680, Train: 1.0513, Val: 1.0543\n",
      "Epoch: 118, Loss: 1.0670, Train: 1.0361, Val: 1.0382\n",
      "Epoch: 119, Loss: 1.0671, Train: 1.0590, Val: 1.0621\n",
      "Epoch: 120, Loss: 1.0635, Train: 1.0742, Val: 1.0774\n",
      "Epoch: 121, Loss: 1.0626, Train: 1.0363, Val: 1.0385\n",
      "Epoch: 122, Loss: 1.0621, Train: 1.0527, Val: 1.0556\n",
      "Epoch: 123, Loss: 1.0600, Train: 1.0691, Val: 1.0708\n",
      "Epoch: 124, Loss: 1.0697, Train: 1.0405, Val: 1.0435\n",
      "Epoch: 125, Loss: 1.0563, Train: 1.0618, Val: 1.0651\n",
      "Epoch: 126, Loss: 1.0642, Train: 1.1246, Val: 1.1270\n",
      "Epoch: 127, Loss: 1.0657, Train: 1.0954, Val: 1.0978\n",
      "Epoch: 128, Loss: 1.0594, Train: 1.0988, Val: 1.1033\n",
      "Epoch: 129, Loss: 1.0625, Train: 1.0793, Val: 1.0829\n",
      "Epoch: 130, Loss: 1.0614, Train: 1.1078, Val: 1.1095\n",
      "Epoch: 131, Loss: 1.0767, Train: 1.0702, Val: 1.0727\n",
      "Epoch: 132, Loss: 1.1042, Train: 1.1417, Val: 1.1431\n",
      "Epoch: 133, Loss: 1.0627, Train: 1.1549, Val: 1.1561\n",
      "Epoch: 134, Loss: 1.0722, Train: 1.0497, Val: 1.0529\n",
      "Epoch: 135, Loss: 1.0746, Train: 1.0935, Val: 1.0952\n",
      "Epoch: 136, Loss: 1.0584, Train: 1.1712, Val: 1.1725\n",
      "Epoch: 137, Loss: 1.0614, Train: 1.0915, Val: 1.0936\n",
      "Epoch: 138, Loss: 1.0533, Train: 1.0611, Val: 1.0635\n",
      "Epoch: 139, Loss: 1.0535, Train: 1.0920, Val: 1.0937\n",
      "Epoch: 140, Loss: 1.0517, Train: 1.0946, Val: 1.0962\n",
      "Epoch: 141, Loss: 1.0430, Train: 1.0640, Val: 1.0660\n",
      "Epoch: 142, Loss: 1.0443, Train: 1.1370, Val: 1.1381\n",
      "Epoch: 143, Loss: 1.0393, Train: 1.1366, Val: 1.1374\n",
      "Epoch: 144, Loss: 1.0348, Train: 1.0443, Val: 1.0462\n",
      "Epoch: 145, Loss: 1.0395, Train: 1.1639, Val: 1.1641\n",
      "Epoch: 146, Loss: 1.0331, Train: 1.1382, Val: 1.1388\n",
      "Epoch: 147, Loss: 1.0263, Train: 1.0652, Val: 1.0668\n",
      "Epoch: 148, Loss: 1.0289, Train: 1.0683, Val: 1.0699\n",
      "Epoch: 149, Loss: 1.0226, Train: 1.0375, Val: 1.0400\n",
      "Epoch: 150, Loss: 1.0213, Train: 1.0371, Val: 1.0405\n",
      "Epoch: 151, Loss: 1.0216, Train: 1.0601, Val: 1.0619\n",
      "Epoch: 152, Loss: 1.0178, Train: 1.0596, Val: 1.0614\n",
      "Epoch: 153, Loss: 1.0168, Train: 1.0222, Val: 1.0258\n",
      "Epoch: 154, Loss: 1.0149, Train: 1.0455, Val: 1.0494\n",
      "Epoch: 155, Loss: 1.0110, Train: 1.0570, Val: 1.0613\n",
      "Epoch: 156, Loss: 1.0088, Train: 1.0305, Val: 1.0351\n",
      "Epoch: 157, Loss: 1.0088, Train: 1.0338, Val: 1.0364\n",
      "Epoch: 158, Loss: 1.0159, Train: 1.0808, Val: 1.0858\n",
      "Epoch: 159, Loss: 1.0071, Train: 1.0841, Val: 1.0886\n",
      "Epoch: 160, Loss: 1.0091, Train: 1.0598, Val: 1.0647\n",
      "Epoch: 161, Loss: 1.0091, Train: 1.0911, Val: 1.0963\n",
      "Epoch: 162, Loss: 1.0074, Train: 1.0654, Val: 1.0697\n",
      "Epoch: 163, Loss: 1.0157, Train: 1.0625, Val: 1.0677\n",
      "Epoch: 164, Loss: 1.0302, Train: 1.2095, Val: 1.2155\n",
      "Epoch: 165, Loss: 1.0567, Train: 1.0316, Val: 1.0354\n",
      "Epoch: 166, Loss: 1.0130, Train: 1.0727, Val: 1.0768\n",
      "Epoch: 167, Loss: 1.0326, Train: 1.0578, Val: 1.0617\n",
      "Epoch: 168, Loss: 1.0227, Train: 1.0340, Val: 1.0373\n",
      "Epoch: 169, Loss: 1.0204, Train: 1.0187, Val: 1.0226\n",
      "Epoch: 170, Loss: 1.0114, Train: 1.0855, Val: 1.0903\n",
      "Epoch: 171, Loss: 1.0134, Train: 1.0899, Val: 1.0950\n",
      "Epoch: 172, Loss: 1.0125, Train: 1.1458, Val: 1.1512\n",
      "Epoch: 173, Loss: 1.0108, Train: 1.1650, Val: 1.1707\n",
      "Epoch: 174, Loss: 1.0077, Train: 1.1658, Val: 1.1719\n",
      "Epoch: 175, Loss: 1.0024, Train: 1.2475, Val: 1.2537\n",
      "Epoch: 176, Loss: 1.0094, Train: 1.0835, Val: 1.0875\n",
      "Epoch: 177, Loss: 1.0404, Train: 1.1406, Val: 1.1467\n",
      "Epoch: 178, Loss: 1.0303, Train: 1.1461, Val: 1.1519\n",
      "Epoch: 179, Loss: 1.0147, Train: 1.0094, Val: 1.0135\n",
      "Epoch: 180, Loss: 1.0413, Train: 1.1064, Val: 1.1118\n",
      "Epoch: 181, Loss: 1.0028, Train: 1.1424, Val: 1.1482\n",
      "Epoch: 182, Loss: 1.0158, Train: 1.0431, Val: 1.0478\n",
      "Epoch: 183, Loss: 1.0128, Train: 1.1220, Val: 1.1276\n",
      "Epoch: 184, Loss: 0.9975, Train: 1.2294, Val: 1.2347\n",
      "Epoch: 185, Loss: 1.0080, Train: 1.0412, Val: 1.0469\n",
      "Epoch: 186, Loss: 0.9974, Train: 1.0406, Val: 1.0460\n",
      "Epoch: 187, Loss: 1.0027, Train: 1.1203, Val: 1.1264\n",
      "Epoch: 188, Loss: 0.9990, Train: 1.1427, Val: 1.1487\n",
      "Epoch: 189, Loss: 0.9939, Train: 1.0694, Val: 1.0753\n",
      "Epoch: 190, Loss: 0.9916, Train: 1.0283, Val: 1.0343\n",
      "Epoch: 191, Loss: 0.9873, Train: 1.0096, Val: 1.0155\n",
      "Epoch: 192, Loss: 0.9900, Train: 0.9982, Val: 1.0037\n",
      "Epoch: 193, Loss: 0.9855, Train: 1.0016, Val: 1.0069\n",
      "Epoch: 194, Loss: 0.9852, Train: 1.0083, Val: 1.0131\n",
      "Epoch: 195, Loss: 0.9811, Train: 1.0487, Val: 1.0522\n",
      "Epoch: 196, Loss: 0.9800, Train: 1.0842, Val: 1.0870\n",
      "Epoch: 197, Loss: 0.9805, Train: 1.0314, Val: 1.0357\n",
      "Epoch: 198, Loss: 0.9759, Train: 1.0138, Val: 1.0192\n",
      "Epoch: 199, Loss: 0.9763, Train: 1.0437, Val: 1.0484\n",
      "Epoch: 200, Loss: 0.9734, Train: 1.0837, Val: 1.0876\n",
      "Epoch: 201, Loss: 0.9724, Train: 1.0465, Val: 1.0514\n",
      "Epoch: 202, Loss: 0.9709, Train: 1.0414, Val: 1.0463\n",
      "Epoch: 203, Loss: 0.9687, Train: 1.0572, Val: 1.0615\n",
      "Epoch: 204, Loss: 0.9667, Train: 1.0654, Val: 1.0694\n",
      "Epoch: 205, Loss: 0.9667, Train: 1.0457, Val: 1.0499\n",
      "Epoch: 206, Loss: 0.9638, Train: 1.0138, Val: 1.0192\n",
      "Epoch: 207, Loss: 0.9627, Train: 1.0115, Val: 1.0173\n",
      "Epoch: 208, Loss: 0.9613, Train: 1.0415, Val: 1.0462\n",
      "Epoch: 209, Loss: 0.9602, Train: 0.9983, Val: 1.0043\n",
      "Epoch: 210, Loss: 0.9574, Train: 0.9881, Val: 0.9945\n",
      "Epoch: 211, Loss: 0.9558, Train: 0.9912, Val: 0.9970\n",
      "Epoch: 212, Loss: 0.9547, Train: 0.9849, Val: 0.9914\n",
      "Epoch: 213, Loss: 0.9531, Train: 0.9845, Val: 0.9904\n",
      "Epoch: 214, Loss: 0.9521, Train: 0.9859, Val: 0.9922\n",
      "Epoch: 215, Loss: 0.9527, Train: 0.9892, Val: 0.9945\n",
      "Epoch: 216, Loss: 0.9560, Train: 0.9950, Val: 1.0019\n",
      "Epoch: 217, Loss: 0.9626, Train: 1.0877, Val: 1.0902\n",
      "Epoch: 218, Loss: 0.9699, Train: 0.9972, Val: 1.0043\n",
      "Epoch: 219, Loss: 0.9559, Train: 1.0098, Val: 1.0157\n",
      "Epoch: 220, Loss: 0.9558, Train: 1.0566, Val: 1.0608\n",
      "Epoch: 221, Loss: 0.9539, Train: 1.0013, Val: 1.0079\n",
      "Epoch: 222, Loss: 0.9582, Train: 0.9967, Val: 1.0035\n",
      "Epoch: 223, Loss: 0.9459, Train: 1.0040, Val: 1.0099\n",
      "Epoch: 224, Loss: 0.9444, Train: 1.0011, Val: 1.0073\n",
      "Epoch: 225, Loss: 0.9497, Train: 1.0665, Val: 1.0707\n",
      "Epoch: 226, Loss: 0.9448, Train: 1.0215, Val: 1.0275\n",
      "Epoch: 227, Loss: 0.9432, Train: 1.0452, Val: 1.0497\n",
      "Epoch: 228, Loss: 0.9425, Train: 1.1174, Val: 1.1208\n",
      "Epoch: 229, Loss: 0.9417, Train: 0.9968, Val: 1.0036\n",
      "Epoch: 230, Loss: 0.9379, Train: 1.0156, Val: 1.0212\n",
      "Epoch: 231, Loss: 0.9327, Train: 1.0203, Val: 1.0256\n",
      "Epoch: 232, Loss: 0.9349, Train: 0.9854, Val: 0.9926\n",
      "Epoch: 233, Loss: 0.9302, Train: 0.9817, Val: 0.9886\n",
      "Epoch: 234, Loss: 0.9324, Train: 0.9835, Val: 0.9896\n",
      "Epoch: 235, Loss: 0.9303, Train: 0.9800, Val: 0.9873\n",
      "Epoch: 236, Loss: 0.9346, Train: 1.0738, Val: 1.0776\n",
      "Epoch: 237, Loss: 0.9572, Train: 1.0118, Val: 1.0192\n",
      "Epoch: 238, Loss: 0.9556, Train: 1.0887, Val: 1.0915\n",
      "Epoch: 239, Loss: 0.9668, Train: 0.9864, Val: 0.9924\n",
      "Epoch: 240, Loss: 0.9390, Train: 1.1057, Val: 1.1133\n",
      "Epoch: 241, Loss: 0.9601, Train: 1.0246, Val: 1.0316\n",
      "Epoch: 242, Loss: 0.9521, Train: 1.0034, Val: 1.0107\n",
      "Epoch: 243, Loss: 0.9381, Train: 1.1083, Val: 1.1162\n",
      "Epoch: 244, Loss: 0.9469, Train: 1.0796, Val: 1.0880\n",
      "Epoch: 245, Loss: 0.9423, Train: 1.0259, Val: 1.0344\n",
      "Epoch: 246, Loss: 0.9301, Train: 1.0524, Val: 1.0601\n",
      "Epoch: 247, Loss: 0.9366, Train: 1.0396, Val: 1.0471\n",
      "Epoch: 248, Loss: 0.9416, Train: 1.0146, Val: 1.0228\n",
      "Epoch: 249, Loss: 0.9302, Train: 1.1146, Val: 1.1232\n",
      "Epoch: 250, Loss: 0.9509, Train: 1.0496, Val: 1.0570\n",
      "Epoch: 251, Loss: 0.9427, Train: 1.0442, Val: 1.0524\n",
      "Epoch: 252, Loss: 0.9399, Train: 1.0399, Val: 1.0486\n",
      "Epoch: 253, Loss: 0.9384, Train: 0.9932, Val: 1.0015\n",
      "Epoch: 254, Loss: 0.9293, Train: 1.0362, Val: 1.0457\n",
      "Epoch: 255, Loss: 0.9375, Train: 1.0576, Val: 1.0663\n",
      "Epoch: 256, Loss: 0.9238, Train: 0.9904, Val: 0.9986\n",
      "Epoch: 257, Loss: 0.9252, Train: 0.9973, Val: 1.0054\n",
      "Epoch: 258, Loss: 0.9279, Train: 0.9920, Val: 1.0003\n",
      "Epoch: 259, Loss: 0.9222, Train: 1.0195, Val: 1.0277\n",
      "Epoch: 260, Loss: 0.9167, Train: 1.0410, Val: 1.0488\n",
      "Epoch: 261, Loss: 0.9198, Train: 0.9881, Val: 0.9954\n",
      "Epoch: 262, Loss: 0.9164, Train: 1.0021, Val: 1.0100\n",
      "Epoch: 263, Loss: 0.9159, Train: 1.0095, Val: 1.0173\n",
      "Epoch: 264, Loss: 0.9138, Train: 1.0186, Val: 1.0264\n",
      "Epoch: 265, Loss: 0.9123, Train: 1.1023, Val: 1.1102\n",
      "Epoch: 266, Loss: 0.9110, Train: 1.0749, Val: 1.0829\n",
      "Epoch: 267, Loss: 0.9106, Train: 1.0282, Val: 1.0360\n",
      "Epoch: 268, Loss: 0.9074, Train: 1.0255, Val: 1.0334\n",
      "Epoch: 269, Loss: 0.9075, Train: 0.9891, Val: 0.9971\n",
      "Epoch: 270, Loss: 0.9060, Train: 1.0151, Val: 1.0235\n",
      "Epoch: 271, Loss: 0.9047, Train: 1.0325, Val: 1.0406\n",
      "Epoch: 272, Loss: 0.9052, Train: 1.0167, Val: 1.0249\n",
      "Epoch: 273, Loss: 0.9062, Train: 0.9886, Val: 0.9965\n",
      "Epoch: 274, Loss: 0.9070, Train: 1.0582, Val: 1.0664\n",
      "Epoch: 275, Loss: 0.9103, Train: 1.0011, Val: 1.0073\n",
      "Epoch: 276, Loss: 0.9118, Train: 1.1428, Val: 1.1506\n",
      "Epoch: 277, Loss: 0.9049, Train: 1.0386, Val: 1.0466\n",
      "Epoch: 278, Loss: 0.8988, Train: 1.0265, Val: 1.0350\n",
      "Epoch: 279, Loss: 0.8981, Train: 1.1062, Val: 1.1145\n",
      "Epoch: 280, Loss: 0.8984, Train: 1.0109, Val: 1.0186\n",
      "Epoch: 281, Loss: 0.8978, Train: 1.1020, Val: 1.1106\n",
      "Epoch: 282, Loss: 0.8968, Train: 1.1013, Val: 1.1094\n",
      "Epoch: 283, Loss: 0.9012, Train: 0.9738, Val: 0.9822\n",
      "Epoch: 284, Loss: 0.9160, Train: 1.0315, Val: 1.0392\n",
      "Epoch: 285, Loss: 0.8978, Train: 1.2553, Val: 1.2627\n",
      "Epoch: 286, Loss: 0.9079, Train: 1.0168, Val: 1.0223\n",
      "Epoch: 287, Loss: 0.9277, Train: 0.9976, Val: 1.0040\n",
      "Epoch: 288, Loss: 0.9110, Train: 1.2445, Val: 1.2514\n",
      "Epoch: 289, Loss: 0.9186, Train: 1.0278, Val: 1.0343\n",
      "Epoch: 290, Loss: 0.9072, Train: 1.0155, Val: 1.0216\n",
      "Epoch: 291, Loss: 0.9234, Train: 1.0199, Val: 1.0266\n",
      "Epoch: 292, Loss: 0.9032, Train: 1.0873, Val: 1.0942\n",
      "Epoch: 293, Loss: 0.9077, Train: 1.0726, Val: 1.0802\n",
      "Epoch: 294, Loss: 0.9141, Train: 1.0392, Val: 1.0444\n",
      "Epoch: 295, Loss: 0.9106, Train: 1.0419, Val: 1.0465\n",
      "Epoch: 296, Loss: 0.9034, Train: 1.1129, Val: 1.1204\n",
      "Epoch: 297, Loss: 0.9031, Train: 1.0412, Val: 1.0483\n",
      "Epoch: 298, Loss: 0.9041, Train: 1.0061, Val: 1.0118\n",
      "Epoch: 299, Loss: 0.8927, Train: 1.0077, Val: 1.0147\n",
      "Epoch: 300, Loss: 0.8982, Train: 1.0500, Val: 1.0569\n",
      "Epoch: 301, Loss: 0.8897, Train: 1.0590, Val: 1.0657\n",
      "Epoch: 302, Loss: 0.8917, Train: 1.0602, Val: 1.0670\n",
      "Epoch: 303, Loss: 0.8922, Train: 1.0020, Val: 1.0081\n",
      "Epoch: 304, Loss: 0.8892, Train: 1.0605, Val: 1.0674\n",
      "Epoch: 305, Loss: 0.8876, Train: 1.0560, Val: 1.0630\n",
      "Epoch: 306, Loss: 0.8841, Train: 1.0612, Val: 1.0682\n",
      "Epoch: 307, Loss: 0.8847, Train: 1.0428, Val: 1.0500\n",
      "Epoch: 308, Loss: 0.8813, Train: 1.0512, Val: 1.0583\n",
      "Epoch: 309, Loss: 0.8816, Train: 1.0515, Val: 1.0587\n",
      "Epoch: 310, Loss: 0.8803, Train: 1.0980, Val: 1.1054\n",
      "Epoch: 311, Loss: 0.8800, Train: 1.0758, Val: 1.0828\n",
      "Epoch: 312, Loss: 0.8797, Train: 1.0712, Val: 1.0785\n",
      "Epoch: 313, Loss: 0.8793, Train: 1.0261, Val: 1.0329\n",
      "Epoch: 314, Loss: 0.8808, Train: 1.1615, Val: 1.1689\n",
      "Epoch: 315, Loss: 0.8821, Train: 1.0421, Val: 1.0490\n",
      "Epoch: 316, Loss: 0.8868, Train: 1.2194, Val: 1.2265\n",
      "Epoch: 317, Loss: 0.8996, Train: 1.0283, Val: 1.0330\n",
      "Epoch: 318, Loss: 0.9251, Train: 1.0988, Val: 1.1064\n",
      "Epoch: 319, Loss: 0.9088, Train: 1.1572, Val: 1.1638\n",
      "Epoch: 320, Loss: 0.8991, Train: 1.0470, Val: 1.0544\n",
      "Epoch: 321, Loss: 0.9000, Train: 0.9896, Val: 0.9974\n",
      "Epoch: 322, Loss: 0.8802, Train: 0.9945, Val: 1.0023\n",
      "Epoch: 323, Loss: 0.8980, Train: 0.9954, Val: 1.0016\n",
      "Epoch: 324, Loss: 0.8803, Train: 0.9946, Val: 1.0006\n",
      "Epoch: 325, Loss: 0.8877, Train: 0.9811, Val: 0.9891\n",
      "Epoch: 326, Loss: 0.8784, Train: 0.9584, Val: 0.9659\n",
      "Epoch: 327, Loss: 0.8809, Train: 0.9719, Val: 0.9783\n",
      "Epoch: 328, Loss: 0.8806, Train: 0.9653, Val: 0.9726\n",
      "Epoch: 329, Loss: 0.8750, Train: 0.9830, Val: 0.9909\n",
      "Epoch: 330, Loss: 0.8782, Train: 0.9924, Val: 0.9988\n",
      "Epoch: 331, Loss: 0.8770, Train: 0.9736, Val: 0.9812\n",
      "Epoch: 332, Loss: 0.8799, Train: 0.9958, Val: 1.0021\n",
      "Epoch: 333, Loss: 0.8845, Train: 1.0018, Val: 1.0080\n",
      "Epoch: 334, Loss: 0.8816, Train: 0.9990, Val: 1.0052\n",
      "Epoch: 335, Loss: 0.8825, Train: 0.9788, Val: 0.9878\n",
      "Epoch: 336, Loss: 0.8786, Train: 0.9503, Val: 0.9586\n",
      "Epoch: 337, Loss: 0.8713, Train: 0.9696, Val: 0.9769\n",
      "Epoch: 338, Loss: 0.8698, Train: 0.9670, Val: 0.9752\n",
      "Epoch: 339, Loss: 0.8695, Train: 0.9434, Val: 0.9527\n",
      "Epoch: 340, Loss: 0.8717, Train: 0.9779, Val: 0.9874\n",
      "Epoch: 341, Loss: 0.8749, Train: 1.0045, Val: 1.0104\n",
      "Epoch: 342, Loss: 0.8761, Train: 0.9723, Val: 0.9806\n",
      "Epoch: 343, Loss: 0.8679, Train: 0.9641, Val: 0.9733\n",
      "Epoch: 344, Loss: 0.8650, Train: 0.9589, Val: 0.9677\n",
      "Epoch: 345, Loss: 0.8671, Train: 0.9827, Val: 0.9917\n",
      "Epoch: 346, Loss: 0.8671, Train: 0.9732, Val: 0.9806\n",
      "Epoch: 347, Loss: 0.8704, Train: 0.9735, Val: 0.9826\n",
      "Epoch: 348, Loss: 0.8685, Train: 0.9536, Val: 0.9627\n",
      "Epoch: 349, Loss: 0.8657, Train: 0.9616, Val: 0.9709\n",
      "Epoch: 350, Loss: 0.8612, Train: 0.9670, Val: 0.9764\n",
      "Epoch: 351, Loss: 0.8613, Train: 0.9512, Val: 0.9607\n",
      "Epoch: 352, Loss: 0.8628, Train: 1.0126, Val: 1.0228\n",
      "Epoch: 353, Loss: 0.8624, Train: 0.9504, Val: 0.9598\n",
      "Epoch: 354, Loss: 0.8624, Train: 1.0056, Val: 1.0152\n",
      "Epoch: 355, Loss: 0.8618, Train: 0.9544, Val: 0.9639\n",
      "Epoch: 356, Loss: 0.8634, Train: 0.9758, Val: 0.9850\n",
      "Epoch: 357, Loss: 0.8675, Train: 0.9894, Val: 0.9987\n",
      "Epoch: 358, Loss: 0.8753, Train: 1.0170, Val: 1.0243\n",
      "Epoch: 359, Loss: 0.8858, Train: 1.0408, Val: 1.0511\n",
      "Epoch: 360, Loss: 0.8644, Train: 1.0430, Val: 1.0523\n",
      "Epoch: 361, Loss: 0.8661, Train: 1.0304, Val: 1.0390\n",
      "Epoch: 362, Loss: 0.8719, Train: 1.1197, Val: 1.1288\n",
      "Epoch: 363, Loss: 0.8627, Train: 1.1311, Val: 1.1392\n",
      "Epoch: 364, Loss: 0.8626, Train: 1.0595, Val: 1.0671\n",
      "Epoch: 365, Loss: 0.8692, Train: 1.1579, Val: 1.1651\n",
      "Epoch: 366, Loss: 0.8566, Train: 1.2289, Val: 1.2346\n",
      "Epoch: 367, Loss: 0.8674, Train: 1.1366, Val: 1.1439\n",
      "Epoch: 368, Loss: 0.8641, Train: 1.2253, Val: 1.2315\n",
      "Epoch: 369, Loss: 0.8589, Train: 1.2823, Val: 1.2886\n",
      "Epoch: 370, Loss: 0.8684, Train: 1.2001, Val: 1.2069\n",
      "Epoch: 371, Loss: 0.8662, Train: 1.1652, Val: 1.1709\n",
      "Epoch: 372, Loss: 0.8654, Train: 1.3145, Val: 1.3201\n",
      "Epoch: 373, Loss: 0.8688, Train: 1.0885, Val: 1.0951\n",
      "Epoch: 374, Loss: 0.8746, Train: 1.1104, Val: 1.1157\n",
      "Epoch: 375, Loss: 0.8718, Train: 1.1345, Val: 1.1406\n",
      "Epoch: 376, Loss: 0.8596, Train: 1.1378, Val: 1.1441\n",
      "Epoch: 377, Loss: 0.8643, Train: 1.1660, Val: 1.1724\n",
      "Epoch: 378, Loss: 0.8746, Train: 1.0200, Val: 1.0262\n",
      "Epoch: 379, Loss: 0.8740, Train: 1.0774, Val: 1.0837\n",
      "Epoch: 380, Loss: 0.8635, Train: 1.1690, Val: 1.1750\n",
      "Epoch: 381, Loss: 0.8584, Train: 1.1348, Val: 1.1414\n",
      "Epoch: 382, Loss: 0.8673, Train: 1.0988, Val: 1.1058\n",
      "Epoch: 383, Loss: 0.8688, Train: 1.0217, Val: 1.0276\n",
      "Epoch: 384, Loss: 0.8687, Train: 1.0118, Val: 1.0187\n",
      "Epoch: 385, Loss: 0.8586, Train: 1.1323, Val: 1.1405\n",
      "Epoch: 386, Loss: 0.8565, Train: 1.1197, Val: 1.1274\n",
      "Epoch: 387, Loss: 0.8691, Train: 1.0572, Val: 1.0659\n",
      "Epoch: 388, Loss: 0.8638, Train: 1.0361, Val: 1.0430\n",
      "Epoch: 389, Loss: 0.8551, Train: 1.0686, Val: 1.0757\n",
      "Epoch: 390, Loss: 0.8565, Train: 1.1082, Val: 1.1172\n",
      "Epoch: 391, Loss: 0.8583, Train: 1.1354, Val: 1.1437\n",
      "Epoch: 392, Loss: 0.8555, Train: 1.1325, Val: 1.1408\n",
      "Epoch: 393, Loss: 0.8497, Train: 1.0349, Val: 1.0434\n",
      "Epoch: 394, Loss: 0.8524, Train: 1.0290, Val: 1.0373\n",
      "Epoch: 395, Loss: 0.8514, Train: 1.0828, Val: 1.0913\n",
      "Epoch: 396, Loss: 0.8494, Train: 1.0142, Val: 1.0243\n",
      "Epoch: 397, Loss: 0.8544, Train: 0.9978, Val: 1.0077\n",
      "Epoch: 398, Loss: 0.8479, Train: 1.0195, Val: 1.0288\n",
      "Epoch: 399, Loss: 0.8464, Train: 1.0122, Val: 1.0215\n",
      "Epoch: 400, Loss: 0.8476, Train: 0.9763, Val: 0.9858\n",
      "Epoch: 401, Loss: 0.8462, Train: 1.0382, Val: 1.0464\n",
      "Epoch: 402, Loss: 0.8488, Train: 1.0004, Val: 1.0110\n",
      "Epoch: 403, Loss: 0.8508, Train: 1.0430, Val: 1.0502\n",
      "Epoch: 404, Loss: 0.8576, Train: 1.0148, Val: 1.0232\n",
      "Epoch: 405, Loss: 0.8560, Train: 1.0379, Val: 1.0454\n",
      "Epoch: 406, Loss: 0.8503, Train: 1.0283, Val: 1.0361\n",
      "Epoch: 407, Loss: 0.8444, Train: 1.0091, Val: 1.0179\n",
      "Epoch: 408, Loss: 0.8468, Train: 1.0860, Val: 1.0923\n",
      "Epoch: 409, Loss: 0.8534, Train: 0.9989, Val: 1.0086\n",
      "Epoch: 410, Loss: 0.8468, Train: 1.1263, Val: 1.1315\n",
      "Epoch: 411, Loss: 0.8495, Train: 1.0228, Val: 1.0307\n",
      "Epoch: 412, Loss: 0.8458, Train: 1.0345, Val: 1.0433\n",
      "Epoch: 413, Loss: 0.8447, Train: 1.0438, Val: 1.0518\n",
      "Epoch: 414, Loss: 0.8442, Train: 1.0157, Val: 1.0253\n",
      "Epoch: 415, Loss: 0.8455, Train: 1.0651, Val: 1.0731\n",
      "Epoch: 416, Loss: 0.8436, Train: 1.0106, Val: 1.0204\n",
      "Epoch: 417, Loss: 0.8440, Train: 1.0951, Val: 1.1018\n",
      "Epoch: 418, Loss: 0.8500, Train: 1.0233, Val: 1.0327\n",
      "Epoch: 419, Loss: 0.8474, Train: 1.0445, Val: 1.0536\n",
      "Epoch: 420, Loss: 0.8588, Train: 1.0346, Val: 1.0432\n",
      "Epoch: 421, Loss: 0.8506, Train: 1.0528, Val: 1.0603\n",
      "Epoch: 422, Loss: 0.8470, Train: 1.0027, Val: 1.0116\n",
      "Epoch: 423, Loss: 0.8411, Train: 1.0086, Val: 1.0179\n",
      "Epoch: 424, Loss: 0.8421, Train: 0.9871, Val: 0.9957\n",
      "Epoch: 425, Loss: 0.8423, Train: 0.9684, Val: 0.9779\n",
      "Epoch: 426, Loss: 0.8435, Train: 1.0487, Val: 1.0566\n",
      "Epoch: 427, Loss: 0.8460, Train: 0.9776, Val: 0.9880\n",
      "Epoch: 428, Loss: 0.8400, Train: 1.0333, Val: 1.0412\n",
      "Epoch: 429, Loss: 0.8432, Train: 0.9799, Val: 0.9902\n",
      "Epoch: 430, Loss: 0.8407, Train: 1.0245, Val: 1.0349\n",
      "Epoch: 431, Loss: 0.8398, Train: 1.0720, Val: 1.0799\n",
      "Epoch: 432, Loss: 0.8409, Train: 1.0001, Val: 1.0111\n",
      "Epoch: 433, Loss: 0.8419, Train: 1.0569, Val: 1.0661\n",
      "Epoch: 434, Loss: 0.8427, Train: 1.0393, Val: 1.0495\n",
      "Epoch: 435, Loss: 0.8388, Train: 1.0150, Val: 1.0239\n",
      "Epoch: 436, Loss: 0.8421, Train: 1.0437, Val: 1.0534\n",
      "Epoch: 437, Loss: 0.8396, Train: 1.0623, Val: 1.0721\n",
      "Epoch: 438, Loss: 0.8446, Train: 1.0971, Val: 1.1035\n",
      "Epoch: 439, Loss: 0.8514, Train: 1.0349, Val: 1.0443\n",
      "Epoch: 440, Loss: 0.8446, Train: 1.0783, Val: 1.0887\n",
      "Epoch: 441, Loss: 0.8386, Train: 1.0434, Val: 1.0536\n",
      "Epoch: 442, Loss: 0.8359, Train: 1.0357, Val: 1.0464\n",
      "Epoch: 443, Loss: 0.8382, Train: 1.0557, Val: 1.0652\n",
      "Epoch: 444, Loss: 0.8426, Train: 1.0356, Val: 1.0469\n",
      "Epoch: 445, Loss: 0.8359, Train: 1.0340, Val: 1.0451\n",
      "Epoch: 446, Loss: 0.8350, Train: 1.0839, Val: 1.0902\n",
      "Epoch: 447, Loss: 0.8426, Train: 1.0602, Val: 1.0720\n",
      "Epoch: 448, Loss: 0.8354, Train: 1.0206, Val: 1.0298\n",
      "Epoch: 449, Loss: 0.8358, Train: 1.0452, Val: 1.0534\n",
      "Epoch: 450, Loss: 0.8380, Train: 1.0334, Val: 1.0439\n",
      "Epoch: 451, Loss: 0.8394, Train: 1.0509, Val: 1.0590\n",
      "Epoch: 452, Loss: 0.8382, Train: 1.1125, Val: 1.1161\n",
      "Epoch: 453, Loss: 0.8433, Train: 1.0957, Val: 1.1064\n",
      "Epoch: 454, Loss: 0.8492, Train: 1.2307, Val: 1.2303\n",
      "Epoch: 455, Loss: 0.8693, Train: 1.0189, Val: 1.0270\n",
      "Epoch: 456, Loss: 0.8490, Train: 1.1999, Val: 1.2095\n",
      "Epoch: 457, Loss: 0.8536, Train: 1.2269, Val: 1.2284\n",
      "Epoch: 458, Loss: 0.8722, Train: 1.1312, Val: 1.1351\n",
      "Epoch: 459, Loss: 0.8572, Train: 1.2931, Val: 1.3017\n",
      "Epoch: 460, Loss: 0.8743, Train: 1.1288, Val: 1.1353\n",
      "Epoch: 461, Loss: 0.8546, Train: 1.1826, Val: 1.1874\n",
      "Epoch: 462, Loss: 0.8627, Train: 1.1098, Val: 1.1181\n",
      "Epoch: 463, Loss: 0.8428, Train: 1.2476, Val: 1.2562\n",
      "Epoch: 464, Loss: 0.8650, Train: 1.1277, Val: 1.1353\n",
      "Epoch: 465, Loss: 0.8514, Train: 1.1978, Val: 1.2039\n",
      "Epoch: 466, Loss: 0.8631, Train: 1.0751, Val: 1.0838\n",
      "Epoch: 467, Loss: 0.8406, Train: 1.1263, Val: 1.1344\n",
      "Epoch: 468, Loss: 0.8629, Train: 1.0466, Val: 1.0560\n",
      "Epoch: 469, Loss: 0.8418, Train: 1.0693, Val: 1.0772\n",
      "Epoch: 470, Loss: 0.8511, Train: 1.0150, Val: 1.0243\n",
      "Epoch: 471, Loss: 0.8420, Train: 1.0041, Val: 1.0145\n",
      "Epoch: 472, Loss: 0.8411, Train: 0.9676, Val: 0.9783\n",
      "Epoch: 473, Loss: 0.8465, Train: 0.9762, Val: 0.9861\n",
      "Epoch: 474, Loss: 0.8384, Train: 0.9987, Val: 1.0078\n",
      "Epoch: 475, Loss: 0.8429, Train: 0.9490, Val: 0.9615\n",
      "Epoch: 476, Loss: 0.8366, Train: 1.0104, Val: 1.0219\n",
      "Epoch: 477, Loss: 0.8383, Train: 0.9269, Val: 0.9385\n",
      "Epoch: 478, Loss: 0.8349, Train: 0.9467, Val: 0.9562\n",
      "Epoch: 479, Loss: 0.8343, Train: 0.9491, Val: 0.9588\n",
      "Epoch: 480, Loss: 0.8355, Train: 0.9238, Val: 0.9346\n",
      "Epoch: 481, Loss: 0.8320, Train: 0.9454, Val: 0.9567\n",
      "Epoch: 482, Loss: 0.8344, Train: 0.9277, Val: 0.9395\n",
      "Epoch: 483, Loss: 0.8305, Train: 0.9256, Val: 0.9374\n",
      "Epoch: 484, Loss: 0.8326, Train: 0.9327, Val: 0.9439\n",
      "Epoch: 485, Loss: 0.8310, Train: 0.9823, Val: 0.9939\n",
      "Epoch: 486, Loss: 0.8327, Train: 0.9557, Val: 0.9674\n",
      "Epoch: 487, Loss: 0.8346, Train: 1.0259, Val: 1.0377\n",
      "Epoch: 488, Loss: 0.8455, Train: 0.9643, Val: 0.9745\n",
      "Epoch: 489, Loss: 0.8714, Train: 1.1037, Val: 1.1146\n",
      "Epoch: 490, Loss: 0.9056, Train: 0.9737, Val: 0.9833\n",
      "Epoch: 491, Loss: 0.9016, Train: 0.9555, Val: 0.9667\n",
      "Epoch: 492, Loss: 0.8552, Train: 1.1010, Val: 1.1105\n",
      "Epoch: 493, Loss: 0.8467, Train: 1.0500, Val: 1.0596\n",
      "Epoch: 494, Loss: 0.8758, Train: 1.1152, Val: 1.1263\n",
      "Epoch: 495, Loss: 0.8695, Train: 1.1204, Val: 1.1295\n",
      "Epoch: 496, Loss: 0.8463, Train: 1.1101, Val: 1.1197\n",
      "Epoch: 497, Loss: 0.8544, Train: 1.2969, Val: 1.3058\n",
      "Epoch: 498, Loss: 0.8617, Train: 1.2763, Val: 1.2845\n",
      "Epoch: 499, Loss: 0.8505, Train: 1.1090, Val: 1.1187\n",
      "Epoch: 500, Loss: 0.8430, Train: 1.2374, Val: 1.2458\n",
      "Test RMSE: 1.2617\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       2.869032       3.583673\n",
      "std      1727.484387     741.673176       0.671733       1.116938\n",
      "min         0.000000       0.000000       0.703548       1.000000\n",
      "25%      1500.000000     259.000000       2.391177       3.000000\n",
      "50%      3066.000000     693.000000       2.858021       4.000000\n",
      "75%      4472.000000    1292.000000       3.335195       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1012.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.04193927193424\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  3\n",
      "Kernel:  spd\n",
      "Model Name:  KPGraphSAGE\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 12.8064, Train: 3.4580, Val: 3.4609\n",
      "Epoch: 002, Loss: 7.0450, Train: 3.2178, Val: 3.2206\n",
      "Epoch: 003, Loss: 1.8249, Train: 2.8426, Val: 2.8454\n",
      "Epoch: 004, Loss: 6.2779, Train: 2.8445, Val: 2.8473\n",
      "Epoch: 005, Loss: 1.8679, Train: 2.9468, Val: 2.9496\n",
      "Epoch: 006, Loss: 1.4981, Train: 2.9619, Val: 2.9647\n",
      "Epoch: 007, Loss: 1.6948, Train: 2.9114, Val: 2.9141\n",
      "Epoch: 008, Loss: 1.3599, Train: 2.8121, Val: 2.8148\n",
      "Epoch: 009, Loss: 1.2871, Train: 2.7272, Val: 2.7299\n",
      "Epoch: 010, Loss: 1.4633, Train: 2.7116, Val: 2.7143\n",
      "Epoch: 011, Loss: 1.3336, Train: 2.7285, Val: 2.7313\n",
      "Epoch: 012, Loss: 1.2802, Train: 2.7251, Val: 2.7279\n",
      "Epoch: 013, Loss: 1.3483, Train: 2.6793, Val: 2.6820\n",
      "Epoch: 014, Loss: 1.3141, Train: 2.5972, Val: 2.5998\n",
      "Epoch: 015, Loss: 1.2562, Train: 2.5051, Val: 2.5077\n",
      "Epoch: 016, Loss: 1.2990, Train: 2.4469, Val: 2.4495\n",
      "Epoch: 017, Loss: 1.3219, Train: 2.4349, Val: 2.4376\n",
      "Epoch: 018, Loss: 1.2643, Train: 2.4405, Val: 2.4432\n",
      "Epoch: 019, Loss: 1.2554, Train: 2.4239, Val: 2.4267\n",
      "Epoch: 020, Loss: 1.2868, Train: 2.3641, Val: 2.3669\n",
      "Epoch: 021, Loss: 1.2767, Train: 2.2684, Val: 2.2712\n",
      "Epoch: 022, Loss: 1.2538, Train: 2.1701, Val: 2.1728\n",
      "Epoch: 023, Loss: 1.2679, Train: 2.1103, Val: 2.1130\n",
      "Epoch: 024, Loss: 1.2693, Train: 2.0976, Val: 2.1003\n",
      "Epoch: 025, Loss: 1.2439, Train: 2.1005, Val: 2.1031\n",
      "Epoch: 026, Loss: 1.2457, Train: 2.0789, Val: 2.0814\n",
      "Epoch: 027, Loss: 1.2600, Train: 2.0148, Val: 2.0173\n",
      "Epoch: 028, Loss: 1.2494, Train: 1.9210, Val: 1.9234\n",
      "Epoch: 029, Loss: 1.2379, Train: 1.8329, Val: 1.8353\n",
      "Epoch: 030, Loss: 1.2429, Train: 1.7850, Val: 1.7874\n",
      "Epoch: 031, Loss: 1.2364, Train: 1.7747, Val: 1.7772\n",
      "Epoch: 032, Loss: 1.2253, Train: 1.7664, Val: 1.7689\n",
      "Epoch: 033, Loss: 1.2268, Train: 1.7344, Val: 1.7370\n",
      "Epoch: 034, Loss: 1.2307, Train: 1.6506, Val: 1.6531\n",
      "Epoch: 035, Loss: 1.2120, Train: 1.5565, Val: 1.5589\n",
      "Epoch: 036, Loss: 1.2163, Train: 1.5173, Val: 1.5196\n",
      "Epoch: 037, Loss: 1.2097, Train: 1.5216, Val: 1.5240\n",
      "Epoch: 038, Loss: 1.2016, Train: 1.5092, Val: 1.5115\n",
      "Epoch: 039, Loss: 1.2063, Train: 1.4519, Val: 1.4542\n",
      "Epoch: 040, Loss: 1.1968, Train: 1.3734, Val: 1.3756\n",
      "Epoch: 041, Loss: 1.1910, Train: 1.3280, Val: 1.3301\n",
      "Epoch: 042, Loss: 1.1921, Train: 1.3301, Val: 1.3322\n",
      "Epoch: 043, Loss: 1.1809, Train: 1.3394, Val: 1.3416\n",
      "Epoch: 044, Loss: 1.1801, Train: 1.3165, Val: 1.3186\n",
      "Epoch: 045, Loss: 1.1736, Train: 1.2677, Val: 1.2697\n",
      "Epoch: 046, Loss: 1.1642, Train: 1.2327, Val: 1.2346\n",
      "Epoch: 047, Loss: 1.1627, Train: 1.2373, Val: 1.2392\n",
      "Epoch: 048, Loss: 1.1533, Train: 1.2611, Val: 1.2630\n",
      "Epoch: 049, Loss: 1.1489, Train: 1.2601, Val: 1.2620\n",
      "Epoch: 050, Loss: 1.1464, Train: 1.2221, Val: 1.2240\n",
      "Epoch: 051, Loss: 1.1396, Train: 1.1827, Val: 1.1846\n",
      "Epoch: 052, Loss: 1.1380, Train: 1.1726, Val: 1.1746\n",
      "Epoch: 053, Loss: 1.1341, Train: 1.1780, Val: 1.1801\n",
      "Epoch: 054, Loss: 1.1316, Train: 1.1658, Val: 1.1680\n",
      "Epoch: 055, Loss: 1.1305, Train: 1.1307, Val: 1.1328\n",
      "Epoch: 056, Loss: 1.1269, Train: 1.1012, Val: 1.1032\n",
      "Epoch: 057, Loss: 1.1257, Train: 1.0920, Val: 1.0941\n",
      "Epoch: 058, Loss: 1.1232, Train: 1.0948, Val: 1.0970\n",
      "Epoch: 059, Loss: 1.1204, Train: 1.0935, Val: 1.0957\n",
      "Epoch: 060, Loss: 1.1192, Train: 1.0834, Val: 1.0857\n",
      "Epoch: 061, Loss: 1.1168, Train: 1.0737, Val: 1.0759\n",
      "Epoch: 062, Loss: 1.1156, Train: 1.0728, Val: 1.0750\n",
      "Epoch: 063, Loss: 1.1142, Train: 1.0791, Val: 1.0815\n",
      "Epoch: 064, Loss: 1.1122, Train: 1.0844, Val: 1.0869\n",
      "Epoch: 065, Loss: 1.1114, Train: 1.0795, Val: 1.0819\n",
      "Epoch: 066, Loss: 1.1095, Train: 1.0717, Val: 1.0741\n",
      "Epoch: 067, Loss: 1.1083, Train: 1.0706, Val: 1.0730\n",
      "Epoch: 068, Loss: 1.1070, Train: 1.0756, Val: 1.0781\n",
      "Epoch: 069, Loss: 1.1055, Train: 1.0751, Val: 1.0776\n",
      "Epoch: 070, Loss: 1.1044, Train: 1.0663, Val: 1.0688\n",
      "Epoch: 071, Loss: 1.1028, Train: 1.0593, Val: 1.0618\n",
      "Epoch: 072, Loss: 1.1019, Train: 1.0608, Val: 1.0633\n",
      "Epoch: 073, Loss: 1.1005, Train: 1.0637, Val: 1.0664\n",
      "Epoch: 074, Loss: 1.0996, Train: 1.0579, Val: 1.0605\n",
      "Epoch: 075, Loss: 1.0981, Train: 1.0523, Val: 1.0549\n",
      "Epoch: 076, Loss: 1.0972, Train: 1.0525, Val: 1.0551\n",
      "Epoch: 077, Loss: 1.0959, Train: 1.0543, Val: 1.0570\n",
      "Epoch: 078, Loss: 1.0950, Train: 1.0521, Val: 1.0547\n",
      "Epoch: 079, Loss: 1.0938, Train: 1.0490, Val: 1.0515\n",
      "Epoch: 080, Loss: 1.0929, Train: 1.0504, Val: 1.0530\n",
      "Epoch: 081, Loss: 1.0918, Train: 1.0522, Val: 1.0549\n",
      "Epoch: 082, Loss: 1.0910, Train: 1.0480, Val: 1.0506\n",
      "Epoch: 083, Loss: 1.0900, Train: 1.0467, Val: 1.0493\n",
      "Epoch: 084, Loss: 1.0892, Train: 1.0489, Val: 1.0515\n",
      "Epoch: 085, Loss: 1.0882, Train: 1.0493, Val: 1.0520\n",
      "Epoch: 086, Loss: 1.0873, Train: 1.0469, Val: 1.0496\n",
      "Epoch: 087, Loss: 1.0865, Train: 1.0488, Val: 1.0516\n",
      "Epoch: 088, Loss: 1.0855, Train: 1.0503, Val: 1.0532\n",
      "Epoch: 089, Loss: 1.0846, Train: 1.0480, Val: 1.0509\n",
      "Epoch: 090, Loss: 1.0837, Train: 1.0547, Val: 1.0578\n",
      "Epoch: 091, Loss: 1.0829, Train: 1.0481, Val: 1.0511\n",
      "Epoch: 092, Loss: 1.0818, Train: 1.0502, Val: 1.0533\n",
      "Epoch: 093, Loss: 1.0808, Train: 1.0520, Val: 1.0552\n",
      "Epoch: 094, Loss: 1.0798, Train: 1.0446, Val: 1.0477\n",
      "Epoch: 095, Loss: 1.0789, Train: 1.0550, Val: 1.0584\n",
      "Epoch: 096, Loss: 1.0787, Train: 1.0385, Val: 1.0415\n",
      "Epoch: 097, Loss: 1.0786, Train: 1.0613, Val: 1.0648\n",
      "Epoch: 098, Loss: 1.0799, Train: 1.0393, Val: 1.0419\n",
      "Epoch: 099, Loss: 1.0790, Train: 1.0381, Val: 1.0410\n",
      "Epoch: 100, Loss: 1.0768, Train: 1.0466, Val: 1.0499\n",
      "Epoch: 101, Loss: 1.0770, Train: 1.0375, Val: 1.0405\n",
      "Epoch: 102, Loss: 1.0762, Train: 1.0618, Val: 1.0655\n",
      "Epoch: 103, Loss: 1.0759, Train: 1.0346, Val: 1.0379\n",
      "Epoch: 104, Loss: 1.0732, Train: 1.0470, Val: 1.0509\n",
      "Epoch: 105, Loss: 1.0709, Train: 1.0360, Val: 1.0395\n",
      "Epoch: 106, Loss: 1.0677, Train: 1.0338, Val: 1.0370\n",
      "Epoch: 107, Loss: 1.0670, Train: 1.0337, Val: 1.0371\n",
      "Epoch: 108, Loss: 1.0649, Train: 1.0475, Val: 1.0514\n",
      "Epoch: 109, Loss: 1.0625, Train: 1.0348, Val: 1.0385\n",
      "Epoch: 110, Loss: 1.0591, Train: 1.0323, Val: 1.0360\n",
      "Epoch: 111, Loss: 1.0579, Train: 1.0449, Val: 1.0490\n",
      "Epoch: 112, Loss: 1.0551, Train: 1.0286, Val: 1.0325\n",
      "Epoch: 113, Loss: 1.0522, Train: 1.0415, Val: 1.0459\n",
      "Epoch: 114, Loss: 1.0512, Train: 1.0388, Val: 1.0416\n",
      "Epoch: 115, Loss: 1.0550, Train: 1.0981, Val: 1.1035\n",
      "Epoch: 116, Loss: 1.0660, Train: 1.0778, Val: 1.0796\n",
      "Epoch: 117, Loss: 1.0646, Train: 1.0500, Val: 1.0525\n",
      "Epoch: 118, Loss: 1.0571, Train: 1.0413, Val: 1.0450\n",
      "Epoch: 119, Loss: 1.0577, Train: 1.0298, Val: 1.0332\n",
      "Epoch: 120, Loss: 1.0649, Train: 1.0450, Val: 1.0489\n",
      "Epoch: 121, Loss: 1.0517, Train: 1.0379, Val: 1.0415\n",
      "Epoch: 122, Loss: 1.0489, Train: 1.0383, Val: 1.0414\n",
      "Epoch: 123, Loss: 1.0520, Train: 1.0343, Val: 1.0383\n",
      "Epoch: 124, Loss: 1.0451, Train: 1.0634, Val: 1.0682\n",
      "Epoch: 125, Loss: 1.0454, Train: 1.0393, Val: 1.0441\n",
      "Epoch: 126, Loss: 1.0425, Train: 1.0333, Val: 1.0380\n",
      "Epoch: 127, Loss: 1.0374, Train: 1.0443, Val: 1.0489\n",
      "Epoch: 128, Loss: 1.0362, Train: 1.0286, Val: 1.0331\n",
      "Epoch: 129, Loss: 1.0329, Train: 1.0228, Val: 1.0276\n",
      "Epoch: 130, Loss: 1.0311, Train: 1.0336, Val: 1.0384\n",
      "Epoch: 131, Loss: 1.0289, Train: 1.0254, Val: 1.0290\n",
      "Epoch: 132, Loss: 1.0278, Train: 1.0241, Val: 1.0279\n",
      "Epoch: 133, Loss: 1.0236, Train: 1.0258, Val: 1.0304\n",
      "Epoch: 134, Loss: 1.0234, Train: 1.0373, Val: 1.0403\n",
      "Epoch: 135, Loss: 1.0247, Train: 1.0259, Val: 1.0298\n",
      "Epoch: 136, Loss: 1.0191, Train: 1.0222, Val: 1.0262\n",
      "Epoch: 137, Loss: 1.0182, Train: 1.0526, Val: 1.0549\n",
      "Epoch: 138, Loss: 1.0241, Train: 1.0197, Val: 1.0232\n",
      "Epoch: 139, Loss: 1.0220, Train: 1.0164, Val: 1.0200\n",
      "Epoch: 140, Loss: 1.0149, Train: 1.0259, Val: 1.0289\n",
      "Epoch: 141, Loss: 1.0175, Train: 1.0187, Val: 1.0229\n",
      "Epoch: 142, Loss: 1.0174, Train: 1.0291, Val: 1.0318\n",
      "Epoch: 143, Loss: 1.0202, Train: 1.0073, Val: 1.0118\n",
      "Epoch: 144, Loss: 1.0133, Train: 1.0080, Val: 1.0129\n",
      "Epoch: 145, Loss: 1.0139, Train: 1.0235, Val: 1.0268\n",
      "Epoch: 146, Loss: 1.0154, Train: 1.0168, Val: 1.0216\n",
      "Epoch: 147, Loss: 1.0069, Train: 1.0359, Val: 1.0420\n",
      "Epoch: 148, Loss: 1.0150, Train: 1.0073, Val: 1.0117\n",
      "Epoch: 149, Loss: 1.0101, Train: 1.0044, Val: 1.0089\n",
      "Epoch: 150, Loss: 1.0084, Train: 1.0477, Val: 1.0536\n",
      "Epoch: 151, Loss: 1.0068, Train: 1.0298, Val: 1.0359\n",
      "Epoch: 152, Loss: 0.9990, Train: 1.0045, Val: 1.0101\n",
      "Epoch: 153, Loss: 1.0063, Train: 1.0001, Val: 1.0055\n",
      "Epoch: 154, Loss: 1.0010, Train: 1.1072, Val: 1.1137\n",
      "Epoch: 155, Loss: 1.0079, Train: 1.0269, Val: 1.0316\n",
      "Epoch: 156, Loss: 1.0224, Train: 1.0090, Val: 1.0141\n",
      "Epoch: 157, Loss: 1.0129, Train: 1.0459, Val: 1.0522\n",
      "Epoch: 158, Loss: 1.0099, Train: 1.0471, Val: 1.0536\n",
      "Epoch: 159, Loss: 1.0040, Train: 1.0203, Val: 1.0265\n",
      "Epoch: 160, Loss: 1.0104, Train: 1.0777, Val: 1.0841\n",
      "Epoch: 161, Loss: 1.0024, Train: 1.0483, Val: 1.0546\n",
      "Epoch: 162, Loss: 1.0036, Train: 1.0002, Val: 1.0057\n",
      "Epoch: 163, Loss: 0.9984, Train: 1.0013, Val: 1.0069\n",
      "Epoch: 164, Loss: 0.9960, Train: 1.0224, Val: 1.0289\n",
      "Epoch: 165, Loss: 0.9999, Train: 1.0001, Val: 1.0052\n",
      "Epoch: 166, Loss: 0.9996, Train: 1.0590, Val: 1.0658\n",
      "Epoch: 167, Loss: 0.9900, Train: 1.1020, Val: 1.1088\n",
      "Epoch: 168, Loss: 0.9949, Train: 1.0022, Val: 1.0083\n",
      "Epoch: 169, Loss: 0.9937, Train: 1.0190, Val: 1.0254\n",
      "Epoch: 170, Loss: 0.9901, Train: 1.0897, Val: 1.0963\n",
      "Epoch: 171, Loss: 0.9873, Train: 1.0331, Val: 1.0394\n",
      "Epoch: 172, Loss: 0.9824, Train: 1.0315, Val: 1.0382\n",
      "Epoch: 173, Loss: 0.9853, Train: 1.0616, Val: 1.0682\n",
      "Epoch: 174, Loss: 0.9778, Train: 1.0852, Val: 1.0918\n",
      "Epoch: 175, Loss: 0.9808, Train: 1.0406, Val: 1.0473\n",
      "Epoch: 176, Loss: 0.9779, Train: 1.0081, Val: 1.0141\n",
      "Epoch: 177, Loss: 0.9783, Train: 1.1179, Val: 1.1246\n",
      "Epoch: 178, Loss: 0.9776, Train: 1.0295, Val: 1.0359\n",
      "Epoch: 179, Loss: 0.9749, Train: 1.0568, Val: 1.0640\n",
      "Epoch: 180, Loss: 0.9731, Train: 1.0494, Val: 1.0561\n",
      "Epoch: 181, Loss: 0.9697, Train: 1.0890, Val: 1.0959\n",
      "Epoch: 182, Loss: 0.9684, Train: 1.0455, Val: 1.0530\n",
      "Epoch: 183, Loss: 0.9672, Train: 1.0379, Val: 1.0450\n",
      "Epoch: 184, Loss: 0.9656, Train: 1.1141, Val: 1.1211\n",
      "Epoch: 185, Loss: 0.9677, Train: 0.9972, Val: 1.0041\n",
      "Epoch: 186, Loss: 0.9705, Train: 1.1058, Val: 1.1138\n",
      "Epoch: 187, Loss: 0.9740, Train: 1.0337, Val: 1.0405\n",
      "Epoch: 188, Loss: 0.9958, Train: 1.0144, Val: 1.0214\n",
      "Epoch: 189, Loss: 0.9909, Train: 1.0227, Val: 1.0298\n",
      "Epoch: 190, Loss: 0.9846, Train: 1.0345, Val: 1.0415\n",
      "Epoch: 191, Loss: 0.9755, Train: 1.0554, Val: 1.0627\n",
      "Epoch: 192, Loss: 0.9767, Train: 1.1071, Val: 1.1146\n",
      "Epoch: 193, Loss: 0.9836, Train: 1.0159, Val: 1.0230\n",
      "Epoch: 194, Loss: 0.9799, Train: 1.0493, Val: 1.0565\n",
      "Epoch: 195, Loss: 0.9748, Train: 0.9979, Val: 1.0044\n",
      "Epoch: 196, Loss: 0.9727, Train: 1.0122, Val: 1.0192\n",
      "Epoch: 197, Loss: 0.9734, Train: 0.9990, Val: 1.0058\n",
      "Epoch: 198, Loss: 0.9701, Train: 1.0853, Val: 1.0930\n",
      "Epoch: 199, Loss: 0.9660, Train: 1.0146, Val: 1.0219\n",
      "Epoch: 200, Loss: 0.9691, Train: 1.0952, Val: 1.1031\n",
      "Epoch: 201, Loss: 0.9682, Train: 1.0056, Val: 1.0131\n",
      "Epoch: 202, Loss: 0.9649, Train: 0.9971, Val: 1.0043\n",
      "Epoch: 203, Loss: 0.9596, Train: 1.0216, Val: 1.0296\n",
      "Epoch: 204, Loss: 0.9601, Train: 0.9877, Val: 0.9952\n",
      "Epoch: 205, Loss: 0.9609, Train: 1.0434, Val: 1.0521\n",
      "Epoch: 206, Loss: 0.9569, Train: 0.9914, Val: 0.9990\n",
      "Epoch: 207, Loss: 0.9540, Train: 0.9886, Val: 0.9961\n",
      "Epoch: 208, Loss: 0.9545, Train: 0.9975, Val: 1.0047\n",
      "Epoch: 209, Loss: 0.9553, Train: 0.9806, Val: 0.9879\n",
      "Epoch: 210, Loss: 0.9512, Train: 0.9919, Val: 0.9995\n",
      "Epoch: 211, Loss: 0.9482, Train: 0.9912, Val: 0.9982\n",
      "Epoch: 212, Loss: 0.9498, Train: 0.9758, Val: 0.9835\n",
      "Epoch: 213, Loss: 0.9508, Train: 0.9931, Val: 0.9998\n",
      "Epoch: 214, Loss: 0.9531, Train: 0.9882, Val: 0.9963\n",
      "Epoch: 215, Loss: 0.9513, Train: 0.9860, Val: 0.9925\n",
      "Epoch: 216, Loss: 0.9576, Train: 0.9824, Val: 0.9907\n",
      "Epoch: 217, Loss: 0.9467, Train: 0.9903, Val: 0.9985\n",
      "Epoch: 218, Loss: 0.9454, Train: 0.9813, Val: 0.9893\n",
      "Epoch: 219, Loss: 0.9494, Train: 0.9828, Val: 0.9911\n",
      "Epoch: 220, Loss: 0.9455, Train: 0.9823, Val: 0.9908\n",
      "Epoch: 221, Loss: 0.9408, Train: 0.9931, Val: 1.0020\n",
      "Epoch: 222, Loss: 0.9468, Train: 0.9808, Val: 0.9889\n",
      "Epoch: 223, Loss: 0.9445, Train: 0.9891, Val: 0.9985\n",
      "Epoch: 224, Loss: 0.9440, Train: 0.9779, Val: 0.9858\n",
      "Epoch: 225, Loss: 0.9458, Train: 0.9883, Val: 0.9972\n",
      "Epoch: 226, Loss: 0.9435, Train: 0.9916, Val: 0.9979\n",
      "Epoch: 227, Loss: 0.9479, Train: 0.9831, Val: 0.9919\n",
      "Epoch: 228, Loss: 0.9386, Train: 0.9854, Val: 0.9943\n",
      "Epoch: 229, Loss: 0.9519, Train: 0.9900, Val: 0.9970\n",
      "Epoch: 230, Loss: 0.9468, Train: 1.0014, Val: 1.0097\n",
      "Epoch: 231, Loss: 0.9552, Train: 1.0003, Val: 1.0064\n",
      "Epoch: 232, Loss: 0.9545, Train: 1.0899, Val: 1.0994\n",
      "Epoch: 233, Loss: 0.9575, Train: 0.9899, Val: 0.9970\n",
      "Epoch: 234, Loss: 0.9566, Train: 0.9845, Val: 0.9931\n",
      "Epoch: 235, Loss: 0.9530, Train: 0.9830, Val: 0.9918\n",
      "Epoch: 236, Loss: 0.9433, Train: 0.9792, Val: 0.9870\n",
      "Epoch: 237, Loss: 0.9559, Train: 1.0697, Val: 1.0797\n",
      "Epoch: 238, Loss: 0.9430, Train: 1.0029, Val: 1.0129\n",
      "Epoch: 239, Loss: 0.9394, Train: 0.9857, Val: 0.9946\n",
      "Epoch: 240, Loss: 0.9446, Train: 1.0053, Val: 1.0146\n",
      "Epoch: 241, Loss: 0.9362, Train: 0.9936, Val: 1.0032\n",
      "Epoch: 242, Loss: 0.9376, Train: 0.9924, Val: 1.0024\n",
      "Epoch: 243, Loss: 0.9359, Train: 1.0115, Val: 1.0219\n",
      "Epoch: 244, Loss: 0.9317, Train: 0.9780, Val: 0.9874\n",
      "Epoch: 245, Loss: 0.9329, Train: 0.9884, Val: 0.9980\n",
      "Epoch: 246, Loss: 0.9307, Train: 1.0022, Val: 1.0127\n",
      "Epoch: 247, Loss: 0.9297, Train: 0.9741, Val: 0.9836\n",
      "Epoch: 248, Loss: 0.9322, Train: 1.0245, Val: 1.0348\n",
      "Epoch: 249, Loss: 0.9269, Train: 0.9675, Val: 0.9766\n",
      "Epoch: 250, Loss: 0.9268, Train: 0.9754, Val: 0.9849\n",
      "Epoch: 251, Loss: 0.9263, Train: 1.0169, Val: 1.0266\n",
      "Epoch: 252, Loss: 0.9293, Train: 0.9669, Val: 0.9748\n",
      "Epoch: 253, Loss: 0.9341, Train: 0.9777, Val: 0.9861\n",
      "Epoch: 254, Loss: 0.9322, Train: 0.9687, Val: 0.9774\n",
      "Epoch: 255, Loss: 0.9216, Train: 0.9650, Val: 0.9734\n",
      "Epoch: 256, Loss: 0.9327, Train: 1.0077, Val: 1.0163\n",
      "Epoch: 257, Loss: 0.9339, Train: 0.9753, Val: 0.9822\n",
      "Epoch: 258, Loss: 0.9312, Train: 0.9733, Val: 0.9802\n",
      "Epoch: 259, Loss: 0.9288, Train: 0.9906, Val: 0.9993\n",
      "Epoch: 260, Loss: 0.9231, Train: 0.9812, Val: 0.9895\n",
      "Epoch: 261, Loss: 0.9268, Train: 0.9657, Val: 0.9744\n",
      "Epoch: 262, Loss: 0.9256, Train: 0.9714, Val: 0.9784\n",
      "Epoch: 263, Loss: 0.9200, Train: 0.9670, Val: 0.9750\n",
      "Epoch: 264, Loss: 0.9220, Train: 0.9838, Val: 0.9913\n",
      "Epoch: 265, Loss: 0.9242, Train: 0.9662, Val: 0.9753\n",
      "Epoch: 266, Loss: 0.9156, Train: 0.9731, Val: 0.9800\n",
      "Epoch: 267, Loss: 0.9222, Train: 0.9633, Val: 0.9716\n",
      "Epoch: 268, Loss: 0.9162, Train: 0.9656, Val: 0.9737\n",
      "Epoch: 269, Loss: 0.9137, Train: 1.0004, Val: 1.0078\n",
      "Epoch: 270, Loss: 0.9214, Train: 0.9629, Val: 0.9709\n",
      "Epoch: 271, Loss: 0.9204, Train: 0.9774, Val: 0.9834\n",
      "Epoch: 272, Loss: 0.9171, Train: 1.0117, Val: 1.0205\n",
      "Epoch: 273, Loss: 0.9185, Train: 0.9755, Val: 0.9840\n",
      "Epoch: 274, Loss: 0.9135, Train: 0.9638, Val: 0.9718\n",
      "Epoch: 275, Loss: 0.9101, Train: 0.9934, Val: 1.0023\n",
      "Epoch: 276, Loss: 0.9158, Train: 0.9703, Val: 0.9787\n",
      "Epoch: 277, Loss: 0.9119, Train: 1.0242, Val: 1.0344\n",
      "Epoch: 278, Loss: 0.9117, Train: 0.9787, Val: 0.9876\n",
      "Epoch: 279, Loss: 0.9082, Train: 0.9784, Val: 0.9876\n",
      "Epoch: 280, Loss: 0.9073, Train: 1.0061, Val: 1.0159\n",
      "Epoch: 281, Loss: 0.9049, Train: 1.0197, Val: 1.0290\n",
      "Epoch: 282, Loss: 0.9084, Train: 1.0231, Val: 1.0328\n",
      "Epoch: 283, Loss: 0.9057, Train: 0.9759, Val: 0.9848\n",
      "Epoch: 284, Loss: 0.9052, Train: 1.0463, Val: 1.0561\n",
      "Epoch: 285, Loss: 0.9018, Train: 1.0262, Val: 1.0360\n",
      "Epoch: 286, Loss: 0.9045, Train: 1.0201, Val: 1.0294\n",
      "Epoch: 287, Loss: 0.9053, Train: 1.0241, Val: 1.0336\n",
      "Epoch: 288, Loss: 0.9043, Train: 1.0220, Val: 1.0316\n",
      "Epoch: 289, Loss: 0.9048, Train: 1.0483, Val: 1.0574\n",
      "Epoch: 290, Loss: 0.9036, Train: 0.9929, Val: 1.0020\n",
      "Epoch: 291, Loss: 0.9056, Train: 1.0246, Val: 1.0334\n",
      "Epoch: 292, Loss: 0.8957, Train: 1.0657, Val: 1.0752\n",
      "Epoch: 293, Loss: 0.8989, Train: 0.9851, Val: 0.9940\n",
      "Epoch: 294, Loss: 0.9052, Train: 1.0480, Val: 1.0571\n",
      "Epoch: 295, Loss: 0.9010, Train: 0.9967, Val: 1.0052\n",
      "Epoch: 296, Loss: 0.8980, Train: 1.0350, Val: 1.0449\n",
      "Epoch: 297, Loss: 0.8993, Train: 0.9629, Val: 0.9701\n",
      "Epoch: 298, Loss: 0.9108, Train: 1.0191, Val: 1.0282\n",
      "Epoch: 299, Loss: 0.9120, Train: 0.9979, Val: 1.0067\n",
      "Epoch: 300, Loss: 0.9106, Train: 0.9911, Val: 1.0000\n",
      "Epoch: 301, Loss: 0.9072, Train: 0.9749, Val: 0.9825\n",
      "Epoch: 302, Loss: 0.9063, Train: 0.9598, Val: 0.9674\n",
      "Epoch: 303, Loss: 0.9069, Train: 0.9758, Val: 0.9848\n",
      "Epoch: 304, Loss: 0.9022, Train: 0.9733, Val: 0.9802\n",
      "Epoch: 305, Loss: 0.9178, Train: 0.9667, Val: 0.9736\n",
      "Epoch: 306, Loss: 0.9070, Train: 0.9816, Val: 0.9882\n",
      "Epoch: 307, Loss: 0.9062, Train: 0.9936, Val: 0.9990\n",
      "Epoch: 308, Loss: 0.9053, Train: 0.9807, Val: 0.9878\n",
      "Epoch: 309, Loss: 0.9089, Train: 1.0159, Val: 1.0207\n",
      "Epoch: 310, Loss: 0.9088, Train: 0.9920, Val: 0.9986\n",
      "Epoch: 311, Loss: 0.9121, Train: 0.9613, Val: 0.9676\n",
      "Epoch: 312, Loss: 0.8984, Train: 1.0062, Val: 1.0141\n",
      "Epoch: 313, Loss: 0.9149, Train: 0.9699, Val: 0.9763\n",
      "Epoch: 314, Loss: 0.8963, Train: 0.9658, Val: 0.9728\n",
      "Epoch: 315, Loss: 0.9072, Train: 0.9570, Val: 0.9644\n",
      "Epoch: 316, Loss: 0.8948, Train: 0.9528, Val: 0.9615\n",
      "Epoch: 317, Loss: 0.8955, Train: 0.9904, Val: 0.9996\n",
      "Epoch: 318, Loss: 0.8995, Train: 0.9624, Val: 0.9710\n",
      "Epoch: 319, Loss: 0.8907, Train: 0.9691, Val: 0.9783\n",
      "Epoch: 320, Loss: 0.8943, Train: 0.9620, Val: 0.9704\n",
      "Epoch: 321, Loss: 0.8914, Train: 1.0104, Val: 1.0194\n",
      "Epoch: 322, Loss: 0.8870, Train: 0.9917, Val: 0.9999\n",
      "Epoch: 323, Loss: 0.8898, Train: 1.0075, Val: 1.0158\n",
      "Epoch: 324, Loss: 0.8869, Train: 0.9708, Val: 0.9788\n",
      "Epoch: 325, Loss: 0.8843, Train: 0.9695, Val: 0.9772\n",
      "Epoch: 326, Loss: 0.8845, Train: 1.0204, Val: 1.0289\n",
      "Epoch: 327, Loss: 0.8845, Train: 1.0239, Val: 1.0309\n",
      "Epoch: 328, Loss: 0.8873, Train: 1.0658, Val: 1.0746\n",
      "Epoch: 329, Loss: 0.8896, Train: 0.9933, Val: 0.9995\n",
      "Epoch: 330, Loss: 0.8971, Train: 1.0080, Val: 1.0169\n",
      "Epoch: 331, Loss: 0.8945, Train: 1.0146, Val: 1.0230\n",
      "Epoch: 332, Loss: 0.8934, Train: 1.0260, Val: 1.0343\n",
      "Epoch: 333, Loss: 0.8962, Train: 1.0271, Val: 1.0363\n",
      "Epoch: 334, Loss: 0.8847, Train: 0.9733, Val: 0.9827\n",
      "Epoch: 335, Loss: 0.8788, Train: 0.9866, Val: 0.9961\n",
      "Epoch: 336, Loss: 0.8852, Train: 1.0210, Val: 1.0305\n",
      "Epoch: 337, Loss: 0.8822, Train: 1.0256, Val: 1.0350\n",
      "Epoch: 338, Loss: 0.8769, Train: 1.0734, Val: 1.0824\n",
      "Epoch: 339, Loss: 0.8763, Train: 1.0505, Val: 1.0594\n",
      "Epoch: 340, Loss: 0.8785, Train: 1.0554, Val: 1.0640\n",
      "Epoch: 341, Loss: 0.8781, Train: 1.0525, Val: 1.0609\n",
      "Epoch: 342, Loss: 0.8729, Train: 1.0753, Val: 1.0833\n",
      "Epoch: 343, Loss: 0.8723, Train: 1.0561, Val: 1.0639\n",
      "Epoch: 344, Loss: 0.8745, Train: 1.0613, Val: 1.0687\n",
      "Epoch: 345, Loss: 0.8741, Train: 1.0749, Val: 1.0818\n",
      "Epoch: 346, Loss: 0.8731, Train: 1.0645, Val: 1.0717\n",
      "Epoch: 347, Loss: 0.8690, Train: 1.0692, Val: 1.0766\n",
      "Epoch: 348, Loss: 0.8691, Train: 1.0346, Val: 1.0418\n",
      "Epoch: 349, Loss: 0.8708, Train: 1.0850, Val: 1.0922\n",
      "Epoch: 350, Loss: 0.8733, Train: 1.0220, Val: 1.0283\n",
      "Epoch: 351, Loss: 0.8872, Train: 1.1141, Val: 1.1214\n",
      "Epoch: 352, Loss: 0.8872, Train: 1.0711, Val: 1.0756\n",
      "Epoch: 353, Loss: 0.9086, Train: 1.1755, Val: 1.1810\n",
      "Epoch: 354, Loss: 0.9017, Train: 1.0618, Val: 1.0686\n",
      "Epoch: 355, Loss: 0.9022, Train: 1.0601, Val: 1.0668\n",
      "Epoch: 356, Loss: 0.8935, Train: 1.1452, Val: 1.1519\n",
      "Epoch: 357, Loss: 0.8876, Train: 1.1595, Val: 1.1654\n",
      "Epoch: 358, Loss: 0.9010, Train: 1.1031, Val: 1.1104\n",
      "Epoch: 359, Loss: 0.8978, Train: 1.1113, Val: 1.1198\n",
      "Epoch: 360, Loss: 0.8872, Train: 1.1427, Val: 1.1500\n",
      "Epoch: 361, Loss: 0.8870, Train: 1.1695, Val: 1.1755\n",
      "Epoch: 362, Loss: 0.8882, Train: 1.1551, Val: 1.1631\n",
      "Epoch: 363, Loss: 0.8842, Train: 1.1127, Val: 1.1199\n",
      "Epoch: 364, Loss: 0.8802, Train: 1.1749, Val: 1.1808\n",
      "Epoch: 365, Loss: 0.8842, Train: 1.1901, Val: 1.1967\n",
      "Epoch: 366, Loss: 0.8723, Train: 1.1156, Val: 1.1226\n",
      "Epoch: 367, Loss: 0.8805, Train: 1.1138, Val: 1.1213\n",
      "Epoch: 368, Loss: 0.8705, Train: 1.0942, Val: 1.1021\n",
      "Epoch: 369, Loss: 0.8728, Train: 1.0491, Val: 1.0558\n",
      "Epoch: 370, Loss: 0.8724, Train: 1.1049, Val: 1.1121\n",
      "Epoch: 371, Loss: 0.8696, Train: 1.1305, Val: 1.1373\n",
      "Epoch: 372, Loss: 0.8679, Train: 1.0748, Val: 1.0818\n",
      "Epoch: 373, Loss: 0.8674, Train: 1.0581, Val: 1.0665\n",
      "Epoch: 374, Loss: 0.8668, Train: 1.0355, Val: 1.0430\n",
      "Epoch: 375, Loss: 0.8654, Train: 1.0841, Val: 1.0919\n",
      "Epoch: 376, Loss: 0.8628, Train: 1.0944, Val: 1.1030\n",
      "Epoch: 377, Loss: 0.8637, Train: 1.0065, Val: 1.0151\n",
      "Epoch: 378, Loss: 0.8629, Train: 1.0063, Val: 1.0160\n",
      "Epoch: 379, Loss: 0.8623, Train: 0.9876, Val: 0.9968\n",
      "Epoch: 380, Loss: 0.8603, Train: 0.9987, Val: 1.0077\n",
      "Epoch: 381, Loss: 0.8594, Train: 1.0147, Val: 1.0243\n",
      "Epoch: 382, Loss: 0.8596, Train: 0.9539, Val: 0.9633\n",
      "Epoch: 383, Loss: 0.8587, Train: 0.9679, Val: 0.9781\n",
      "Epoch: 384, Loss: 0.8580, Train: 0.9840, Val: 0.9940\n",
      "Epoch: 385, Loss: 0.8560, Train: 0.9984, Val: 1.0084\n",
      "Epoch: 386, Loss: 0.8564, Train: 0.9665, Val: 0.9770\n",
      "Epoch: 387, Loss: 0.8553, Train: 0.9523, Val: 0.9622\n",
      "Epoch: 388, Loss: 0.8552, Train: 0.9844, Val: 0.9945\n",
      "Epoch: 389, Loss: 0.8535, Train: 1.0135, Val: 1.0229\n",
      "Epoch: 390, Loss: 0.8537, Train: 1.0278, Val: 1.0370\n",
      "Epoch: 391, Loss: 0.8525, Train: 1.0112, Val: 1.0207\n",
      "Epoch: 392, Loss: 0.8530, Train: 0.9902, Val: 0.9996\n",
      "Epoch: 393, Loss: 0.8543, Train: 1.0525, Val: 1.0613\n",
      "Epoch: 394, Loss: 0.8613, Train: 0.9871, Val: 0.9953\n",
      "Epoch: 395, Loss: 0.8669, Train: 0.9747, Val: 0.9846\n",
      "Epoch: 396, Loss: 0.8701, Train: 0.9518, Val: 0.9593\n",
      "Epoch: 397, Loss: 0.8816, Train: 0.9810, Val: 0.9884\n",
      "Epoch: 398, Loss: 0.8705, Train: 0.9692, Val: 0.9781\n",
      "Epoch: 399, Loss: 0.8711, Train: 1.0710, Val: 1.0752\n",
      "Epoch: 400, Loss: 0.8672, Train: 1.0344, Val: 1.0401\n",
      "Epoch: 401, Loss: 0.8582, Train: 1.0016, Val: 1.0093\n",
      "Epoch: 402, Loss: 0.8736, Train: 1.0633, Val: 1.0678\n",
      "Epoch: 403, Loss: 0.8704, Train: 1.0391, Val: 1.0443\n",
      "Epoch: 404, Loss: 0.8603, Train: 1.0053, Val: 1.0130\n",
      "Epoch: 405, Loss: 0.8689, Train: 1.0528, Val: 1.0582\n",
      "Epoch: 406, Loss: 0.8673, Train: 1.0266, Val: 1.0315\n",
      "Epoch: 407, Loss: 0.8739, Train: 0.9985, Val: 1.0055\n",
      "Epoch: 408, Loss: 0.8558, Train: 1.0344, Val: 1.0412\n",
      "Epoch: 409, Loss: 0.8717, Train: 0.9791, Val: 0.9856\n",
      "Epoch: 410, Loss: 0.8657, Train: 1.0083, Val: 1.0137\n",
      "Epoch: 411, Loss: 0.8666, Train: 1.0022, Val: 1.0096\n",
      "Epoch: 412, Loss: 0.8554, Train: 1.0072, Val: 1.0149\n",
      "Epoch: 413, Loss: 0.8590, Train: 0.9977, Val: 1.0035\n",
      "Epoch: 414, Loss: 0.8577, Train: 0.9769, Val: 0.9833\n",
      "Epoch: 415, Loss: 0.8592, Train: 1.0074, Val: 1.0157\n",
      "Epoch: 416, Loss: 0.8632, Train: 1.0216, Val: 1.0290\n",
      "Epoch: 417, Loss: 0.8665, Train: 0.9801, Val: 0.9872\n",
      "Epoch: 418, Loss: 0.8637, Train: 0.9676, Val: 0.9753\n",
      "Epoch: 419, Loss: 0.8573, Train: 1.0094, Val: 1.0175\n",
      "Epoch: 420, Loss: 0.8624, Train: 0.9961, Val: 1.0039\n",
      "Epoch: 421, Loss: 0.8529, Train: 0.9775, Val: 0.9848\n",
      "Epoch: 422, Loss: 0.8548, Train: 0.9796, Val: 0.9872\n",
      "Epoch: 423, Loss: 0.8543, Train: 0.9943, Val: 1.0027\n",
      "Epoch: 424, Loss: 0.8493, Train: 1.0027, Val: 1.0110\n",
      "Epoch: 425, Loss: 0.8514, Train: 0.9674, Val: 0.9754\n",
      "Epoch: 426, Loss: 0.8491, Train: 0.9691, Val: 0.9774\n",
      "Epoch: 427, Loss: 0.8483, Train: 0.9923, Val: 1.0005\n",
      "Epoch: 428, Loss: 0.8472, Train: 0.9866, Val: 0.9946\n",
      "Epoch: 429, Loss: 0.8464, Train: 0.9523, Val: 0.9611\n",
      "Epoch: 430, Loss: 0.8457, Train: 0.9613, Val: 0.9700\n",
      "Epoch: 431, Loss: 0.8437, Train: 0.9964, Val: 1.0043\n",
      "Epoch: 432, Loss: 0.8447, Train: 0.9819, Val: 0.9899\n",
      "Epoch: 433, Loss: 0.8420, Train: 0.9655, Val: 0.9736\n",
      "Epoch: 434, Loss: 0.8419, Train: 0.9717, Val: 0.9799\n",
      "Epoch: 435, Loss: 0.8418, Train: 0.9718, Val: 0.9801\n",
      "Epoch: 436, Loss: 0.8403, Train: 0.9787, Val: 0.9865\n",
      "Epoch: 437, Loss: 0.8406, Train: 0.9743, Val: 0.9822\n",
      "Epoch: 438, Loss: 0.8392, Train: 0.9574, Val: 0.9664\n",
      "Epoch: 439, Loss: 0.8404, Train: 0.9907, Val: 0.9985\n",
      "Epoch: 440, Loss: 0.8400, Train: 0.9572, Val: 0.9658\n",
      "Epoch: 441, Loss: 0.8419, Train: 0.9660, Val: 0.9747\n",
      "Epoch: 442, Loss: 0.8447, Train: 0.9494, Val: 0.9590\n",
      "Epoch: 443, Loss: 0.8545, Train: 0.9930, Val: 1.0009\n",
      "Epoch: 444, Loss: 0.8715, Train: 0.9664, Val: 0.9748\n",
      "Epoch: 445, Loss: 0.8787, Train: 0.9539, Val: 0.9631\n",
      "Epoch: 446, Loss: 0.8522, Train: 0.9485, Val: 0.9586\n",
      "Epoch: 447, Loss: 0.8487, Train: 0.9534, Val: 0.9626\n",
      "Epoch: 448, Loss: 0.8543, Train: 0.9479, Val: 0.9575\n",
      "Epoch: 449, Loss: 0.8526, Train: 0.9675, Val: 0.9773\n",
      "Epoch: 450, Loss: 0.8437, Train: 0.9703, Val: 0.9797\n",
      "Epoch: 451, Loss: 0.8474, Train: 0.9263, Val: 0.9372\n",
      "Epoch: 452, Loss: 0.8458, Train: 0.9617, Val: 0.9732\n",
      "Epoch: 453, Loss: 0.8424, Train: 0.9805, Val: 0.9901\n",
      "Epoch: 454, Loss: 0.8451, Train: 0.9324, Val: 0.9423\n",
      "Epoch: 455, Loss: 0.8429, Train: 0.9347, Val: 0.9458\n",
      "Epoch: 456, Loss: 0.8389, Train: 0.9809, Val: 0.9916\n",
      "Epoch: 457, Loss: 0.8433, Train: 0.9354, Val: 0.9449\n",
      "Epoch: 458, Loss: 0.8419, Train: 0.9358, Val: 0.9452\n",
      "Epoch: 459, Loss: 0.8392, Train: 0.9774, Val: 0.9881\n",
      "Epoch: 460, Loss: 0.8411, Train: 0.9330, Val: 0.9437\n",
      "Epoch: 461, Loss: 0.8356, Train: 0.9357, Val: 0.9445\n",
      "Epoch: 462, Loss: 0.8372, Train: 0.9428, Val: 0.9525\n",
      "Epoch: 463, Loss: 0.8369, Train: 0.9340, Val: 0.9448\n",
      "Epoch: 464, Loss: 0.8345, Train: 0.9272, Val: 0.9367\n",
      "Epoch: 465, Loss: 0.8363, Train: 0.9516, Val: 0.9598\n",
      "Epoch: 466, Loss: 0.8339, Train: 0.9328, Val: 0.9429\n",
      "Epoch: 467, Loss: 0.8339, Train: 0.9313, Val: 0.9408\n",
      "Epoch: 468, Loss: 0.8309, Train: 0.9495, Val: 0.9579\n",
      "Epoch: 469, Loss: 0.8327, Train: 0.9268, Val: 0.9366\n",
      "Epoch: 470, Loss: 0.8308, Train: 0.9367, Val: 0.9463\n",
      "Epoch: 471, Loss: 0.8312, Train: 0.9410, Val: 0.9498\n",
      "Epoch: 472, Loss: 0.8311, Train: 0.9335, Val: 0.9430\n",
      "Epoch: 473, Loss: 0.8299, Train: 0.9237, Val: 0.9341\n",
      "Epoch: 474, Loss: 0.8307, Train: 0.9657, Val: 0.9736\n",
      "Epoch: 475, Loss: 0.8320, Train: 0.9308, Val: 0.9401\n",
      "Epoch: 476, Loss: 0.8311, Train: 0.9378, Val: 0.9477\n",
      "Epoch: 477, Loss: 0.8336, Train: 0.9374, Val: 0.9460\n",
      "Epoch: 478, Loss: 0.8358, Train: 0.9992, Val: 1.0066\n",
      "Epoch: 479, Loss: 0.8379, Train: 0.9195, Val: 0.9301\n",
      "Epoch: 480, Loss: 0.8360, Train: 0.9931, Val: 1.0011\n",
      "Epoch: 481, Loss: 0.8361, Train: 0.9572, Val: 0.9655\n",
      "Epoch: 482, Loss: 0.8315, Train: 0.9470, Val: 0.9565\n",
      "Epoch: 483, Loss: 0.8279, Train: 0.9530, Val: 0.9623\n",
      "Epoch: 484, Loss: 0.8283, Train: 0.9400, Val: 0.9490\n",
      "Epoch: 485, Loss: 0.8302, Train: 0.9612, Val: 0.9701\n",
      "Epoch: 486, Loss: 0.8309, Train: 0.9339, Val: 0.9439\n",
      "Epoch: 487, Loss: 0.8326, Train: 0.9786, Val: 0.9870\n",
      "Epoch: 488, Loss: 0.8325, Train: 0.9389, Val: 0.9482\n",
      "Epoch: 489, Loss: 0.8279, Train: 0.9325, Val: 0.9428\n",
      "Epoch: 490, Loss: 0.8273, Train: 0.9463, Val: 0.9559\n",
      "Epoch: 491, Loss: 0.8290, Train: 0.9522, Val: 0.9613\n",
      "Epoch: 492, Loss: 0.8362, Train: 0.9482, Val: 0.9580\n",
      "Epoch: 493, Loss: 0.8269, Train: 0.9346, Val: 0.9449\n",
      "Epoch: 494, Loss: 0.8298, Train: 0.9602, Val: 0.9688\n",
      "Epoch: 495, Loss: 0.8384, Train: 0.9726, Val: 0.9805\n",
      "Epoch: 496, Loss: 0.8279, Train: 0.9336, Val: 0.9442\n",
      "Epoch: 497, Loss: 0.8328, Train: 0.9700, Val: 0.9781\n",
      "Epoch: 498, Loss: 0.8345, Train: 0.9676, Val: 0.9754\n",
      "Epoch: 499, Loss: 0.8285, Train: 0.9317, Val: 0.9423\n",
      "Epoch: 500, Loss: 0.8340, Train: 0.9427, Val: 0.9524\n",
      "Test RMSE: 0.9662\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.703203       3.583673\n",
      "std      1727.484387     741.673176       0.586542       1.116938\n",
      "min         0.000000       0.000000       0.782680       1.000000\n",
      "25%      1500.000000     259.000000       3.362849       3.000000\n",
      "50%      3066.000000     693.000000       3.768409       4.000000\n",
      "75%      4472.000000    1292.000000       4.114955       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1039.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.29860761617178\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## KPGraphSAGE\n",
    "args.model_name = \"KPGraphSAGE\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for k, kernel in variations:\n",
    "    args.K = k\n",
    "    args.kernel = kernel\n",
    "    model = Model(hidden_channels=32).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    def train(loss_type=\"mse\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        splitted_data = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        edge_types=[('user', 'rates', 'movie')],\n",
    "        rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    "    )(data)\n",
    "        pred = model(train_data[\"user\", \"rates\", \"movie\"], train_data['user', 'movie'].edge_label_index)\n",
    "        target = train_data['user', 'movie'].edge_label\n",
    "        if loss_type ==\"mse\":\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        elif loss_type == \"BPR\":\n",
    "            loss = F.mse_loss(pred, target)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return float(loss)\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test(data):\n",
    "        data = data.to(device)\n",
    "        model.eval()\n",
    "        pred = model(data[\"user\", \"rates\", \"movie\"],\n",
    "                     data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        return float(rmse)\n",
    "    \n",
    "    # ep = 500\n",
    "    # for i in [1]:\n",
    "    #     model = get_model__()\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        train_data = train_data.to(device)\n",
    "        loss = train()\n",
    "        train_rmse = test(train_data)\n",
    "        val_rmse = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "              f'Val: {val_rmse:.4f}')\n",
    "    with torch.no_grad():\n",
    "        test_data = test_data.to(device)\n",
    "        pred = model(test_data[\"user\", \"rates\", \"movie\"],\n",
    "                     test_data['user', 'movie'].edge_label_index)\n",
    "        \n",
    "        # pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
    "        #              test_data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = test_data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "    userId = test_data['user', 'movie'].edge_label_index[0].cpu().numpy()\n",
    "    movieId = test_data['user', 'movie'].edge_label_index[1].cpu().numpy()\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "    predicted_df = pd.DataFrame({'userId': userId, 'movieId': movieId, 'rating': pred, 'target': target})\n",
    "    \n",
    "    print(predicted_df.describe())\n",
    "    \n",
    "    print(hit_rate_top_k(k=10, predicted_df=predicted_df))\n",
    "\n",
    "    print(\"-------------------------------------------------\"*10)\n",
    "    print(\"-------------------------------------------------\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "K:  1\n",
      "Kernel:  gd\n",
      "Model Name:  KPGINPlus\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 13.9445, Train: 2.7917, Val: 2.7940\n",
      "Epoch: 002, Loss: 11.5519, Train: 2.7623, Val: 2.7657\n",
      "Epoch: 003, Loss: 9.3448, Train: 2.5980, Val: 2.6005\n",
      "Epoch: 004, Loss: 6.8528, Train: 2.3546, Val: 2.3556\n",
      "Epoch: 005, Loss: 3.9333, Train: 1.9845, Val: 1.9836\n",
      "Epoch: 006, Loss: 2.0399, Train: 1.6997, Val: 1.6980\n",
      "Epoch: 007, Loss: 2.0187, Train: 1.5629, Val: 1.5605\n",
      "Epoch: 008, Loss: 1.9424, Train: 1.4834, Val: 1.4808\n",
      "Epoch: 009, Loss: 1.4696, Train: 1.4384, Val: 1.4358\n",
      "Epoch: 010, Loss: 1.3670, Train: 1.4232, Val: 1.4201\n",
      "Epoch: 011, Loss: 1.4433, Train: 1.4160, Val: 1.4129\n",
      "Epoch: 012, Loss: 1.6142, Train: 1.4148, Val: 1.4116\n",
      "Epoch: 013, Loss: 1.4122, Train: 1.4069, Val: 1.4040\n",
      "Epoch: 014, Loss: 1.3987, Train: 1.3925, Val: 1.3901\n",
      "Epoch: 015, Loss: 1.3289, Train: 1.3839, Val: 1.3818\n",
      "Epoch: 016, Loss: 1.3187, Train: 1.3769, Val: 1.3748\n",
      "Epoch: 017, Loss: 1.3042, Train: 1.3630, Val: 1.3605\n",
      "Epoch: 018, Loss: 1.2839, Train: 1.3506, Val: 1.3470\n",
      "Epoch: 019, Loss: 1.2912, Train: 1.3404, Val: 1.3361\n",
      "Epoch: 020, Loss: 1.2867, Train: 1.3225, Val: 1.3178\n",
      "Epoch: 021, Loss: 1.2694, Train: 1.3045, Val: 1.3000\n",
      "Epoch: 022, Loss: 1.2727, Train: 1.2890, Val: 1.2857\n",
      "Epoch: 023, Loss: 1.2653, Train: 1.2750, Val: 1.2733\n",
      "Epoch: 024, Loss: 1.2564, Train: 1.2712, Val: 1.2705\n",
      "Epoch: 025, Loss: 1.2628, Train: 1.2622, Val: 1.2626\n",
      "Epoch: 026, Loss: 1.2576, Train: 1.2578, Val: 1.2587\n",
      "Epoch: 027, Loss: 1.2609, Train: 1.2647, Val: 1.2660\n",
      "Epoch: 028, Loss: 1.2608, Train: 1.3106, Val: 1.3123\n",
      "Epoch: 029, Loss: 1.2557, Train: 1.3422, Val: 1.3443\n",
      "Epoch: 030, Loss: 1.2566, Train: 1.3139, Val: 1.3162\n",
      "Epoch: 031, Loss: 1.2544, Train: 1.2810, Val: 1.2833\n",
      "Epoch: 032, Loss: 1.2497, Train: 1.2529, Val: 1.2552\n",
      "Epoch: 033, Loss: 1.2504, Train: 1.2392, Val: 1.2413\n",
      "Epoch: 034, Loss: 1.2510, Train: 1.2412, Val: 1.2431\n",
      "Epoch: 035, Loss: 1.2503, Train: 1.2481, Val: 1.2499\n",
      "Epoch: 036, Loss: 1.2511, Train: 1.2448, Val: 1.2465\n",
      "Epoch: 037, Loss: 1.2496, Train: 1.2317, Val: 1.2333\n",
      "Epoch: 038, Loss: 1.2484, Train: 1.2180, Val: 1.2195\n",
      "Epoch: 039, Loss: 1.2494, Train: 1.2085, Val: 1.2100\n",
      "Epoch: 040, Loss: 1.2483, Train: 1.2008, Val: 1.2023\n",
      "Epoch: 041, Loss: 1.2472, Train: 1.1890, Val: 1.1905\n",
      "Epoch: 042, Loss: 1.2472, Train: 1.1704, Val: 1.1718\n",
      "Epoch: 043, Loss: 1.2466, Train: 1.1515, Val: 1.1527\n",
      "Epoch: 044, Loss: 1.2472, Train: 1.1386, Val: 1.1397\n",
      "Epoch: 045, Loss: 1.2477, Train: 1.1318, Val: 1.1328\n",
      "Epoch: 046, Loss: 1.2468, Train: 1.1295, Val: 1.1306\n",
      "Epoch: 047, Loss: 1.2466, Train: 1.1282, Val: 1.1293\n",
      "Epoch: 048, Loss: 1.2464, Train: 1.1265, Val: 1.1275\n",
      "Epoch: 049, Loss: 1.2461, Train: 1.1247, Val: 1.1255\n",
      "Epoch: 050, Loss: 1.2461, Train: 1.1230, Val: 1.1238\n",
      "Epoch: 051, Loss: 1.2457, Train: 1.1216, Val: 1.1223\n",
      "Epoch: 052, Loss: 1.2455, Train: 1.1206, Val: 1.1212\n",
      "Epoch: 053, Loss: 1.2457, Train: 1.1207, Val: 1.1212\n",
      "Epoch: 054, Loss: 1.2455, Train: 1.1221, Val: 1.1225\n",
      "Epoch: 055, Loss: 1.2454, Train: 1.1234, Val: 1.1239\n",
      "Epoch: 056, Loss: 1.2452, Train: 1.1236, Val: 1.1240\n",
      "Epoch: 057, Loss: 1.2449, Train: 1.1236, Val: 1.1241\n",
      "Epoch: 058, Loss: 1.2449, Train: 1.1241, Val: 1.1246\n",
      "Epoch: 059, Loss: 1.2447, Train: 1.1249, Val: 1.1253\n",
      "Epoch: 060, Loss: 1.2447, Train: 1.1249, Val: 1.1253\n",
      "Epoch: 061, Loss: 1.2447, Train: 1.1237, Val: 1.1241\n",
      "Epoch: 062, Loss: 1.2445, Train: 1.1232, Val: 1.1236\n",
      "Epoch: 063, Loss: 1.2444, Train: 1.1234, Val: 1.1238\n",
      "Epoch: 064, Loss: 1.2443, Train: 1.1238, Val: 1.1241\n",
      "Epoch: 065, Loss: 1.2442, Train: 1.1239, Val: 1.1242\n",
      "Epoch: 066, Loss: 1.2441, Train: 1.1231, Val: 1.1234\n",
      "Epoch: 067, Loss: 1.2439, Train: 1.1222, Val: 1.1226\n",
      "Epoch: 068, Loss: 1.2439, Train: 1.1227, Val: 1.1231\n",
      "Epoch: 069, Loss: 1.2438, Train: 1.1237, Val: 1.1241\n",
      "Epoch: 070, Loss: 1.2438, Train: 1.1241, Val: 1.1244\n",
      "Epoch: 071, Loss: 1.2437, Train: 1.1234, Val: 1.1237\n",
      "Epoch: 072, Loss: 1.2436, Train: 1.1234, Val: 1.1238\n",
      "Epoch: 073, Loss: 1.2435, Train: 1.1243, Val: 1.1246\n",
      "Epoch: 074, Loss: 1.2434, Train: 1.1258, Val: 1.1261\n",
      "Epoch: 075, Loss: 1.2434, Train: 1.1264, Val: 1.1267\n",
      "Epoch: 076, Loss: 1.2433, Train: 1.1257, Val: 1.1260\n",
      "Epoch: 077, Loss: 1.2432, Train: 1.1244, Val: 1.1247\n",
      "Epoch: 078, Loss: 1.2432, Train: 1.1241, Val: 1.1245\n",
      "Epoch: 079, Loss: 1.2431, Train: 1.1241, Val: 1.1244\n",
      "Epoch: 080, Loss: 1.2430, Train: 1.1236, Val: 1.1240\n",
      "Epoch: 081, Loss: 1.2429, Train: 1.1231, Val: 1.1235\n",
      "Epoch: 082, Loss: 1.2429, Train: 1.1228, Val: 1.1232\n",
      "Epoch: 083, Loss: 1.2428, Train: 1.1224, Val: 1.1228\n",
      "Epoch: 084, Loss: 1.2427, Train: 1.1218, Val: 1.1222\n",
      "Epoch: 085, Loss: 1.2427, Train: 1.1210, Val: 1.1215\n",
      "Epoch: 086, Loss: 1.2426, Train: 1.1200, Val: 1.1205\n",
      "Epoch: 087, Loss: 1.2426, Train: 1.1193, Val: 1.1198\n",
      "Epoch: 088, Loss: 1.2425, Train: 1.1188, Val: 1.1193\n",
      "Epoch: 089, Loss: 1.2424, Train: 1.1177, Val: 1.1183\n",
      "Epoch: 090, Loss: 1.2424, Train: 1.1171, Val: 1.1176\n",
      "Epoch: 091, Loss: 1.2423, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 092, Loss: 1.2422, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 093, Loss: 1.2422, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 094, Loss: 1.2421, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 095, Loss: 1.2420, Train: 1.1159, Val: 1.1165\n",
      "Epoch: 096, Loss: 1.2419, Train: 1.1157, Val: 1.1163\n",
      "Epoch: 097, Loss: 1.2418, Train: 1.1156, Val: 1.1162\n",
      "Epoch: 098, Loss: 1.2418, Train: 1.1155, Val: 1.1160\n",
      "Epoch: 099, Loss: 1.2417, Train: 1.1153, Val: 1.1159\n",
      "Epoch: 100, Loss: 1.2416, Train: 1.1152, Val: 1.1158\n",
      "Epoch: 101, Loss: 1.2415, Train: 1.1152, Val: 1.1158\n",
      "Epoch: 102, Loss: 1.2414, Train: 1.1152, Val: 1.1157\n",
      "Epoch: 103, Loss: 1.2413, Train: 1.1152, Val: 1.1157\n",
      "Epoch: 104, Loss: 1.2413, Train: 1.1151, Val: 1.1157\n",
      "Epoch: 105, Loss: 1.2412, Train: 1.1150, Val: 1.1156\n",
      "Epoch: 106, Loss: 1.2411, Train: 1.1149, Val: 1.1155\n",
      "Epoch: 107, Loss: 1.2410, Train: 1.1152, Val: 1.1158\n",
      "Epoch: 108, Loss: 1.2409, Train: 1.1153, Val: 1.1159\n",
      "Epoch: 109, Loss: 1.2408, Train: 1.1151, Val: 1.1157\n",
      "Epoch: 110, Loss: 1.2407, Train: 1.1149, Val: 1.1155\n",
      "Epoch: 111, Loss: 1.2407, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 112, Loss: 1.2406, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 113, Loss: 1.2405, Train: 1.1164, Val: 1.1170\n",
      "Epoch: 114, Loss: 1.2404, Train: 1.1189, Val: 1.1194\n",
      "Epoch: 115, Loss: 1.2404, Train: 1.1194, Val: 1.1198\n",
      "Epoch: 116, Loss: 1.2403, Train: 1.1182, Val: 1.1187\n",
      "Epoch: 117, Loss: 1.2402, Train: 1.1158, Val: 1.1165\n",
      "Epoch: 118, Loss: 1.2401, Train: 1.1146, Val: 1.1152\n",
      "Epoch: 119, Loss: 1.2401, Train: 1.1144, Val: 1.1150\n",
      "Epoch: 120, Loss: 1.2400, Train: 1.1142, Val: 1.1148\n",
      "Epoch: 121, Loss: 1.2399, Train: 1.1145, Val: 1.1152\n",
      "Epoch: 122, Loss: 1.2398, Train: 1.1149, Val: 1.1156\n",
      "Epoch: 123, Loss: 1.2397, Train: 1.1147, Val: 1.1154\n",
      "Epoch: 124, Loss: 1.2397, Train: 1.1142, Val: 1.1149\n",
      "Epoch: 125, Loss: 1.2396, Train: 1.1143, Val: 1.1150\n",
      "Epoch: 126, Loss: 1.2395, Train: 1.1160, Val: 1.1167\n",
      "Epoch: 127, Loss: 1.2394, Train: 1.1183, Val: 1.1190\n",
      "Epoch: 128, Loss: 1.2394, Train: 1.1179, Val: 1.1186\n",
      "Epoch: 129, Loss: 1.2393, Train: 1.1158, Val: 1.1164\n",
      "Epoch: 130, Loss: 1.2392, Train: 1.1146, Val: 1.1153\n",
      "Epoch: 131, Loss: 1.2391, Train: 1.1151, Val: 1.1157\n",
      "Epoch: 132, Loss: 1.2391, Train: 1.1153, Val: 1.1158\n",
      "Epoch: 133, Loss: 1.2390, Train: 1.1146, Val: 1.1152\n",
      "Epoch: 134, Loss: 1.2389, Train: 1.1150, Val: 1.1157\n",
      "Epoch: 135, Loss: 1.2388, Train: 1.1161, Val: 1.1168\n",
      "Epoch: 136, Loss: 1.2387, Train: 1.1167, Val: 1.1175\n",
      "Epoch: 137, Loss: 1.2386, Train: 1.1170, Val: 1.1177\n",
      "Epoch: 138, Loss: 1.2385, Train: 1.1172, Val: 1.1180\n",
      "Epoch: 139, Loss: 1.2384, Train: 1.1170, Val: 1.1178\n",
      "Epoch: 140, Loss: 1.2384, Train: 1.1170, Val: 1.1177\n",
      "Epoch: 141, Loss: 1.2382, Train: 1.1176, Val: 1.1183\n",
      "Epoch: 142, Loss: 1.2381, Train: 1.1181, Val: 1.1188\n",
      "Epoch: 143, Loss: 1.2380, Train: 1.1191, Val: 1.1198\n",
      "Epoch: 144, Loss: 1.2380, Train: 1.1189, Val: 1.1196\n",
      "Epoch: 145, Loss: 1.2378, Train: 1.1189, Val: 1.1197\n",
      "Epoch: 146, Loss: 1.2377, Train: 1.1191, Val: 1.1199\n",
      "Epoch: 147, Loss: 1.2376, Train: 1.1199, Val: 1.1206\n",
      "Epoch: 148, Loss: 1.2375, Train: 1.1202, Val: 1.1208\n",
      "Epoch: 149, Loss: 1.2374, Train: 1.1200, Val: 1.1205\n",
      "Epoch: 150, Loss: 1.2373, Train: 1.1199, Val: 1.1205\n",
      "Epoch: 151, Loss: 1.2372, Train: 1.1200, Val: 1.1205\n",
      "Epoch: 152, Loss: 1.2371, Train: 1.1200, Val: 1.1205\n",
      "Epoch: 153, Loss: 1.2370, Train: 1.1200, Val: 1.1204\n",
      "Epoch: 154, Loss: 1.2369, Train: 1.1199, Val: 1.1204\n",
      "Epoch: 155, Loss: 1.2368, Train: 1.1201, Val: 1.1207\n",
      "Epoch: 156, Loss: 1.2367, Train: 1.1191, Val: 1.1197\n",
      "Epoch: 157, Loss: 1.2366, Train: 1.1190, Val: 1.1196\n",
      "Epoch: 158, Loss: 1.2365, Train: 1.1178, Val: 1.1185\n",
      "Epoch: 159, Loss: 1.2363, Train: 1.1189, Val: 1.1196\n",
      "Epoch: 160, Loss: 1.2363, Train: 1.1172, Val: 1.1179\n",
      "Epoch: 161, Loss: 1.2366, Train: 1.1189, Val: 1.1196\n",
      "Epoch: 162, Loss: 1.2363, Train: 1.1208, Val: 1.1215\n",
      "Epoch: 163, Loss: 1.2363, Train: 1.1186, Val: 1.1193\n",
      "Epoch: 164, Loss: 1.2360, Train: 1.1178, Val: 1.1184\n",
      "Epoch: 165, Loss: 1.2358, Train: 1.1197, Val: 1.1203\n",
      "Epoch: 166, Loss: 1.2361, Train: 1.1204, Val: 1.1210\n",
      "Epoch: 167, Loss: 1.2366, Train: 1.1238, Val: 1.1245\n",
      "Epoch: 168, Loss: 1.2377, Train: 1.1243, Val: 1.1250\n",
      "Epoch: 169, Loss: 1.2377, Train: 1.1234, Val: 1.1241\n",
      "Epoch: 170, Loss: 1.2374, Train: 1.1244, Val: 1.1251\n",
      "Epoch: 171, Loss: 1.2373, Train: 1.1270, Val: 1.1278\n",
      "Epoch: 172, Loss: 1.2373, Train: 1.1269, Val: 1.1277\n",
      "Epoch: 173, Loss: 1.2373, Train: 1.1268, Val: 1.1275\n",
      "Epoch: 174, Loss: 1.2372, Train: 1.1278, Val: 1.1283\n",
      "Epoch: 175, Loss: 1.2370, Train: 1.1276, Val: 1.1282\n",
      "Epoch: 176, Loss: 1.2369, Train: 1.1256, Val: 1.1263\n",
      "Epoch: 177, Loss: 1.2367, Train: 1.1256, Val: 1.1265\n",
      "Epoch: 178, Loss: 1.2365, Train: 1.1265, Val: 1.1272\n",
      "Epoch: 179, Loss: 1.2363, Train: 1.1266, Val: 1.1267\n",
      "Epoch: 180, Loss: 1.2362, Train: 1.1313, Val: 1.1309\n",
      "Epoch: 181, Loss: 1.2361, Train: 1.1365, Val: 1.1358\n",
      "Epoch: 182, Loss: 1.2359, Train: 1.1329, Val: 1.1324\n",
      "Epoch: 183, Loss: 1.2357, Train: 1.1302, Val: 1.1299\n",
      "Epoch: 184, Loss: 1.2355, Train: 1.1328, Val: 1.1324\n",
      "Epoch: 185, Loss: 1.2353, Train: 1.1406, Val: 1.1397\n",
      "Epoch: 186, Loss: 1.2352, Train: 1.1494, Val: 1.1480\n",
      "Epoch: 187, Loss: 1.2350, Train: 1.1531, Val: 1.1515\n",
      "Epoch: 188, Loss: 1.2347, Train: 1.1459, Val: 1.1448\n",
      "Epoch: 189, Loss: 1.2345, Train: 1.1413, Val: 1.1404\n",
      "Epoch: 190, Loss: 1.2343, Train: 1.1504, Val: 1.1490\n",
      "Epoch: 191, Loss: 1.2344, Train: 1.1250, Val: 1.1254\n",
      "Epoch: 192, Loss: 1.2350, Train: 1.1289, Val: 1.1292\n",
      "Epoch: 193, Loss: 1.2344, Train: 1.1565, Val: 1.1550\n",
      "Epoch: 194, Loss: 1.2346, Train: 1.1262, Val: 1.1266\n",
      "Epoch: 195, Loss: 1.2356, Train: 1.1340, Val: 1.1339\n",
      "Epoch: 196, Loss: 1.2354, Train: 1.1334, Val: 1.1332\n",
      "Epoch: 197, Loss: 1.2345, Train: 1.1289, Val: 1.1288\n",
      "Epoch: 198, Loss: 1.2352, Train: 1.1251, Val: 1.1257\n",
      "Epoch: 199, Loss: 1.2342, Train: 1.1231, Val: 1.1238\n",
      "Epoch: 200, Loss: 1.2345, Train: 1.1197, Val: 1.1203\n",
      "Epoch: 201, Loss: 1.2344, Train: 1.1188, Val: 1.1194\n",
      "Epoch: 202, Loss: 1.2336, Train: 1.1191, Val: 1.1199\n",
      "Epoch: 203, Loss: 1.2340, Train: 1.1222, Val: 1.1228\n",
      "Epoch: 204, Loss: 1.2334, Train: 1.1257, Val: 1.1261\n",
      "Epoch: 205, Loss: 1.2331, Train: 1.1290, Val: 1.1290\n",
      "Epoch: 206, Loss: 1.2333, Train: 1.1381, Val: 1.1376\n",
      "Epoch: 207, Loss: 1.2327, Train: 1.1703, Val: 1.1681\n",
      "Epoch: 208, Loss: 1.2325, Train: 1.1984, Val: 1.1950\n",
      "Epoch: 209, Loss: 1.2330, Train: 1.1267, Val: 1.1270\n",
      "Epoch: 210, Loss: 1.2343, Train: 1.1265, Val: 1.1274\n",
      "Epoch: 211, Loss: 1.2341, Train: 1.1281, Val: 1.1289\n",
      "Epoch: 212, Loss: 1.2336, Train: 1.1265, Val: 1.1272\n",
      "Epoch: 213, Loss: 1.2337, Train: 1.1279, Val: 1.1286\n",
      "Epoch: 214, Loss: 1.2333, Train: 1.1307, Val: 1.1314\n",
      "Epoch: 215, Loss: 1.2332, Train: 1.1323, Val: 1.1326\n",
      "Epoch: 216, Loss: 1.2329, Train: 1.1374, Val: 1.1377\n",
      "Epoch: 217, Loss: 1.2320, Train: 1.1413, Val: 1.1417\n",
      "Epoch: 218, Loss: 1.2323, Train: 1.1379, Val: 1.1383\n",
      "Epoch: 219, Loss: 1.2321, Train: 1.1406, Val: 1.1402\n",
      "Epoch: 220, Loss: 1.2315, Train: 1.1487, Val: 1.1477\n",
      "Epoch: 221, Loss: 1.2314, Train: 1.1513, Val: 1.1501\n",
      "Epoch: 222, Loss: 1.2314, Train: 1.1561, Val: 1.1546\n",
      "Epoch: 223, Loss: 1.2314, Train: 1.1426, Val: 1.1416\n",
      "Epoch: 224, Loss: 1.2324, Train: 1.1491, Val: 1.1478\n",
      "Epoch: 225, Loss: 1.2318, Train: 1.1658, Val: 1.1641\n",
      "Epoch: 226, Loss: 1.2318, Train: 1.2341, Val: 1.2309\n",
      "Epoch: 227, Loss: 1.2316, Train: 1.1391, Val: 1.1378\n",
      "Epoch: 228, Loss: 1.2314, Train: 1.1327, Val: 1.1329\n",
      "Epoch: 229, Loss: 1.2314, Train: 1.1248, Val: 1.1244\n",
      "Epoch: 230, Loss: 1.2313, Train: 1.1709, Val: 1.1687\n",
      "Epoch: 231, Loss: 1.2311, Train: 1.2579, Val: 1.2543\n",
      "Epoch: 232, Loss: 1.2312, Train: 1.1802, Val: 1.1777\n",
      "Epoch: 233, Loss: 1.2308, Train: 1.1801, Val: 1.1774\n",
      "Epoch: 234, Loss: 1.2305, Train: 1.1258, Val: 1.1265\n",
      "Epoch: 235, Loss: 1.2304, Train: 1.1256, Val: 1.1264\n",
      "Epoch: 236, Loss: 1.2300, Train: 1.1165, Val: 1.1174\n",
      "Epoch: 237, Loss: 1.2312, Train: 1.1160, Val: 1.1167\n",
      "Epoch: 238, Loss: 1.2318, Train: 1.1384, Val: 1.1374\n",
      "Epoch: 239, Loss: 1.2318, Train: 1.2266, Val: 1.2228\n",
      "Epoch: 240, Loss: 1.2316, Train: 1.2488, Val: 1.2442\n",
      "Epoch: 241, Loss: 1.2315, Train: 1.1135, Val: 1.1143\n",
      "Epoch: 242, Loss: 1.2315, Train: 1.1323, Val: 1.1330\n",
      "Epoch: 243, Loss: 1.2315, Train: 1.1152, Val: 1.1160\n",
      "Epoch: 244, Loss: 1.2313, Train: 1.1206, Val: 1.1207\n",
      "Epoch: 245, Loss: 1.2310, Train: 1.1852, Val: 1.1819\n",
      "Epoch: 246, Loss: 1.2306, Train: 1.2993, Val: 1.2934\n",
      "Epoch: 247, Loss: 1.2302, Train: 1.2669, Val: 1.2613\n",
      "Epoch: 248, Loss: 1.2302, Train: 1.1896, Val: 1.1863\n",
      "Epoch: 249, Loss: 1.2300, Train: 1.1797, Val: 1.1771\n",
      "Epoch: 250, Loss: 1.2297, Train: 1.1385, Val: 1.1386\n",
      "Epoch: 251, Loss: 1.2293, Train: 1.1419, Val: 1.1425\n",
      "Epoch: 252, Loss: 1.2293, Train: 1.1228, Val: 1.1232\n",
      "Epoch: 253, Loss: 1.2290, Train: 1.1578, Val: 1.1555\n",
      "Epoch: 254, Loss: 1.2287, Train: 1.2055, Val: 1.2013\n",
      "Epoch: 255, Loss: 1.2285, Train: 1.1397, Val: 1.1403\n",
      "Epoch: 256, Loss: 1.2291, Train: 1.1547, Val: 1.1529\n",
      "Epoch: 257, Loss: 1.2287, Train: 1.1619, Val: 1.1611\n",
      "Epoch: 258, Loss: 1.2284, Train: 1.1699, Val: 1.1690\n",
      "Epoch: 259, Loss: 1.2282, Train: 1.1910, Val: 1.1904\n",
      "Epoch: 260, Loss: 1.2277, Train: 1.1653, Val: 1.1640\n",
      "Epoch: 261, Loss: 1.2273, Train: 1.1486, Val: 1.1477\n",
      "Epoch: 262, Loss: 1.2272, Train: 1.2532, Val: 1.2480\n",
      "Epoch: 263, Loss: 1.2269, Train: 1.1439, Val: 1.1447\n",
      "Epoch: 264, Loss: 1.2270, Train: 1.1415, Val: 1.1419\n",
      "Epoch: 265, Loss: 1.2266, Train: 1.1856, Val: 1.1832\n",
      "Epoch: 266, Loss: 1.2271, Train: 1.2007, Val: 1.2010\n",
      "Epoch: 267, Loss: 1.2272, Train: 1.1931, Val: 1.1923\n",
      "Epoch: 268, Loss: 1.2273, Train: 1.1614, Val: 1.1622\n",
      "Epoch: 269, Loss: 1.2274, Train: 1.1672, Val: 1.1686\n",
      "Epoch: 270, Loss: 1.2275, Train: 1.1664, Val: 1.1658\n",
      "Epoch: 271, Loss: 1.2276, Train: 1.2058, Val: 1.2061\n",
      "Epoch: 272, Loss: 1.2266, Train: 1.2460, Val: 1.2445\n",
      "Epoch: 273, Loss: 1.2272, Train: 1.2159, Val: 1.2159\n",
      "Epoch: 274, Loss: 1.2270, Train: 1.2358, Val: 1.2341\n",
      "Epoch: 275, Loss: 1.2269, Train: 1.2533, Val: 1.2523\n",
      "Epoch: 276, Loss: 1.2272, Train: 1.2204, Val: 1.2194\n",
      "Epoch: 277, Loss: 1.2256, Train: 1.4240, Val: 1.4180\n",
      "Epoch: 278, Loss: 1.2314, Train: 1.4662, Val: 1.4612\n",
      "Epoch: 279, Loss: 1.2359, Train: 1.3927, Val: 1.3891\n",
      "Epoch: 280, Loss: 1.2344, Train: 1.2973, Val: 1.2917\n",
      "Epoch: 281, Loss: 1.2350, Train: 1.2030, Val: 1.2037\n",
      "Epoch: 282, Loss: 1.2323, Train: 1.2056, Val: 1.2066\n",
      "Epoch: 283, Loss: 1.2314, Train: 1.1487, Val: 1.1492\n",
      "Epoch: 284, Loss: 1.2311, Train: 1.1496, Val: 1.1503\n",
      "Epoch: 285, Loss: 1.2318, Train: 1.1644, Val: 1.1652\n",
      "Epoch: 286, Loss: 1.2312, Train: 1.1649, Val: 1.1657\n",
      "Epoch: 287, Loss: 1.2304, Train: 1.1663, Val: 1.1669\n",
      "Epoch: 288, Loss: 1.2296, Train: 1.1845, Val: 1.1846\n",
      "Epoch: 289, Loss: 1.2303, Train: 1.1928, Val: 1.1927\n",
      "Epoch: 290, Loss: 1.2300, Train: 1.2211, Val: 1.2204\n",
      "Epoch: 291, Loss: 1.2295, Train: 1.2159, Val: 1.2146\n",
      "Epoch: 292, Loss: 1.2284, Train: 1.2174, Val: 1.2157\n",
      "Epoch: 293, Loss: 1.2282, Train: 1.2227, Val: 1.2208\n",
      "Epoch: 294, Loss: 1.2285, Train: 1.2119, Val: 1.2101\n",
      "Epoch: 295, Loss: 1.2285, Train: 1.2133, Val: 1.2113\n",
      "Epoch: 296, Loss: 1.2278, Train: 1.2054, Val: 1.2038\n",
      "Epoch: 297, Loss: 1.2272, Train: 1.2021, Val: 1.2009\n",
      "Epoch: 298, Loss: 1.2268, Train: 1.2070, Val: 1.2060\n",
      "Epoch: 299, Loss: 1.2267, Train: 1.2112, Val: 1.2099\n",
      "Epoch: 300, Loss: 1.2270, Train: 1.2204, Val: 1.2191\n",
      "Epoch: 301, Loss: 1.2267, Train: 1.2119, Val: 1.2108\n",
      "Epoch: 302, Loss: 1.2265, Train: 1.2133, Val: 1.2124\n",
      "Epoch: 303, Loss: 1.2257, Train: 1.2034, Val: 1.2025\n",
      "Epoch: 304, Loss: 1.2253, Train: 1.1925, Val: 1.1919\n",
      "Epoch: 305, Loss: 1.2253, Train: 1.1923, Val: 1.1920\n",
      "Epoch: 306, Loss: 1.2248, Train: 1.1886, Val: 1.1883\n",
      "Epoch: 307, Loss: 1.2248, Train: 1.1897, Val: 1.1894\n",
      "Epoch: 308, Loss: 1.2245, Train: 1.1946, Val: 1.1937\n",
      "Epoch: 309, Loss: 1.2242, Train: 1.2203, Val: 1.2188\n",
      "Epoch: 310, Loss: 1.2239, Train: 1.2084, Val: 1.2075\n",
      "Epoch: 311, Loss: 1.2236, Train: 1.2153, Val: 1.2142\n",
      "Epoch: 312, Loss: 1.2234, Train: 1.2330, Val: 1.2314\n",
      "Epoch: 313, Loss: 1.2234, Train: 1.2054, Val: 1.2046\n",
      "Epoch: 314, Loss: 1.2231, Train: 1.1968, Val: 1.1966\n",
      "Epoch: 315, Loss: 1.2230, Train: 1.1948, Val: 1.1945\n",
      "Epoch: 316, Loss: 1.2228, Train: 1.1952, Val: 1.1944\n",
      "Epoch: 317, Loss: 1.2225, Train: 1.2091, Val: 1.2077\n",
      "Epoch: 318, Loss: 1.2223, Train: 1.1845, Val: 1.1839\n",
      "Epoch: 319, Loss: 1.2221, Train: 1.1815, Val: 1.1809\n",
      "Epoch: 320, Loss: 1.2221, Train: 1.1859, Val: 1.1855\n",
      "Epoch: 321, Loss: 1.2225, Train: 1.1858, Val: 1.1860\n",
      "Epoch: 322, Loss: 1.2241, Train: 1.2697, Val: 1.2671\n",
      "Epoch: 323, Loss: 1.2285, Train: 1.2821, Val: 1.2799\n",
      "Epoch: 324, Loss: 1.2347, Train: 1.2485, Val: 1.2484\n",
      "Epoch: 325, Loss: 1.2540, Train: 1.2157, Val: 1.2159\n",
      "Epoch: 326, Loss: 1.2695, Train: 1.1567, Val: 1.1581\n",
      "Epoch: 327, Loss: 1.2807, Train: 1.1561, Val: 1.1569\n",
      "Epoch: 328, Loss: 1.2439, Train: 1.1455, Val: 1.1468\n",
      "Epoch: 329, Loss: 1.2246, Train: 1.1296, Val: 1.1313\n",
      "Epoch: 330, Loss: 1.2424, Train: 1.1952, Val: 1.1945\n",
      "Epoch: 331, Loss: 1.2443, Train: 1.1383, Val: 1.1379\n",
      "Epoch: 332, Loss: 1.2298, Train: 1.4348, Val: 1.4313\n",
      "Epoch: 333, Loss: 1.2296, Train: 1.4622, Val: 1.4578\n",
      "Epoch: 334, Loss: 1.2371, Train: 1.4447, Val: 1.4407\n",
      "Epoch: 335, Loss: 1.2288, Train: 1.4398, Val: 1.4364\n",
      "Epoch: 336, Loss: 1.2264, Train: 1.4378, Val: 1.4350\n",
      "Epoch: 337, Loss: 1.2334, Train: 1.4298, Val: 1.4276\n",
      "Epoch: 338, Loss: 1.2268, Train: 1.4128, Val: 1.4111\n",
      "Epoch: 339, Loss: 1.2258, Train: 1.3011, Val: 1.2985\n",
      "Epoch: 340, Loss: 1.2307, Train: 1.2552, Val: 1.2534\n",
      "Epoch: 341, Loss: 1.2259, Train: 1.2847, Val: 1.2826\n",
      "Epoch: 342, Loss: 1.2239, Train: 1.3692, Val: 1.3659\n",
      "Epoch: 343, Loss: 1.2277, Train: 1.3662, Val: 1.3625\n",
      "Epoch: 344, Loss: 1.2236, Train: 1.2426, Val: 1.2412\n",
      "Epoch: 345, Loss: 1.2235, Train: 1.1931, Val: 1.1942\n",
      "Epoch: 346, Loss: 1.2250, Train: 1.1987, Val: 1.1999\n",
      "Epoch: 347, Loss: 1.2227, Train: 1.1806, Val: 1.1819\n",
      "Epoch: 348, Loss: 1.2226, Train: 1.1774, Val: 1.1789\n",
      "Epoch: 349, Loss: 1.2228, Train: 1.2027, Val: 1.2034\n",
      "Epoch: 350, Loss: 1.2211, Train: 1.1925, Val: 1.1937\n",
      "Epoch: 351, Loss: 1.2214, Train: 1.1721, Val: 1.1730\n",
      "Epoch: 352, Loss: 1.2215, Train: 1.1770, Val: 1.1784\n",
      "Epoch: 353, Loss: 1.2201, Train: 1.1885, Val: 1.1897\n",
      "Epoch: 354, Loss: 1.2206, Train: 1.1658, Val: 1.1650\n",
      "Epoch: 355, Loss: 1.2204, Train: 1.2713, Val: 1.2674\n",
      "Epoch: 356, Loss: 1.2192, Train: 1.3528, Val: 1.3480\n",
      "Epoch: 357, Loss: 1.2200, Train: 1.4210, Val: 1.4160\n",
      "Epoch: 358, Loss: 1.2195, Train: 1.4528, Val: 1.4472\n",
      "Epoch: 359, Loss: 1.2192, Train: 1.4752, Val: 1.4694\n",
      "Epoch: 360, Loss: 1.2192, Train: 1.4856, Val: 1.4799\n",
      "Epoch: 361, Loss: 1.2182, Train: 1.4703, Val: 1.4644\n",
      "Epoch: 362, Loss: 1.2189, Train: 1.3263, Val: 1.3229\n",
      "Epoch: 363, Loss: 1.2193, Train: 1.2093, Val: 1.2080\n",
      "Epoch: 364, Loss: 1.2198, Train: 1.1857, Val: 1.1843\n",
      "Epoch: 365, Loss: 1.2210, Train: 1.1785, Val: 1.1779\n",
      "Epoch: 366, Loss: 1.2192, Train: 1.1736, Val: 1.1731\n",
      "Epoch: 367, Loss: 1.2188, Train: 1.1677, Val: 1.1674\n",
      "Epoch: 368, Loss: 1.2189, Train: 1.1714, Val: 1.1710\n",
      "Epoch: 369, Loss: 1.2191, Train: 1.1621, Val: 1.1620\n",
      "Epoch: 370, Loss: 1.2180, Train: 1.1590, Val: 1.1593\n",
      "Epoch: 371, Loss: 1.2180, Train: 1.1489, Val: 1.1491\n",
      "Epoch: 372, Loss: 1.2169, Train: 1.1397, Val: 1.1399\n",
      "Epoch: 373, Loss: 1.2172, Train: 1.1387, Val: 1.1389\n",
      "Epoch: 374, Loss: 1.2173, Train: 1.1504, Val: 1.1501\n",
      "Epoch: 375, Loss: 1.2177, Train: 1.1609, Val: 1.1609\n",
      "Epoch: 376, Loss: 1.2166, Train: 1.1840, Val: 1.1841\n",
      "Epoch: 377, Loss: 1.2162, Train: 1.2235, Val: 1.2222\n",
      "Epoch: 378, Loss: 1.2162, Train: 1.2093, Val: 1.2074\n",
      "Epoch: 379, Loss: 1.2159, Train: 1.2814, Val: 1.2772\n",
      "Epoch: 380, Loss: 1.2153, Train: 1.2468, Val: 1.2425\n",
      "Epoch: 381, Loss: 1.2155, Train: 1.2486, Val: 1.2446\n",
      "Epoch: 382, Loss: 1.2153, Train: 1.1903, Val: 1.1882\n",
      "Epoch: 383, Loss: 1.2157, Train: 1.4904, Val: 1.4848\n",
      "Epoch: 384, Loss: 1.2173, Train: 1.5059, Val: 1.5006\n",
      "Epoch: 385, Loss: 1.2195, Train: 1.5230, Val: 1.5182\n",
      "Epoch: 386, Loss: 1.2201, Train: 1.5286, Val: 1.5241\n",
      "Epoch: 387, Loss: 1.2180, Train: 1.5238, Val: 1.5189\n",
      "Epoch: 388, Loss: 1.2196, Train: 1.4412, Val: 1.4361\n",
      "Epoch: 389, Loss: 1.2172, Train: 1.2345, Val: 1.2326\n",
      "Epoch: 390, Loss: 1.2158, Train: 1.2240, Val: 1.2224\n",
      "Epoch: 391, Loss: 1.2179, Train: 1.1757, Val: 1.1756\n",
      "Epoch: 392, Loss: 1.2169, Train: 1.1463, Val: 1.1456\n",
      "Epoch: 393, Loss: 1.2191, Train: 1.2003, Val: 1.1977\n",
      "Epoch: 394, Loss: 1.2220, Train: 1.3791, Val: 1.3742\n",
      "Epoch: 395, Loss: 1.2179, Train: 1.4440, Val: 1.4385\n",
      "Epoch: 396, Loss: 1.2160, Train: 1.4645, Val: 1.4588\n",
      "Epoch: 397, Loss: 1.2230, Train: 1.3836, Val: 1.3785\n",
      "Epoch: 398, Loss: 1.2327, Train: 1.3698, Val: 1.3664\n",
      "Epoch: 399, Loss: 1.2248, Train: 1.3708, Val: 1.3675\n",
      "Epoch: 400, Loss: 1.2265, Train: 1.3403, Val: 1.3365\n",
      "Epoch: 401, Loss: 1.2272, Train: 1.3239, Val: 1.3200\n",
      "Epoch: 402, Loss: 1.2266, Train: 1.3224, Val: 1.3192\n",
      "Epoch: 403, Loss: 1.2278, Train: 1.2980, Val: 1.2951\n",
      "Epoch: 404, Loss: 1.2259, Train: 1.2513, Val: 1.2487\n",
      "Epoch: 405, Loss: 1.2260, Train: 1.2331, Val: 1.2307\n",
      "Epoch: 406, Loss: 1.2279, Train: 1.2509, Val: 1.2480\n",
      "Epoch: 407, Loss: 1.2295, Train: 1.3467, Val: 1.3435\n",
      "Epoch: 408, Loss: 1.2286, Train: 1.4314, Val: 1.4271\n",
      "Epoch: 409, Loss: 1.2268, Train: 1.4662, Val: 1.4613\n",
      "Epoch: 410, Loss: 1.2258, Train: 1.4652, Val: 1.4599\n",
      "Epoch: 411, Loss: 1.2256, Train: 1.4623, Val: 1.4572\n",
      "Epoch: 412, Loss: 1.2255, Train: 1.4651, Val: 1.4600\n",
      "Epoch: 413, Loss: 1.2251, Train: 1.4586, Val: 1.4535\n",
      "Epoch: 414, Loss: 1.2248, Train: 1.4563, Val: 1.4512\n",
      "Epoch: 415, Loss: 1.2231, Train: 1.4556, Val: 1.4504\n",
      "Epoch: 416, Loss: 1.2225, Train: 1.4560, Val: 1.4507\n",
      "Epoch: 417, Loss: 1.2225, Train: 1.4256, Val: 1.4215\n",
      "Epoch: 418, Loss: 1.2218, Train: 1.3397, Val: 1.3365\n",
      "Epoch: 419, Loss: 1.2210, Train: 1.2307, Val: 1.2271\n",
      "Epoch: 420, Loss: 1.2206, Train: 1.1998, Val: 1.1981\n",
      "Epoch: 421, Loss: 1.2202, Train: 1.1887, Val: 1.1879\n",
      "Epoch: 422, Loss: 1.2198, Train: 1.1879, Val: 1.1880\n",
      "Epoch: 423, Loss: 1.2197, Train: 1.1653, Val: 1.1657\n",
      "Epoch: 424, Loss: 1.2192, Train: 1.1512, Val: 1.1514\n",
      "Epoch: 425, Loss: 1.2187, Train: 1.1535, Val: 1.1531\n",
      "Epoch: 426, Loss: 1.2184, Train: 1.1531, Val: 1.1525\n",
      "Epoch: 427, Loss: 1.2180, Train: 1.1640, Val: 1.1630\n",
      "Epoch: 428, Loss: 1.2177, Train: 1.1667, Val: 1.1659\n",
      "Epoch: 429, Loss: 1.2177, Train: 1.1668, Val: 1.1659\n",
      "Epoch: 430, Loss: 1.2182, Train: 1.1600, Val: 1.1597\n",
      "Epoch: 431, Loss: 1.2201, Train: 1.2036, Val: 1.2015\n",
      "Epoch: 432, Loss: 1.2257, Train: 1.1850, Val: 1.1839\n",
      "Epoch: 433, Loss: 1.2371, Train: 1.2367, Val: 1.2332\n",
      "Epoch: 434, Loss: 1.2487, Train: 1.2103, Val: 1.2096\n",
      "Epoch: 435, Loss: 1.2411, Train: 1.2249, Val: 1.2227\n",
      "Epoch: 436, Loss: 1.2219, Train: 1.2311, Val: 1.2281\n",
      "Epoch: 437, Loss: 1.2260, Train: 1.1722, Val: 1.1710\n",
      "Epoch: 438, Loss: 1.2294, Train: 1.2102, Val: 1.2094\n",
      "Epoch: 439, Loss: 1.2209, Train: 1.2061, Val: 1.2060\n",
      "Epoch: 440, Loss: 1.2223, Train: 1.1782, Val: 1.1798\n",
      "Epoch: 441, Loss: 1.2239, Train: 1.2091, Val: 1.2092\n",
      "Epoch: 442, Loss: 1.2185, Train: 1.2188, Val: 1.2183\n",
      "Epoch: 443, Loss: 1.2203, Train: 1.1943, Val: 1.1945\n",
      "Epoch: 444, Loss: 1.2198, Train: 1.2137, Val: 1.2139\n",
      "Epoch: 445, Loss: 1.2174, Train: 1.2050, Val: 1.2034\n",
      "Epoch: 446, Loss: 1.2192, Train: 1.1670, Val: 1.1673\n",
      "Epoch: 447, Loss: 1.2161, Train: 1.1523, Val: 1.1538\n",
      "Epoch: 448, Loss: 1.2176, Train: 1.1900, Val: 1.1889\n",
      "Epoch: 449, Loss: 1.2164, Train: 1.3155, Val: 1.3114\n",
      "Epoch: 450, Loss: 1.2156, Train: 1.3151, Val: 1.3111\n",
      "Epoch: 451, Loss: 1.2161, Train: 1.2319, Val: 1.2300\n",
      "Epoch: 452, Loss: 1.2146, Train: 1.3138, Val: 1.3104\n",
      "Epoch: 453, Loss: 1.2144, Train: 1.3130, Val: 1.3089\n",
      "Epoch: 454, Loss: 1.2145, Train: 1.2635, Val: 1.2644\n",
      "Epoch: 455, Loss: 1.2139, Train: 1.3117, Val: 1.3094\n",
      "Epoch: 456, Loss: 1.2137, Train: 1.3235, Val: 1.3201\n",
      "Epoch: 457, Loss: 1.2146, Train: 1.3016, Val: 1.3026\n",
      "Epoch: 458, Loss: 1.2141, Train: 1.3285, Val: 1.3317\n",
      "Epoch: 459, Loss: 1.2125, Train: 1.3301, Val: 1.3260\n",
      "Epoch: 460, Loss: 1.2121, Train: 1.2902, Val: 1.2879\n",
      "Epoch: 461, Loss: 1.2136, Train: 1.3083, Val: 1.3051\n",
      "Epoch: 462, Loss: 1.2126, Train: 1.2812, Val: 1.2798\n",
      "Epoch: 463, Loss: 1.2155, Train: 1.5012, Val: 1.4992\n",
      "Epoch: 464, Loss: 1.2139, Train: 1.5197, Val: 1.5151\n",
      "Epoch: 465, Loss: 1.2150, Train: 1.4646, Val: 1.4605\n",
      "Epoch: 466, Loss: 1.2149, Train: 1.5087, Val: 1.5042\n",
      "Epoch: 467, Loss: 1.2138, Train: 1.4196, Val: 1.4153\n",
      "Epoch: 468, Loss: 1.2136, Train: 1.2393, Val: 1.2388\n",
      "Epoch: 469, Loss: 1.2144, Train: 1.2717, Val: 1.2698\n",
      "Epoch: 470, Loss: 1.2136, Train: 1.2936, Val: 1.2911\n",
      "Epoch: 471, Loss: 1.2129, Train: 1.2955, Val: 1.2918\n",
      "Epoch: 472, Loss: 1.2129, Train: 1.3243, Val: 1.3210\n",
      "Epoch: 473, Loss: 1.2132, Train: 1.3159, Val: 1.3121\n",
      "Epoch: 474, Loss: 1.2131, Train: 1.3480, Val: 1.3439\n",
      "Epoch: 475, Loss: 1.2133, Train: 1.3186, Val: 1.3141\n",
      "Epoch: 476, Loss: 1.2156, Train: 1.3558, Val: 1.3513\n",
      "Epoch: 477, Loss: 1.2222, Train: 1.3461, Val: 1.3418\n",
      "Epoch: 478, Loss: 1.2402, Train: 1.3994, Val: 1.3956\n",
      "Epoch: 479, Loss: 1.2468, Train: 1.3227, Val: 1.3180\n",
      "Epoch: 480, Loss: 1.2378, Train: 1.3022, Val: 1.2985\n",
      "Epoch: 481, Loss: 1.2210, Train: 1.3256, Val: 1.3210\n",
      "Epoch: 482, Loss: 1.2227, Train: 1.2699, Val: 1.2657\n",
      "Epoch: 483, Loss: 1.2231, Train: 1.3368, Val: 1.3321\n",
      "Epoch: 484, Loss: 1.2282, Train: 1.2241, Val: 1.2218\n",
      "Epoch: 485, Loss: 1.2206, Train: 1.1914, Val: 1.1891\n",
      "Epoch: 486, Loss: 1.2165, Train: 1.2593, Val: 1.2567\n",
      "Epoch: 487, Loss: 1.2220, Train: 1.2096, Val: 1.2071\n",
      "Epoch: 488, Loss: 1.2206, Train: 1.2716, Val: 1.2691\n",
      "Epoch: 489, Loss: 1.2157, Train: 1.2108, Val: 1.2095\n",
      "Epoch: 490, Loss: 1.2150, Train: 1.2027, Val: 1.2020\n",
      "Epoch: 491, Loss: 1.2168, Train: 1.2970, Val: 1.2954\n",
      "Epoch: 492, Loss: 1.2160, Train: 1.2646, Val: 1.2629\n",
      "Epoch: 493, Loss: 1.2118, Train: 1.2087, Val: 1.2070\n",
      "Epoch: 494, Loss: 1.2121, Train: 1.2881, Val: 1.2870\n",
      "Epoch: 495, Loss: 1.2133, Train: 1.2156, Val: 1.2149\n",
      "Epoch: 496, Loss: 1.2114, Train: 1.1979, Val: 1.1976\n",
      "Epoch: 497, Loss: 1.2096, Train: 1.2101, Val: 1.2107\n",
      "Epoch: 498, Loss: 1.2107, Train: 1.1696, Val: 1.1712\n",
      "Epoch: 499, Loss: 1.2106, Train: 1.1834, Val: 1.1855\n",
      "Epoch: 500, Loss: 1.2083, Train: 1.1882, Val: 1.1900\n",
      "Test RMSE: 1.3644\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       4.254896       3.583673\n",
      "std      1727.484387     741.673176       0.402697       1.116938\n",
      "min         0.000000       0.000000       2.866282       1.000000\n",
      "25%      1500.000000     259.000000       4.077806       3.000000\n",
      "50%      3066.000000     693.000000       4.077806       4.000000\n",
      "75%      4472.000000    1292.000000       4.575845       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:06<00:00, 960.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80909243415533\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  1\n",
      "Kernel:  spd\n",
      "Model Name:  KPGINPlus\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 14.9149, Train: 2.9143, Val: 2.9181\n",
      "Epoch: 002, Loss: 12.7856, Train: 2.9354, Val: 2.9395\n",
      "Epoch: 003, Loss: 10.1106, Train: 2.8611, Val: 2.8652\n",
      "Epoch: 004, Loss: 7.4366, Train: 2.6601, Val: 2.6635\n",
      "Epoch: 005, Loss: 3.4662, Train: 2.2321, Val: 2.2331\n",
      "Epoch: 006, Loss: 1.8712, Train: 1.6972, Val: 1.6939\n",
      "Epoch: 007, Loss: 3.6255, Train: 1.6427, Val: 1.6395\n",
      "Epoch: 008, Loss: 1.4401, Train: 1.6304, Val: 1.6277\n",
      "Epoch: 009, Loss: 1.7318, Train: 1.5967, Val: 1.5946\n",
      "Epoch: 010, Loss: 1.6011, Train: 1.5158, Val: 1.5133\n",
      "Epoch: 011, Loss: 1.4244, Train: 1.4099, Val: 1.4069\n",
      "Epoch: 012, Loss: 1.4784, Train: 1.3723, Val: 1.3698\n",
      "Epoch: 013, Loss: 1.4837, Train: 1.3957, Val: 1.3950\n",
      "Epoch: 014, Loss: 1.3528, Train: 1.4470, Val: 1.4473\n",
      "Epoch: 015, Loss: 1.3059, Train: 1.4666, Val: 1.4672\n",
      "Epoch: 016, Loss: 1.3384, Train: 1.4130, Val: 1.4135\n",
      "Epoch: 017, Loss: 1.3077, Train: 1.3132, Val: 1.3133\n",
      "Epoch: 018, Loss: 1.2773, Train: 1.2449, Val: 1.2445\n",
      "Epoch: 019, Loss: 1.3240, Train: 1.2362, Val: 1.2358\n",
      "Epoch: 020, Loss: 1.3261, Train: 1.2772, Val: 1.2772\n",
      "Epoch: 021, Loss: 1.2754, Train: 1.3424, Val: 1.3429\n",
      "Epoch: 022, Loss: 1.2819, Train: 1.3637, Val: 1.3645\n",
      "Epoch: 023, Loss: 1.3028, Train: 1.3038, Val: 1.3045\n",
      "Epoch: 024, Loss: 1.2801, Train: 1.2185, Val: 1.2185\n",
      "Epoch: 025, Loss: 1.2662, Train: 1.1776, Val: 1.1771\n",
      "Epoch: 026, Loss: 1.2832, Train: 1.1723, Val: 1.1715\n",
      "Epoch: 027, Loss: 1.2735, Train: 1.1723, Val: 1.1716\n",
      "Epoch: 028, Loss: 1.2541, Train: 1.1782, Val: 1.1779\n",
      "Epoch: 029, Loss: 1.2649, Train: 1.1787, Val: 1.1789\n",
      "Epoch: 030, Loss: 1.2729, Train: 1.1673, Val: 1.1679\n",
      "Epoch: 031, Loss: 1.2588, Train: 1.1611, Val: 1.1618\n",
      "Epoch: 032, Loss: 1.2560, Train: 1.1703, Val: 1.1715\n",
      "Epoch: 033, Loss: 1.2629, Train: 1.2022, Val: 1.2037\n",
      "Epoch: 034, Loss: 1.2542, Train: 1.2529, Val: 1.2547\n",
      "Epoch: 035, Loss: 1.2494, Train: 1.2920, Val: 1.2939\n",
      "Epoch: 036, Loss: 1.2580, Train: 1.2851, Val: 1.2871\n",
      "Epoch: 037, Loss: 1.2575, Train: 1.2424, Val: 1.2443\n",
      "Epoch: 038, Loss: 1.2505, Train: 1.2023, Val: 1.2039\n",
      "Epoch: 039, Loss: 1.2528, Train: 1.1836, Val: 1.1849\n",
      "Epoch: 040, Loss: 1.2544, Train: 1.1776, Val: 1.1788\n",
      "Epoch: 041, Loss: 1.2495, Train: 1.1767, Val: 1.1779\n",
      "Epoch: 042, Loss: 1.2508, Train: 1.1701, Val: 1.1714\n",
      "Epoch: 043, Loss: 1.2538, Train: 1.1556, Val: 1.1568\n",
      "Epoch: 044, Loss: 1.2499, Train: 1.1442, Val: 1.1453\n",
      "Epoch: 045, Loss: 1.2479, Train: 1.1396, Val: 1.1407\n",
      "Epoch: 046, Loss: 1.2502, Train: 1.1364, Val: 1.1374\n",
      "Epoch: 047, Loss: 1.2487, Train: 1.1355, Val: 1.1366\n",
      "Epoch: 048, Loss: 1.2475, Train: 1.1356, Val: 1.1367\n",
      "Epoch: 049, Loss: 1.2496, Train: 1.1309, Val: 1.1319\n",
      "Epoch: 050, Loss: 1.2488, Train: 1.1263, Val: 1.1272\n",
      "Epoch: 051, Loss: 1.2469, Train: 1.1274, Val: 1.1281\n",
      "Epoch: 052, Loss: 1.2481, Train: 1.1274, Val: 1.1281\n",
      "Epoch: 053, Loss: 1.2480, Train: 1.1240, Val: 1.1246\n",
      "Epoch: 054, Loss: 1.2467, Train: 1.1217, Val: 1.1224\n",
      "Epoch: 055, Loss: 1.2475, Train: 1.1210, Val: 1.1217\n",
      "Epoch: 056, Loss: 1.2474, Train: 1.1204, Val: 1.1210\n",
      "Epoch: 057, Loss: 1.2461, Train: 1.1201, Val: 1.1206\n",
      "Epoch: 058, Loss: 1.2465, Train: 1.1189, Val: 1.1194\n",
      "Epoch: 059, Loss: 1.2467, Train: 1.1189, Val: 1.1196\n",
      "Epoch: 060, Loss: 1.2459, Train: 1.1208, Val: 1.1214\n",
      "Epoch: 061, Loss: 1.2463, Train: 1.1201, Val: 1.1207\n",
      "Epoch: 062, Loss: 1.2463, Train: 1.1174, Val: 1.1179\n",
      "Epoch: 063, Loss: 1.2456, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 064, Loss: 1.2459, Train: 1.1173, Val: 1.1176\n",
      "Epoch: 065, Loss: 1.2459, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 066, Loss: 1.2454, Train: 1.1165, Val: 1.1170\n",
      "Epoch: 067, Loss: 1.2457, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 068, Loss: 1.2456, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 069, Loss: 1.2452, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 070, Loss: 1.2454, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 071, Loss: 1.2453, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 072, Loss: 1.2451, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 073, Loss: 1.2453, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 074, Loss: 1.2451, Train: 1.1162, Val: 1.1166\n",
      "Epoch: 075, Loss: 1.2449, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 076, Loss: 1.2451, Train: 1.1162, Val: 1.1166\n",
      "Epoch: 077, Loss: 1.2449, Train: 1.1159, Val: 1.1164\n",
      "Epoch: 078, Loss: 1.2449, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 079, Loss: 1.2449, Train: 1.1159, Val: 1.1164\n",
      "Epoch: 080, Loss: 1.2447, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 081, Loss: 1.2448, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 082, Loss: 1.2447, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 083, Loss: 1.2446, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 084, Loss: 1.2446, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 085, Loss: 1.2446, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 086, Loss: 1.2445, Train: 1.1160, Val: 1.1163\n",
      "Epoch: 087, Loss: 1.2445, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 088, Loss: 1.2444, Train: 1.1157, Val: 1.1162\n",
      "Epoch: 089, Loss: 1.2444, Train: 1.1157, Val: 1.1162\n",
      "Epoch: 090, Loss: 1.2444, Train: 1.1157, Val: 1.1162\n",
      "Epoch: 091, Loss: 1.2443, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 092, Loss: 1.2443, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 093, Loss: 1.2442, Train: 1.1157, Val: 1.1162\n",
      "Epoch: 094, Loss: 1.2442, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 095, Loss: 1.2442, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 096, Loss: 1.2441, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 097, Loss: 1.2441, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 098, Loss: 1.2440, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 099, Loss: 1.2440, Train: 1.1165, Val: 1.1170\n",
      "Epoch: 100, Loss: 1.2440, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 101, Loss: 1.2439, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 102, Loss: 1.2439, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 103, Loss: 1.2438, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 104, Loss: 1.2438, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 105, Loss: 1.2438, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 106, Loss: 1.2437, Train: 1.1159, Val: 1.1164\n",
      "Epoch: 107, Loss: 1.2437, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 108, Loss: 1.2437, Train: 1.1158, Val: 1.1163\n",
      "Epoch: 109, Loss: 1.2436, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 110, Loss: 1.2436, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 111, Loss: 1.2435, Train: 1.1155, Val: 1.1159\n",
      "Epoch: 112, Loss: 1.2435, Train: 1.1154, Val: 1.1158\n",
      "Epoch: 113, Loss: 1.2435, Train: 1.1154, Val: 1.1158\n",
      "Epoch: 114, Loss: 1.2434, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 115, Loss: 1.2434, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 116, Loss: 1.2433, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 117, Loss: 1.2433, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 118, Loss: 1.2433, Train: 1.1158, Val: 1.1163\n",
      "Epoch: 119, Loss: 1.2432, Train: 1.1159, Val: 1.1164\n",
      "Epoch: 120, Loss: 1.2432, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 121, Loss: 1.2432, Train: 1.1157, Val: 1.1162\n",
      "Epoch: 122, Loss: 1.2431, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 123, Loss: 1.2431, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 124, Loss: 1.2431, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 125, Loss: 1.2430, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 126, Loss: 1.2430, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 127, Loss: 1.2430, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 128, Loss: 1.2429, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 129, Loss: 1.2429, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 130, Loss: 1.2429, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 131, Loss: 1.2428, Train: 1.1165, Val: 1.1170\n",
      "Epoch: 132, Loss: 1.2428, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 133, Loss: 1.2427, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 134, Loss: 1.2427, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 135, Loss: 1.2427, Train: 1.1158, Val: 1.1163\n",
      "Epoch: 136, Loss: 1.2426, Train: 1.1156, Val: 1.1161\n",
      "Epoch: 137, Loss: 1.2426, Train: 1.1155, Val: 1.1160\n",
      "Epoch: 138, Loss: 1.2426, Train: 1.1154, Val: 1.1159\n",
      "Epoch: 139, Loss: 1.2425, Train: 1.1153, Val: 1.1157\n",
      "Epoch: 140, Loss: 1.2425, Train: 1.1151, Val: 1.1156\n",
      "Epoch: 141, Loss: 1.2425, Train: 1.1150, Val: 1.1155\n",
      "Epoch: 142, Loss: 1.2424, Train: 1.1150, Val: 1.1155\n",
      "Epoch: 143, Loss: 1.2424, Train: 1.1149, Val: 1.1154\n",
      "Epoch: 144, Loss: 1.2424, Train: 1.1149, Val: 1.1153\n",
      "Epoch: 145, Loss: 1.2423, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 146, Loss: 1.2423, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 147, Loss: 1.2423, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 148, Loss: 1.2422, Train: 1.1149, Val: 1.1153\n",
      "Epoch: 149, Loss: 1.2422, Train: 1.1149, Val: 1.1154\n",
      "Epoch: 150, Loss: 1.2422, Train: 1.1150, Val: 1.1154\n",
      "Epoch: 151, Loss: 1.2421, Train: 1.1150, Val: 1.1154\n",
      "Epoch: 152, Loss: 1.2421, Train: 1.1150, Val: 1.1154\n",
      "Epoch: 153, Loss: 1.2421, Train: 1.1149, Val: 1.1153\n",
      "Epoch: 154, Loss: 1.2420, Train: 1.1149, Val: 1.1153\n",
      "Epoch: 155, Loss: 1.2420, Train: 1.1150, Val: 1.1154\n",
      "Epoch: 156, Loss: 1.2419, Train: 1.1148, Val: 1.1152\n",
      "Epoch: 157, Loss: 1.2419, Train: 1.1147, Val: 1.1151\n",
      "Epoch: 158, Loss: 1.2419, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 159, Loss: 1.2418, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 160, Loss: 1.2418, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 161, Loss: 1.2418, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 162, Loss: 1.2417, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 163, Loss: 1.2417, Train: 1.1146, Val: 1.1151\n",
      "Epoch: 164, Loss: 1.2417, Train: 1.1146, Val: 1.1150\n",
      "Epoch: 165, Loss: 1.2416, Train: 1.1146, Val: 1.1151\n",
      "Epoch: 166, Loss: 1.2416, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 167, Loss: 1.2416, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 168, Loss: 1.2415, Train: 1.1149, Val: 1.1155\n",
      "Epoch: 169, Loss: 1.2415, Train: 1.1151, Val: 1.1156\n",
      "Epoch: 170, Loss: 1.2414, Train: 1.1151, Val: 1.1157\n",
      "Epoch: 171, Loss: 1.2414, Train: 1.1151, Val: 1.1157\n",
      "Epoch: 172, Loss: 1.2414, Train: 1.1154, Val: 1.1161\n",
      "Epoch: 173, Loss: 1.2413, Train: 1.1158, Val: 1.1166\n",
      "Epoch: 174, Loss: 1.2413, Train: 1.1162, Val: 1.1170\n",
      "Epoch: 175, Loss: 1.2413, Train: 1.1167, Val: 1.1175\n",
      "Epoch: 176, Loss: 1.2413, Train: 1.1176, Val: 1.1184\n",
      "Epoch: 177, Loss: 1.2412, Train: 1.1181, Val: 1.1190\n",
      "Epoch: 178, Loss: 1.2412, Train: 1.1180, Val: 1.1189\n",
      "Epoch: 179, Loss: 1.2411, Train: 1.1180, Val: 1.1189\n",
      "Epoch: 180, Loss: 1.2411, Train: 1.1180, Val: 1.1189\n",
      "Epoch: 181, Loss: 1.2411, Train: 1.1182, Val: 1.1190\n",
      "Epoch: 182, Loss: 1.2410, Train: 1.1184, Val: 1.1192\n",
      "Epoch: 183, Loss: 1.2410, Train: 1.1182, Val: 1.1190\n",
      "Epoch: 184, Loss: 1.2410, Train: 1.1181, Val: 1.1189\n",
      "Epoch: 185, Loss: 1.2409, Train: 1.1181, Val: 1.1189\n",
      "Epoch: 186, Loss: 1.2409, Train: 1.1178, Val: 1.1186\n",
      "Epoch: 187, Loss: 1.2409, Train: 1.1181, Val: 1.1189\n",
      "Epoch: 188, Loss: 1.2408, Train: 1.1174, Val: 1.1182\n",
      "Epoch: 189, Loss: 1.2408, Train: 1.1181, Val: 1.1189\n",
      "Epoch: 190, Loss: 1.2408, Train: 1.1177, Val: 1.1185\n",
      "Epoch: 191, Loss: 1.2407, Train: 1.1175, Val: 1.1182\n",
      "Epoch: 192, Loss: 1.2407, Train: 1.1178, Val: 1.1186\n",
      "Epoch: 193, Loss: 1.2408, Train: 1.1177, Val: 1.1184\n",
      "Epoch: 194, Loss: 1.2407, Train: 1.1171, Val: 1.1178\n",
      "Epoch: 195, Loss: 1.2409, Train: 1.1183, Val: 1.1190\n",
      "Epoch: 196, Loss: 1.2412, Train: 1.1177, Val: 1.1184\n",
      "Epoch: 197, Loss: 1.2414, Train: 1.1162, Val: 1.1168\n",
      "Epoch: 198, Loss: 1.2409, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 199, Loss: 1.2409, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 200, Loss: 1.2411, Train: 1.1181, Val: 1.1187\n",
      "Epoch: 201, Loss: 1.2408, Train: 1.1204, Val: 1.1210\n",
      "Epoch: 202, Loss: 1.2412, Train: 1.1192, Val: 1.1198\n",
      "Epoch: 203, Loss: 1.2409, Train: 1.1176, Val: 1.1181\n",
      "Epoch: 204, Loss: 1.2404, Train: 1.1182, Val: 1.1187\n",
      "Epoch: 205, Loss: 1.2419, Train: 1.1202, Val: 1.1208\n",
      "Epoch: 206, Loss: 1.2404, Train: 1.1240, Val: 1.1246\n",
      "Epoch: 207, Loss: 1.2410, Train: 1.1262, Val: 1.1269\n",
      "Epoch: 208, Loss: 1.2410, Train: 1.1255, Val: 1.1263\n",
      "Epoch: 209, Loss: 1.2410, Train: 1.1240, Val: 1.1249\n",
      "Epoch: 210, Loss: 1.2409, Train: 1.1228, Val: 1.1236\n",
      "Epoch: 211, Loss: 1.2408, Train: 1.1207, Val: 1.1213\n",
      "Epoch: 212, Loss: 1.2405, Train: 1.1188, Val: 1.1194\n",
      "Epoch: 213, Loss: 1.2405, Train: 1.1200, Val: 1.1208\n",
      "Epoch: 214, Loss: 1.2404, Train: 1.1223, Val: 1.1234\n",
      "Epoch: 215, Loss: 1.2401, Train: 1.1235, Val: 1.1248\n",
      "Epoch: 216, Loss: 1.2401, Train: 1.1228, Val: 1.1241\n",
      "Epoch: 217, Loss: 1.2399, Train: 1.1221, Val: 1.1234\n",
      "Epoch: 218, Loss: 1.2400, Train: 1.1235, Val: 1.1248\n",
      "Epoch: 219, Loss: 1.2398, Train: 1.1258, Val: 1.1272\n",
      "Epoch: 220, Loss: 1.2398, Train: 1.1265, Val: 1.1279\n",
      "Epoch: 221, Loss: 1.2398, Train: 1.1256, Val: 1.1270\n",
      "Epoch: 222, Loss: 1.2396, Train: 1.1238, Val: 1.1252\n",
      "Epoch: 223, Loss: 1.2396, Train: 1.1222, Val: 1.1236\n",
      "Epoch: 224, Loss: 1.2394, Train: 1.1199, Val: 1.1211\n",
      "Epoch: 225, Loss: 1.2394, Train: 1.1179, Val: 1.1188\n",
      "Epoch: 226, Loss: 1.2393, Train: 1.1160, Val: 1.1166\n",
      "Epoch: 227, Loss: 1.2392, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 228, Loss: 1.2391, Train: 1.1178, Val: 1.1182\n",
      "Epoch: 229, Loss: 1.2391, Train: 1.1186, Val: 1.1191\n",
      "Epoch: 230, Loss: 1.2391, Train: 1.1189, Val: 1.1197\n",
      "Epoch: 231, Loss: 1.2390, Train: 1.1184, Val: 1.1188\n",
      "Epoch: 232, Loss: 1.2389, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 233, Loss: 1.2388, Train: 1.1147, Val: 1.1150\n",
      "Epoch: 234, Loss: 1.2387, Train: 1.1143, Val: 1.1148\n",
      "Epoch: 235, Loss: 1.2387, Train: 1.1148, Val: 1.1153\n",
      "Epoch: 236, Loss: 1.2386, Train: 1.1150, Val: 1.1156\n",
      "Epoch: 237, Loss: 1.2385, Train: 1.1153, Val: 1.1157\n",
      "Epoch: 238, Loss: 1.2386, Train: 1.1205, Val: 1.1212\n",
      "Epoch: 239, Loss: 1.2391, Train: 1.1263, Val: 1.1270\n",
      "Epoch: 240, Loss: 1.2389, Train: 1.1439, Val: 1.1446\n",
      "Epoch: 241, Loss: 1.2389, Train: 1.1774, Val: 1.1758\n",
      "Epoch: 242, Loss: 1.2388, Train: 1.1779, Val: 1.1764\n",
      "Epoch: 243, Loss: 1.2386, Train: 1.1458, Val: 1.1467\n",
      "Epoch: 244, Loss: 1.2385, Train: 1.1511, Val: 1.1521\n",
      "Epoch: 245, Loss: 1.2384, Train: 1.1549, Val: 1.1558\n",
      "Epoch: 246, Loss: 1.2384, Train: 1.1577, Val: 1.1587\n",
      "Epoch: 247, Loss: 1.2383, Train: 1.1514, Val: 1.1526\n",
      "Epoch: 248, Loss: 1.2383, Train: 1.1456, Val: 1.1467\n",
      "Epoch: 249, Loss: 1.2383, Train: 1.1363, Val: 1.1372\n",
      "Epoch: 250, Loss: 1.2382, Train: 1.1324, Val: 1.1332\n",
      "Epoch: 251, Loss: 1.2381, Train: 1.1287, Val: 1.1296\n",
      "Epoch: 252, Loss: 1.2380, Train: 1.1276, Val: 1.1285\n",
      "Epoch: 253, Loss: 1.2380, Train: 1.1267, Val: 1.1276\n",
      "Epoch: 254, Loss: 1.2379, Train: 1.1247, Val: 1.1257\n",
      "Epoch: 255, Loss: 1.2378, Train: 1.1244, Val: 1.1255\n",
      "Epoch: 256, Loss: 1.2378, Train: 1.1254, Val: 1.1265\n",
      "Epoch: 257, Loss: 1.2378, Train: 1.1254, Val: 1.1265\n",
      "Epoch: 258, Loss: 1.2377, Train: 1.1246, Val: 1.1258\n",
      "Epoch: 259, Loss: 1.2375, Train: 1.1245, Val: 1.1255\n",
      "Epoch: 260, Loss: 1.2375, Train: 1.1236, Val: 1.1243\n",
      "Epoch: 261, Loss: 1.2375, Train: 1.1217, Val: 1.1225\n",
      "Epoch: 262, Loss: 1.2373, Train: 1.1225, Val: 1.1235\n",
      "Epoch: 263, Loss: 1.2373, Train: 1.1257, Val: 1.1269\n",
      "Epoch: 264, Loss: 1.2373, Train: 1.1285, Val: 1.1299\n",
      "Epoch: 265, Loss: 1.2372, Train: 1.1267, Val: 1.1280\n",
      "Epoch: 266, Loss: 1.2370, Train: 1.1217, Val: 1.1225\n",
      "Epoch: 267, Loss: 1.2370, Train: 1.1207, Val: 1.1213\n",
      "Epoch: 268, Loss: 1.2369, Train: 1.1217, Val: 1.1224\n",
      "Epoch: 269, Loss: 1.2368, Train: 1.1200, Val: 1.1208\n",
      "Epoch: 270, Loss: 1.2368, Train: 1.1207, Val: 1.1218\n",
      "Epoch: 271, Loss: 1.2367, Train: 1.1256, Val: 1.1270\n",
      "Epoch: 272, Loss: 1.2366, Train: 1.1277, Val: 1.1289\n",
      "Epoch: 273, Loss: 1.2365, Train: 1.1248, Val: 1.1262\n",
      "Epoch: 274, Loss: 1.2366, Train: 1.1214, Val: 1.1227\n",
      "Epoch: 275, Loss: 1.2365, Train: 1.1207, Val: 1.1219\n",
      "Epoch: 276, Loss: 1.2365, Train: 1.1213, Val: 1.1225\n",
      "Epoch: 277, Loss: 1.2364, Train: 1.1205, Val: 1.1218\n",
      "Epoch: 278, Loss: 1.2362, Train: 1.1214, Val: 1.1227\n",
      "Epoch: 279, Loss: 1.2362, Train: 1.1227, Val: 1.1238\n",
      "Epoch: 280, Loss: 1.2360, Train: 1.1244, Val: 1.1254\n",
      "Epoch: 281, Loss: 1.2360, Train: 1.1241, Val: 1.1253\n",
      "Epoch: 282, Loss: 1.2360, Train: 1.1247, Val: 1.1257\n",
      "Epoch: 283, Loss: 1.2357, Train: 1.1248, Val: 1.1257\n",
      "Epoch: 284, Loss: 1.2362, Train: 1.1258, Val: 1.1268\n",
      "Epoch: 285, Loss: 1.2371, Train: 1.1818, Val: 1.1808\n",
      "Epoch: 286, Loss: 1.2370, Train: 1.1843, Val: 1.1830\n",
      "Epoch: 287, Loss: 1.2370, Train: 1.1501, Val: 1.1500\n",
      "Epoch: 288, Loss: 1.2373, Train: 1.1470, Val: 1.1473\n",
      "Epoch: 289, Loss: 1.2367, Train: 1.1530, Val: 1.1542\n",
      "Epoch: 290, Loss: 1.2366, Train: 1.1563, Val: 1.1581\n",
      "Epoch: 291, Loss: 1.2377, Train: 1.1543, Val: 1.1555\n",
      "Epoch: 292, Loss: 1.2367, Train: 1.1538, Val: 1.1548\n",
      "Epoch: 293, Loss: 1.2368, Train: 1.1564, Val: 1.1578\n",
      "Epoch: 294, Loss: 1.2367, Train: 1.1593, Val: 1.1609\n",
      "Epoch: 295, Loss: 1.2367, Train: 1.1506, Val: 1.1519\n",
      "Epoch: 296, Loss: 1.2364, Train: 1.1398, Val: 1.1409\n",
      "Epoch: 297, Loss: 1.2363, Train: 1.1389, Val: 1.1401\n",
      "Epoch: 298, Loss: 1.2360, Train: 1.1411, Val: 1.1423\n",
      "Epoch: 299, Loss: 1.2361, Train: 1.1390, Val: 1.1401\n",
      "Epoch: 300, Loss: 1.2363, Train: 1.1432, Val: 1.1445\n",
      "Epoch: 301, Loss: 1.2363, Train: 1.1438, Val: 1.1448\n",
      "Epoch: 302, Loss: 1.2361, Train: 1.1463, Val: 1.1475\n",
      "Epoch: 303, Loss: 1.2363, Train: 1.1531, Val: 1.1544\n",
      "Epoch: 304, Loss: 1.2360, Train: 1.1467, Val: 1.1481\n",
      "Epoch: 305, Loss: 1.2356, Train: 1.1395, Val: 1.1409\n",
      "Epoch: 306, Loss: 1.2359, Train: 1.1401, Val: 1.1416\n",
      "Epoch: 307, Loss: 1.2358, Train: 1.1381, Val: 1.1395\n",
      "Epoch: 308, Loss: 1.2363, Train: 1.1400, Val: 1.1417\n",
      "Epoch: 309, Loss: 1.2359, Train: 1.1427, Val: 1.1445\n",
      "Epoch: 310, Loss: 1.2364, Train: 1.1345, Val: 1.1363\n",
      "Epoch: 311, Loss: 1.2363, Train: 1.1251, Val: 1.1267\n",
      "Epoch: 312, Loss: 1.2355, Train: 1.1224, Val: 1.1238\n",
      "Epoch: 313, Loss: 1.2352, Train: 1.1225, Val: 1.1237\n",
      "Epoch: 314, Loss: 1.2360, Train: 1.1234, Val: 1.1246\n",
      "Epoch: 315, Loss: 1.2360, Train: 1.1259, Val: 1.1274\n",
      "Epoch: 316, Loss: 1.2368, Train: 1.1272, Val: 1.1286\n",
      "Epoch: 317, Loss: 1.2365, Train: 1.1247, Val: 1.1262\n",
      "Epoch: 318, Loss: 1.2376, Train: 1.1299, Val: 1.1312\n",
      "Epoch: 319, Loss: 1.2372, Train: 1.1393, Val: 1.1409\n",
      "Epoch: 320, Loss: 1.2367, Train: 1.1443, Val: 1.1458\n",
      "Epoch: 321, Loss: 1.2365, Train: 1.1454, Val: 1.1469\n",
      "Epoch: 322, Loss: 1.2356, Train: 1.1409, Val: 1.1425\n",
      "Epoch: 323, Loss: 1.2360, Train: 1.1365, Val: 1.1380\n",
      "Epoch: 324, Loss: 1.2357, Train: 1.1415, Val: 1.1433\n",
      "Epoch: 325, Loss: 1.2355, Train: 1.1302, Val: 1.1315\n",
      "Epoch: 326, Loss: 1.2368, Train: 1.1285, Val: 1.1299\n",
      "Epoch: 327, Loss: 1.2360, Train: 1.1274, Val: 1.1286\n",
      "Epoch: 328, Loss: 1.2356, Train: 1.1290, Val: 1.1302\n",
      "Epoch: 329, Loss: 1.2355, Train: 1.1302, Val: 1.1315\n",
      "Epoch: 330, Loss: 1.2351, Train: 1.1316, Val: 1.1328\n",
      "Epoch: 331, Loss: 1.2345, Train: 1.1306, Val: 1.1317\n",
      "Epoch: 332, Loss: 1.2349, Train: 1.1309, Val: 1.1319\n",
      "Epoch: 333, Loss: 1.2348, Train: 1.1365, Val: 1.1376\n",
      "Epoch: 334, Loss: 1.2352, Train: 1.1332, Val: 1.1343\n",
      "Epoch: 335, Loss: 1.2345, Train: 1.1295, Val: 1.1305\n",
      "Epoch: 336, Loss: 1.2346, Train: 1.1358, Val: 1.1367\n",
      "Epoch: 337, Loss: 1.2345, Train: 1.1378, Val: 1.1387\n",
      "Epoch: 338, Loss: 1.2343, Train: 1.1367, Val: 1.1375\n",
      "Epoch: 339, Loss: 1.2338, Train: 1.1377, Val: 1.1385\n",
      "Epoch: 340, Loss: 1.2339, Train: 1.1384, Val: 1.1391\n",
      "Epoch: 341, Loss: 1.2338, Train: 1.1448, Val: 1.1454\n",
      "Epoch: 342, Loss: 1.2335, Train: 1.1466, Val: 1.1471\n",
      "Epoch: 343, Loss: 1.2333, Train: 1.1432, Val: 1.1437\n",
      "Epoch: 344, Loss: 1.2331, Train: 1.1441, Val: 1.1448\n",
      "Epoch: 345, Loss: 1.2330, Train: 1.1471, Val: 1.1478\n",
      "Epoch: 346, Loss: 1.2329, Train: 1.1412, Val: 1.1420\n",
      "Epoch: 347, Loss: 1.2327, Train: 1.1351, Val: 1.1361\n",
      "Epoch: 348, Loss: 1.2329, Train: 1.1727, Val: 1.1736\n",
      "Epoch: 349, Loss: 1.2329, Train: 1.4881, Val: 1.4829\n",
      "Epoch: 350, Loss: 1.2330, Train: 1.4841, Val: 1.4789\n",
      "Epoch: 351, Loss: 1.2337, Train: 1.4650, Val: 1.4603\n",
      "Epoch: 352, Loss: 1.2335, Train: 1.3923, Val: 1.3876\n",
      "Epoch: 353, Loss: 1.2331, Train: 1.2057, Val: 1.2038\n",
      "Epoch: 354, Loss: 1.2326, Train: 1.1836, Val: 1.1849\n",
      "Epoch: 355, Loss: 1.2326, Train: 1.1756, Val: 1.1766\n",
      "Epoch: 356, Loss: 1.2328, Train: 1.1690, Val: 1.1699\n",
      "Epoch: 357, Loss: 1.2324, Train: 1.1623, Val: 1.1628\n",
      "Epoch: 358, Loss: 1.2321, Train: 1.1535, Val: 1.1550\n",
      "Epoch: 359, Loss: 1.2322, Train: 1.1357, Val: 1.1368\n",
      "Epoch: 360, Loss: 1.2318, Train: 1.1270, Val: 1.1281\n",
      "Epoch: 361, Loss: 1.2320, Train: 1.1275, Val: 1.1285\n",
      "Epoch: 362, Loss: 1.2315, Train: 1.1344, Val: 1.1356\n",
      "Epoch: 363, Loss: 1.2314, Train: 1.1321, Val: 1.1332\n",
      "Epoch: 364, Loss: 1.2314, Train: 1.1271, Val: 1.1282\n",
      "Epoch: 365, Loss: 1.2312, Train: 1.1232, Val: 1.1242\n",
      "Epoch: 366, Loss: 1.2311, Train: 1.1227, Val: 1.1236\n",
      "Epoch: 367, Loss: 1.2307, Train: 1.1252, Val: 1.1262\n",
      "Epoch: 368, Loss: 1.2307, Train: 1.1235, Val: 1.1238\n",
      "Epoch: 369, Loss: 1.2306, Train: 1.1210, Val: 1.1212\n",
      "Epoch: 370, Loss: 1.2307, Train: 1.1380, Val: 1.1392\n",
      "Epoch: 371, Loss: 1.2307, Train: 1.1616, Val: 1.1620\n",
      "Epoch: 372, Loss: 1.2313, Train: 1.1425, Val: 1.1427\n",
      "Epoch: 373, Loss: 1.2321, Train: 1.1323, Val: 1.1326\n",
      "Epoch: 374, Loss: 1.2346, Train: 1.1819, Val: 1.1834\n",
      "Epoch: 375, Loss: 1.2369, Train: 1.1381, Val: 1.1391\n",
      "Epoch: 376, Loss: 1.2400, Train: 1.3055, Val: 1.2997\n",
      "Epoch: 377, Loss: 1.2366, Train: 1.4781, Val: 1.4732\n",
      "Epoch: 378, Loss: 1.2323, Train: 1.4914, Val: 1.4863\n",
      "Epoch: 379, Loss: 1.2313, Train: 1.4748, Val: 1.4698\n",
      "Epoch: 380, Loss: 1.2343, Train: 1.4546, Val: 1.4496\n",
      "Epoch: 381, Loss: 1.2344, Train: 1.4535, Val: 1.4487\n",
      "Epoch: 382, Loss: 1.2322, Train: 1.4376, Val: 1.4326\n",
      "Epoch: 383, Loss: 1.2321, Train: 1.3442, Val: 1.3380\n",
      "Epoch: 384, Loss: 1.2323, Train: 1.2043, Val: 1.2038\n",
      "Epoch: 385, Loss: 1.2319, Train: 1.1974, Val: 1.1969\n",
      "Epoch: 386, Loss: 1.2318, Train: 1.1933, Val: 1.1928\n",
      "Epoch: 387, Loss: 1.2309, Train: 1.1960, Val: 1.1959\n",
      "Epoch: 388, Loss: 1.2313, Train: 1.1836, Val: 1.1836\n",
      "Epoch: 389, Loss: 1.2314, Train: 1.1626, Val: 1.1636\n",
      "Epoch: 390, Loss: 1.2300, Train: 1.1460, Val: 1.1471\n",
      "Epoch: 391, Loss: 1.2306, Train: 1.1456, Val: 1.1465\n",
      "Epoch: 392, Loss: 1.2300, Train: 1.1473, Val: 1.1481\n",
      "Epoch: 393, Loss: 1.2296, Train: 1.1468, Val: 1.1476\n",
      "Epoch: 394, Loss: 1.2299, Train: 1.1480, Val: 1.1489\n",
      "Epoch: 395, Loss: 1.2291, Train: 1.1439, Val: 1.1448\n",
      "Epoch: 396, Loss: 1.2292, Train: 1.1575, Val: 1.1583\n",
      "Epoch: 397, Loss: 1.2301, Train: 1.2014, Val: 1.2010\n",
      "Epoch: 398, Loss: 1.2298, Train: 1.1251, Val: 1.1255\n",
      "Epoch: 399, Loss: 1.2336, Train: 1.1727, Val: 1.1742\n",
      "Epoch: 400, Loss: 1.2324, Train: 1.2026, Val: 1.2036\n",
      "Epoch: 401, Loss: 1.2336, Train: 1.1942, Val: 1.1953\n",
      "Epoch: 402, Loss: 1.2335, Train: 1.2008, Val: 1.2017\n",
      "Epoch: 403, Loss: 1.2320, Train: 1.1790, Val: 1.1803\n",
      "Epoch: 404, Loss: 1.2325, Train: 1.1453, Val: 1.1467\n",
      "Epoch: 405, Loss: 1.2299, Train: 1.1314, Val: 1.1316\n",
      "Epoch: 406, Loss: 1.2303, Train: 1.1299, Val: 1.1298\n",
      "Epoch: 407, Loss: 1.2297, Train: 1.1299, Val: 1.1297\n",
      "Epoch: 408, Loss: 1.2306, Train: 1.1567, Val: 1.1565\n",
      "Epoch: 409, Loss: 1.2305, Train: 1.1436, Val: 1.1437\n",
      "Epoch: 410, Loss: 1.2302, Train: 1.1666, Val: 1.1666\n",
      "Epoch: 411, Loss: 1.2298, Train: 1.1349, Val: 1.1349\n",
      "Epoch: 412, Loss: 1.2291, Train: 1.1406, Val: 1.1405\n",
      "Epoch: 413, Loss: 1.2287, Train: 1.1439, Val: 1.1440\n",
      "Epoch: 414, Loss: 1.2283, Train: 1.1473, Val: 1.1476\n",
      "Epoch: 415, Loss: 1.2282, Train: 1.1662, Val: 1.1670\n",
      "Epoch: 416, Loss: 1.2281, Train: 1.1711, Val: 1.1721\n",
      "Epoch: 417, Loss: 1.2280, Train: 1.1709, Val: 1.1720\n",
      "Epoch: 418, Loss: 1.2276, Train: 1.1476, Val: 1.1486\n",
      "Epoch: 419, Loss: 1.2274, Train: 1.1396, Val: 1.1399\n",
      "Epoch: 420, Loss: 1.2272, Train: 1.1290, Val: 1.1289\n",
      "Epoch: 421, Loss: 1.2269, Train: 1.1329, Val: 1.1331\n",
      "Epoch: 422, Loss: 1.2266, Train: 1.1415, Val: 1.1424\n",
      "Epoch: 423, Loss: 1.2268, Train: 1.1359, Val: 1.1370\n",
      "Epoch: 424, Loss: 1.2265, Train: 1.1394, Val: 1.1406\n",
      "Epoch: 425, Loss: 1.2266, Train: 1.1322, Val: 1.1332\n",
      "Epoch: 426, Loss: 1.2269, Train: 1.1684, Val: 1.1689\n",
      "Epoch: 427, Loss: 1.2294, Train: 1.1613, Val: 1.1621\n",
      "Epoch: 428, Loss: 1.2385, Train: 1.4404, Val: 1.4427\n",
      "Epoch: 429, Loss: 1.2743, Train: 1.1852, Val: 1.1874\n",
      "Epoch: 430, Loss: 1.2753, Train: 1.1700, Val: 1.1710\n",
      "Epoch: 431, Loss: 1.2580, Train: 1.1818, Val: 1.1820\n",
      "Epoch: 432, Loss: 1.2343, Train: 1.1561, Val: 1.1562\n",
      "Epoch: 433, Loss: 1.2489, Train: 1.2197, Val: 1.2198\n",
      "Epoch: 434, Loss: 1.2478, Train: 1.1320, Val: 1.1326\n",
      "Epoch: 435, Loss: 1.2381, Train: 1.1577, Val: 1.1587\n",
      "Epoch: 436, Loss: 1.2381, Train: 1.2316, Val: 1.2309\n",
      "Epoch: 437, Loss: 1.2437, Train: 1.1828, Val: 1.1819\n",
      "Epoch: 438, Loss: 1.2345, Train: 1.1770, Val: 1.1773\n",
      "Epoch: 439, Loss: 1.2386, Train: 1.1729, Val: 1.1738\n",
      "Epoch: 440, Loss: 1.2379, Train: 1.1739, Val: 1.1745\n",
      "Epoch: 441, Loss: 1.2322, Train: 1.1516, Val: 1.1523\n",
      "Epoch: 442, Loss: 1.2375, Train: 1.1599, Val: 1.1606\n",
      "Epoch: 443, Loss: 1.2335, Train: 1.1431, Val: 1.1440\n",
      "Epoch: 444, Loss: 1.2319, Train: 1.1597, Val: 1.1608\n",
      "Epoch: 445, Loss: 1.2347, Train: 1.1698, Val: 1.1707\n",
      "Epoch: 446, Loss: 1.2315, Train: 1.1675, Val: 1.1687\n",
      "Epoch: 447, Loss: 1.2317, Train: 1.1372, Val: 1.1384\n",
      "Epoch: 448, Loss: 1.2326, Train: 1.1375, Val: 1.1386\n",
      "Epoch: 449, Loss: 1.2299, Train: 1.1471, Val: 1.1483\n",
      "Epoch: 450, Loss: 1.2309, Train: 1.1366, Val: 1.1372\n",
      "Epoch: 451, Loss: 1.2308, Train: 1.1449, Val: 1.1453\n",
      "Epoch: 452, Loss: 1.2293, Train: 1.1720, Val: 1.1728\n",
      "Epoch: 453, Loss: 1.2300, Train: 1.1781, Val: 1.1780\n",
      "Epoch: 454, Loss: 1.2290, Train: 1.2383, Val: 1.2366\n",
      "Epoch: 455, Loss: 1.2285, Train: 1.2151, Val: 1.2143\n",
      "Epoch: 456, Loss: 1.2299, Train: 1.1829, Val: 1.1824\n",
      "Epoch: 457, Loss: 1.2297, Train: 1.1791, Val: 1.1787\n",
      "Epoch: 458, Loss: 1.2308, Train: 1.1842, Val: 1.1838\n",
      "Epoch: 459, Loss: 1.2301, Train: 1.1900, Val: 1.1893\n",
      "Epoch: 460, Loss: 1.2295, Train: 1.1820, Val: 1.1829\n",
      "Epoch: 461, Loss: 1.2294, Train: 1.1476, Val: 1.1483\n",
      "Epoch: 462, Loss: 1.2295, Train: 1.1388, Val: 1.1397\n",
      "Epoch: 463, Loss: 1.2286, Train: 1.1340, Val: 1.1348\n",
      "Epoch: 464, Loss: 1.2284, Train: 1.1476, Val: 1.1482\n",
      "Epoch: 465, Loss: 1.2278, Train: 1.1466, Val: 1.1475\n",
      "Epoch: 466, Loss: 1.2270, Train: 1.1494, Val: 1.1511\n",
      "Epoch: 467, Loss: 1.2288, Train: 1.1559, Val: 1.1562\n",
      "Epoch: 468, Loss: 1.2312, Train: 1.1637, Val: 1.1658\n",
      "Epoch: 469, Loss: 1.2296, Train: 1.1900, Val: 1.1923\n",
      "Epoch: 470, Loss: 1.2329, Train: 1.1936, Val: 1.1960\n",
      "Epoch: 471, Loss: 1.2328, Train: 1.1973, Val: 1.1998\n",
      "Epoch: 472, Loss: 1.2317, Train: 1.1984, Val: 1.2009\n",
      "Epoch: 473, Loss: 1.2311, Train: 1.1905, Val: 1.1930\n",
      "Epoch: 474, Loss: 1.2296, Train: 1.1855, Val: 1.1880\n",
      "Epoch: 475, Loss: 1.2296, Train: 1.2014, Val: 1.2039\n",
      "Epoch: 476, Loss: 1.2314, Train: 1.2220, Val: 1.2243\n",
      "Epoch: 477, Loss: 1.2294, Train: 1.2409, Val: 1.2427\n",
      "Epoch: 478, Loss: 1.2290, Train: 1.2363, Val: 1.2381\n",
      "Epoch: 479, Loss: 1.2283, Train: 1.2299, Val: 1.2316\n",
      "Epoch: 480, Loss: 1.2278, Train: 1.2359, Val: 1.2377\n",
      "Epoch: 481, Loss: 1.2280, Train: 1.2487, Val: 1.2505\n",
      "Epoch: 482, Loss: 1.2274, Train: 1.2624, Val: 1.2642\n",
      "Epoch: 483, Loss: 1.2269, Train: 1.2789, Val: 1.2808\n",
      "Epoch: 484, Loss: 1.2265, Train: 1.2908, Val: 1.2927\n",
      "Epoch: 485, Loss: 1.2258, Train: 1.2857, Val: 1.2875\n",
      "Epoch: 486, Loss: 1.2254, Train: 1.2828, Val: 1.2843\n",
      "Epoch: 487, Loss: 1.2252, Train: 1.2842, Val: 1.2857\n",
      "Epoch: 488, Loss: 1.2246, Train: 1.2697, Val: 1.2713\n",
      "Epoch: 489, Loss: 1.2240, Train: 1.2455, Val: 1.2467\n",
      "Epoch: 490, Loss: 1.2238, Train: 1.2329, Val: 1.2344\n",
      "Epoch: 491, Loss: 1.2236, Train: 1.2162, Val: 1.2180\n",
      "Epoch: 492, Loss: 1.2228, Train: 1.2027, Val: 1.2049\n",
      "Epoch: 493, Loss: 1.2221, Train: 1.1867, Val: 1.1890\n",
      "Epoch: 494, Loss: 1.2220, Train: 1.1714, Val: 1.1739\n",
      "Epoch: 495, Loss: 1.2216, Train: 1.1587, Val: 1.1602\n",
      "Epoch: 496, Loss: 1.2212, Train: 1.1591, Val: 1.1607\n",
      "Epoch: 497, Loss: 1.2208, Train: 1.1498, Val: 1.1519\n",
      "Epoch: 498, Loss: 1.2207, Train: 1.1551, Val: 1.1547\n",
      "Epoch: 499, Loss: 1.2203, Train: 1.1348, Val: 1.1364\n",
      "Epoch: 500, Loss: 1.2204, Train: 1.1770, Val: 1.1771\n",
      "Test RMSE: 1.1462\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.551714       3.583673\n",
      "std      1727.484387     741.673176       0.258405       1.116938\n",
      "min         0.000000       0.000000       2.046153       1.000000\n",
      "25%      1500.000000     259.000000       3.520392       3.000000\n",
      "50%      3066.000000     693.000000       3.520392       4.000000\n",
      "75%      4472.000000    1292.000000       3.660580       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1007.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80909243415533\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  2\n",
      "Kernel:  gd\n",
      "Model Name:  KPGINPlus\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 14.4641, Train: 3.1406, Val: 3.1427\n",
      "Epoch: 002, Loss: 11.5840, Train: 3.1465, Val: 3.1496\n",
      "Epoch: 003, Loss: 8.2741, Train: 2.7842, Val: 2.7850\n",
      "Epoch: 004, Loss: 3.9360, Train: 2.3396, Val: 2.3389\n",
      "Epoch: 005, Loss: 3.1412, Train: 1.9173, Val: 1.9160\n",
      "Epoch: 006, Loss: 2.9764, Train: 1.8653, Val: 1.8657\n",
      "Epoch: 007, Loss: 1.4379, Train: 1.8006, Val: 1.8014\n",
      "Epoch: 008, Loss: 1.4981, Train: 1.6767, Val: 1.6773\n",
      "Epoch: 009, Loss: 1.4713, Train: 1.6041, Val: 1.6044\n",
      "Epoch: 010, Loss: 1.4588, Train: 1.5975, Val: 1.5978\n",
      "Epoch: 011, Loss: 1.4008, Train: 1.6622, Val: 1.6628\n",
      "Epoch: 012, Loss: 1.3011, Train: 1.7488, Val: 1.7497\n",
      "Epoch: 013, Loss: 1.3007, Train: 1.7872, Val: 1.7884\n",
      "Epoch: 014, Loss: 1.3202, Train: 1.7662, Val: 1.7676\n",
      "Epoch: 015, Loss: 1.3249, Train: 1.7064, Val: 1.7078\n",
      "Epoch: 016, Loss: 1.3327, Train: 1.6686, Val: 1.6700\n",
      "Epoch: 017, Loss: 1.3133, Train: 1.6621, Val: 1.6637\n",
      "Epoch: 018, Loss: 1.2759, Train: 1.6638, Val: 1.6655\n",
      "Epoch: 019, Loss: 1.2649, Train: 1.6370, Val: 1.6388\n",
      "Epoch: 020, Loss: 1.2753, Train: 1.5673, Val: 1.5691\n",
      "Epoch: 021, Loss: 1.2820, Train: 1.4842, Val: 1.4860\n",
      "Epoch: 022, Loss: 1.2832, Train: 1.4140, Val: 1.4158\n",
      "Epoch: 023, Loss: 1.2832, Train: 1.3743, Val: 1.3761\n",
      "Epoch: 024, Loss: 1.2729, Train: 1.3574, Val: 1.3592\n",
      "Epoch: 025, Loss: 1.2606, Train: 1.3375, Val: 1.3393\n",
      "Epoch: 026, Loss: 1.2595, Train: 1.3160, Val: 1.3179\n",
      "Epoch: 027, Loss: 1.2574, Train: 1.2939, Val: 1.2958\n",
      "Epoch: 028, Loss: 1.2591, Train: 1.2846, Val: 1.2865\n",
      "Epoch: 029, Loss: 1.2599, Train: 1.2861, Val: 1.2881\n",
      "Epoch: 030, Loss: 1.2563, Train: 1.2874, Val: 1.2894\n",
      "Epoch: 031, Loss: 1.2545, Train: 1.2763, Val: 1.2783\n",
      "Epoch: 032, Loss: 1.2535, Train: 1.2508, Val: 1.2529\n",
      "Epoch: 033, Loss: 1.2515, Train: 1.2228, Val: 1.2248\n",
      "Epoch: 034, Loss: 1.2517, Train: 1.2038, Val: 1.2058\n",
      "Epoch: 035, Loss: 1.2529, Train: 1.1947, Val: 1.1966\n",
      "Epoch: 036, Loss: 1.2519, Train: 1.1891, Val: 1.1910\n",
      "Epoch: 037, Loss: 1.2511, Train: 1.1804, Val: 1.1823\n",
      "Epoch: 038, Loss: 1.2511, Train: 1.1665, Val: 1.1682\n",
      "Epoch: 039, Loss: 1.2499, Train: 1.1528, Val: 1.1543\n",
      "Epoch: 040, Loss: 1.2496, Train: 1.1442, Val: 1.1456\n",
      "Epoch: 041, Loss: 1.2504, Train: 1.1417, Val: 1.1431\n",
      "Epoch: 042, Loss: 1.2498, Train: 1.1434, Val: 1.1447\n",
      "Epoch: 043, Loss: 1.2492, Train: 1.1445, Val: 1.1458\n",
      "Epoch: 044, Loss: 1.2488, Train: 1.1418, Val: 1.1431\n",
      "Epoch: 045, Loss: 1.2479, Train: 1.1372, Val: 1.1385\n",
      "Epoch: 046, Loss: 1.2477, Train: 1.1341, Val: 1.1353\n",
      "Epoch: 047, Loss: 1.2484, Train: 1.1332, Val: 1.1344\n",
      "Epoch: 048, Loss: 1.2484, Train: 1.1326, Val: 1.1338\n",
      "Epoch: 049, Loss: 1.2483, Train: 1.1296, Val: 1.1307\n",
      "Epoch: 050, Loss: 1.2481, Train: 1.1250, Val: 1.1260\n",
      "Epoch: 051, Loss: 1.2473, Train: 1.1216, Val: 1.1224\n",
      "Epoch: 052, Loss: 1.2470, Train: 1.1201, Val: 1.1208\n",
      "Epoch: 053, Loss: 1.2471, Train: 1.1195, Val: 1.1201\n",
      "Epoch: 054, Loss: 1.2471, Train: 1.1190, Val: 1.1196\n",
      "Epoch: 055, Loss: 1.2473, Train: 1.1186, Val: 1.1192\n",
      "Epoch: 056, Loss: 1.2472, Train: 1.1183, Val: 1.1189\n",
      "Epoch: 057, Loss: 1.2469, Train: 1.1182, Val: 1.1187\n",
      "Epoch: 058, Loss: 1.2467, Train: 1.1180, Val: 1.1185\n",
      "Epoch: 059, Loss: 1.2465, Train: 1.1178, Val: 1.1184\n",
      "Epoch: 060, Loss: 1.2464, Train: 1.1178, Val: 1.1183\n",
      "Epoch: 061, Loss: 1.2466, Train: 1.1175, Val: 1.1180\n",
      "Epoch: 062, Loss: 1.2465, Train: 1.1171, Val: 1.1176\n",
      "Epoch: 063, Loss: 1.2464, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 064, Loss: 1.2463, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 065, Loss: 1.2461, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 066, Loss: 1.2461, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 067, Loss: 1.2461, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 068, Loss: 1.2461, Train: 1.1170, Val: 1.1173\n",
      "Epoch: 069, Loss: 1.2461, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 070, Loss: 1.2460, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 071, Loss: 1.2459, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 072, Loss: 1.2459, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 073, Loss: 1.2458, Train: 1.1164, Val: 1.1167\n",
      "Epoch: 074, Loss: 1.2458, Train: 1.1163, Val: 1.1166\n",
      "Epoch: 075, Loss: 1.2457, Train: 1.1162, Val: 1.1165\n",
      "Epoch: 076, Loss: 1.2456, Train: 1.1162, Val: 1.1164\n",
      "Epoch: 077, Loss: 1.2457, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 078, Loss: 1.2456, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 079, Loss: 1.2456, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 080, Loss: 1.2455, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 081, Loss: 1.2455, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 082, Loss: 1.2455, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 083, Loss: 1.2454, Train: 1.1162, Val: 1.1164\n",
      "Epoch: 084, Loss: 1.2454, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 085, Loss: 1.2454, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 086, Loss: 1.2454, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 087, Loss: 1.2453, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 088, Loss: 1.2453, Train: 1.1162, Val: 1.1164\n",
      "Epoch: 089, Loss: 1.2453, Train: 1.1162, Val: 1.1165\n",
      "Epoch: 090, Loss: 1.2452, Train: 1.1162, Val: 1.1165\n",
      "Epoch: 091, Loss: 1.2452, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 092, Loss: 1.2452, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 093, Loss: 1.2452, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 094, Loss: 1.2451, Train: 1.1162, Val: 1.1164\n",
      "Epoch: 095, Loss: 1.2451, Train: 1.1162, Val: 1.1165\n",
      "Epoch: 096, Loss: 1.2451, Train: 1.1164, Val: 1.1167\n",
      "Epoch: 097, Loss: 1.2451, Train: 1.1165, Val: 1.1167\n",
      "Epoch: 098, Loss: 1.2450, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 099, Loss: 1.2450, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 100, Loss: 1.2450, Train: 1.1167, Val: 1.1169\n",
      "Epoch: 101, Loss: 1.2449, Train: 1.1167, Val: 1.1169\n",
      "Epoch: 102, Loss: 1.2449, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 103, Loss: 1.2449, Train: 1.1166, Val: 1.1168\n",
      "Epoch: 104, Loss: 1.2448, Train: 1.1168, Val: 1.1170\n",
      "Epoch: 105, Loss: 1.2448, Train: 1.1167, Val: 1.1169\n",
      "Epoch: 106, Loss: 1.2448, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 107, Loss: 1.2448, Train: 1.1169, Val: 1.1172\n",
      "Epoch: 108, Loss: 1.2447, Train: 1.1170, Val: 1.1172\n",
      "Epoch: 109, Loss: 1.2447, Train: 1.1171, Val: 1.1173\n",
      "Epoch: 110, Loss: 1.2447, Train: 1.1172, Val: 1.1174\n",
      "Epoch: 111, Loss: 1.2447, Train: 1.1168, Val: 1.1170\n",
      "Epoch: 112, Loss: 1.2446, Train: 1.1169, Val: 1.1171\n",
      "Epoch: 113, Loss: 1.2446, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 114, Loss: 1.2446, Train: 1.1166, Val: 1.1168\n",
      "Epoch: 115, Loss: 1.2446, Train: 1.1166, Val: 1.1168\n",
      "Epoch: 116, Loss: 1.2445, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 117, Loss: 1.2446, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 118, Loss: 1.2446, Train: 1.1175, Val: 1.1177\n",
      "Epoch: 119, Loss: 1.2445, Train: 1.1176, Val: 1.1178\n",
      "Epoch: 120, Loss: 1.2446, Train: 1.1172, Val: 1.1174\n",
      "Epoch: 121, Loss: 1.2445, Train: 1.1168, Val: 1.1170\n",
      "Epoch: 122, Loss: 1.2445, Train: 1.1168, Val: 1.1170\n",
      "Epoch: 123, Loss: 1.2444, Train: 1.1167, Val: 1.1169\n",
      "Epoch: 124, Loss: 1.2444, Train: 1.1163, Val: 1.1165\n",
      "Epoch: 125, Loss: 1.2444, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 126, Loss: 1.2444, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 127, Loss: 1.2444, Train: 1.1160, Val: 1.1163\n",
      "Epoch: 128, Loss: 1.2443, Train: 1.1159, Val: 1.1162\n",
      "Epoch: 129, Loss: 1.2443, Train: 1.1158, Val: 1.1161\n",
      "Epoch: 130, Loss: 1.2443, Train: 1.1159, Val: 1.1162\n",
      "Epoch: 131, Loss: 1.2443, Train: 1.1158, Val: 1.1161\n",
      "Epoch: 132, Loss: 1.2442, Train: 1.1157, Val: 1.1160\n",
      "Epoch: 133, Loss: 1.2442, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 134, Loss: 1.2441, Train: 1.1157, Val: 1.1160\n",
      "Epoch: 135, Loss: 1.2441, Train: 1.1156, Val: 1.1159\n",
      "Epoch: 136, Loss: 1.2441, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 137, Loss: 1.2441, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 138, Loss: 1.2440, Train: 1.1155, Val: 1.1159\n",
      "Epoch: 139, Loss: 1.2440, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 140, Loss: 1.2440, Train: 1.1156, Val: 1.1159\n",
      "Epoch: 141, Loss: 1.2439, Train: 1.1154, Val: 1.1158\n",
      "Epoch: 142, Loss: 1.2440, Train: 1.1155, Val: 1.1159\n",
      "Epoch: 143, Loss: 1.2439, Train: 1.1155, Val: 1.1158\n",
      "Epoch: 144, Loss: 1.2438, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 145, Loss: 1.2439, Train: 1.1155, Val: 1.1158\n",
      "Epoch: 146, Loss: 1.2438, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 147, Loss: 1.2437, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 148, Loss: 1.2438, Train: 1.1154, Val: 1.1158\n",
      "Epoch: 149, Loss: 1.2437, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 150, Loss: 1.2437, Train: 1.1153, Val: 1.1156\n",
      "Epoch: 151, Loss: 1.2436, Train: 1.1152, Val: 1.1155\n",
      "Epoch: 152, Loss: 1.2435, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 153, Loss: 1.2436, Train: 1.1153, Val: 1.1156\n",
      "Epoch: 154, Loss: 1.2435, Train: 1.1152, Val: 1.1155\n",
      "Epoch: 155, Loss: 1.2436, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 156, Loss: 1.2435, Train: 1.1153, Val: 1.1156\n",
      "Epoch: 157, Loss: 1.2434, Train: 1.1152, Val: 1.1155\n",
      "Epoch: 158, Loss: 1.2434, Train: 1.1152, Val: 1.1155\n",
      "Epoch: 159, Loss: 1.2432, Train: 1.1154, Val: 1.1156\n",
      "Epoch: 160, Loss: 1.2432, Train: 1.1153, Val: 1.1156\n",
      "Epoch: 161, Loss: 1.2434, Train: 1.1153, Val: 1.1156\n",
      "Epoch: 162, Loss: 1.2435, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 163, Loss: 1.2437, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 164, Loss: 1.2437, Train: 1.1154, Val: 1.1156\n",
      "Epoch: 165, Loss: 1.2435, Train: 1.1154, Val: 1.1156\n",
      "Epoch: 166, Loss: 1.2430, Train: 1.1157, Val: 1.1159\n",
      "Epoch: 167, Loss: 1.2438, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 168, Loss: 1.2434, Train: 1.1156, Val: 1.1159\n",
      "Epoch: 169, Loss: 1.2437, Train: 1.1156, Val: 1.1159\n",
      "Epoch: 170, Loss: 1.2437, Train: 1.1157, Val: 1.1160\n",
      "Epoch: 171, Loss: 1.2435, Train: 1.1159, Val: 1.1161\n",
      "Epoch: 172, Loss: 1.2434, Train: 1.1161, Val: 1.1162\n",
      "Epoch: 173, Loss: 1.2433, Train: 1.1160, Val: 1.1162\n",
      "Epoch: 174, Loss: 1.2428, Train: 1.1161, Val: 1.1162\n",
      "Epoch: 175, Loss: 1.2426, Train: 1.1164, Val: 1.1166\n",
      "Epoch: 176, Loss: 1.2423, Train: 1.1168, Val: 1.1169\n",
      "Epoch: 177, Loss: 1.2426, Train: 1.1163, Val: 1.1165\n",
      "Epoch: 178, Loss: 1.2422, Train: 1.1155, Val: 1.1158\n",
      "Epoch: 179, Loss: 1.2427, Train: 1.1150, Val: 1.1153\n",
      "Epoch: 180, Loss: 1.2429, Train: 1.1154, Val: 1.1157\n",
      "Epoch: 181, Loss: 1.2430, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 182, Loss: 1.2432, Train: 1.1155, Val: 1.1159\n",
      "Epoch: 183, Loss: 1.2428, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 184, Loss: 1.2428, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 185, Loss: 1.2428, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 186, Loss: 1.2425, Train: 1.1169, Val: 1.1174\n",
      "Epoch: 187, Loss: 1.2423, Train: 1.1168, Val: 1.1173\n",
      "Epoch: 188, Loss: 1.2422, Train: 1.1165, Val: 1.1170\n",
      "Epoch: 189, Loss: 1.2422, Train: 1.1172, Val: 1.1178\n",
      "Epoch: 190, Loss: 1.2419, Train: 1.1173, Val: 1.1178\n",
      "Epoch: 191, Loss: 1.2418, Train: 1.1174, Val: 1.1180\n",
      "Epoch: 192, Loss: 1.2416, Train: 1.1185, Val: 1.1192\n",
      "Epoch: 193, Loss: 1.2418, Train: 1.1173, Val: 1.1179\n",
      "Epoch: 194, Loss: 1.2411, Train: 1.1177, Val: 1.1183\n",
      "Epoch: 195, Loss: 1.2407, Train: 1.1184, Val: 1.1190\n",
      "Epoch: 196, Loss: 1.2400, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 197, Loss: 1.2386, Train: 1.1161, Val: 1.1167\n",
      "Epoch: 198, Loss: 1.2478, Train: 1.1236, Val: 1.1240\n",
      "Epoch: 199, Loss: 1.2459, Train: 1.1308, Val: 1.1311\n",
      "Epoch: 200, Loss: 1.2431, Train: 1.1326, Val: 1.1329\n",
      "Epoch: 201, Loss: 1.2478, Train: 1.1582, Val: 1.1582\n",
      "Epoch: 202, Loss: 1.2467, Train: 1.1586, Val: 1.1582\n",
      "Epoch: 203, Loss: 1.2444, Train: 1.1509, Val: 1.1500\n",
      "Epoch: 204, Loss: 1.2437, Train: 1.1747, Val: 1.1733\n",
      "Epoch: 205, Loss: 1.2426, Train: 1.1848, Val: 1.1834\n",
      "Epoch: 206, Loss: 1.2438, Train: 1.1747, Val: 1.1735\n",
      "Epoch: 207, Loss: 1.2427, Train: 1.1622, Val: 1.1613\n",
      "Epoch: 208, Loss: 1.2421, Train: 1.1640, Val: 1.1633\n",
      "Epoch: 209, Loss: 1.2416, Train: 1.1687, Val: 1.1681\n",
      "Epoch: 210, Loss: 1.2428, Train: 1.1420, Val: 1.1417\n",
      "Epoch: 211, Loss: 1.2416, Train: 1.1342, Val: 1.1340\n",
      "Epoch: 212, Loss: 1.2411, Train: 1.1275, Val: 1.1275\n",
      "Epoch: 213, Loss: 1.2447, Train: 1.1290, Val: 1.1289\n",
      "Epoch: 214, Loss: 1.2465, Train: 1.1245, Val: 1.1244\n",
      "Epoch: 215, Loss: 1.2454, Train: 1.1241, Val: 1.1240\n",
      "Epoch: 216, Loss: 1.2450, Train: 1.1346, Val: 1.1346\n",
      "Epoch: 217, Loss: 1.2441, Train: 1.1419, Val: 1.1418\n",
      "Epoch: 218, Loss: 1.2449, Train: 1.1338, Val: 1.1339\n",
      "Epoch: 219, Loss: 1.2449, Train: 1.1294, Val: 1.1295\n",
      "Epoch: 220, Loss: 1.2443, Train: 1.1278, Val: 1.1279\n",
      "Epoch: 221, Loss: 1.2436, Train: 1.1208, Val: 1.1209\n",
      "Epoch: 222, Loss: 1.2432, Train: 1.1173, Val: 1.1175\n",
      "Epoch: 223, Loss: 1.2437, Train: 1.1182, Val: 1.1183\n",
      "Epoch: 224, Loss: 1.2434, Train: 1.1207, Val: 1.1209\n",
      "Epoch: 225, Loss: 1.2431, Train: 1.1213, Val: 1.1216\n",
      "Epoch: 226, Loss: 1.2425, Train: 1.1240, Val: 1.1242\n",
      "Epoch: 227, Loss: 1.2424, Train: 1.1295, Val: 1.1297\n",
      "Epoch: 228, Loss: 1.2423, Train: 1.1281, Val: 1.1284\n",
      "Epoch: 229, Loss: 1.2419, Train: 1.1226, Val: 1.1230\n",
      "Epoch: 230, Loss: 1.2414, Train: 1.1216, Val: 1.1220\n",
      "Epoch: 231, Loss: 1.2408, Train: 1.1215, Val: 1.1219\n",
      "Epoch: 232, Loss: 1.2405, Train: 1.1195, Val: 1.1199\n",
      "Epoch: 233, Loss: 1.2399, Train: 1.1199, Val: 1.1204\n",
      "Epoch: 234, Loss: 1.2391, Train: 1.1238, Val: 1.1242\n",
      "Epoch: 235, Loss: 1.2383, Train: 1.1262, Val: 1.1265\n",
      "Epoch: 236, Loss: 1.2376, Train: 1.1254, Val: 1.1257\n",
      "Epoch: 237, Loss: 1.2361, Train: 1.1301, Val: 1.1302\n",
      "Epoch: 238, Loss: 1.2346, Train: 1.1351, Val: 1.1350\n",
      "Epoch: 239, Loss: 1.2347, Train: 1.1279, Val: 1.1282\n",
      "Epoch: 240, Loss: 1.2394, Train: 1.1221, Val: 1.1225\n",
      "Epoch: 241, Loss: 1.2402, Train: 1.1208, Val: 1.1212\n",
      "Epoch: 242, Loss: 1.2394, Train: 1.1198, Val: 1.1202\n",
      "Epoch: 243, Loss: 1.2383, Train: 1.1197, Val: 1.1200\n",
      "Epoch: 244, Loss: 1.2423, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 245, Loss: 1.2383, Train: 1.1168, Val: 1.1175\n",
      "Epoch: 246, Loss: 1.2393, Train: 1.1196, Val: 1.1203\n",
      "Epoch: 247, Loss: 1.2395, Train: 1.1198, Val: 1.1206\n",
      "Epoch: 248, Loss: 1.2390, Train: 1.1200, Val: 1.1208\n",
      "Epoch: 249, Loss: 1.2416, Train: 1.1271, Val: 1.1280\n",
      "Epoch: 250, Loss: 1.2389, Train: 1.1368, Val: 1.1379\n",
      "Epoch: 251, Loss: 1.2386, Train: 1.1372, Val: 1.1383\n",
      "Epoch: 252, Loss: 1.2388, Train: 1.1320, Val: 1.1331\n",
      "Epoch: 253, Loss: 1.2374, Train: 1.1262, Val: 1.1272\n",
      "Epoch: 254, Loss: 1.2349, Train: 1.1214, Val: 1.1223\n",
      "Epoch: 255, Loss: 1.2329, Train: 1.1211, Val: 1.1218\n",
      "Epoch: 256, Loss: 1.2321, Train: 1.1255, Val: 1.1260\n",
      "Epoch: 257, Loss: 1.2297, Train: 1.1384, Val: 1.1386\n",
      "Epoch: 258, Loss: 1.2531, Train: 1.1346, Val: 1.1354\n",
      "Epoch: 259, Loss: 1.2484, Train: 1.1262, Val: 1.1273\n",
      "Epoch: 260, Loss: 1.2485, Train: 1.1333, Val: 1.1340\n",
      "Epoch: 261, Loss: 1.2336, Train: 1.1992, Val: 1.1996\n",
      "Epoch: 262, Loss: 1.2431, Train: 1.1340, Val: 1.1344\n",
      "Epoch: 263, Loss: 1.2455, Train: 1.1160, Val: 1.1168\n",
      "Epoch: 264, Loss: 1.2306, Train: 1.1174, Val: 1.1180\n",
      "Epoch: 265, Loss: 1.2400, Train: 1.1188, Val: 1.1194\n",
      "Epoch: 266, Loss: 1.2372, Train: 1.1246, Val: 1.1251\n",
      "Epoch: 267, Loss: 1.2348, Train: 1.1217, Val: 1.1219\n",
      "Epoch: 268, Loss: 1.2325, Train: 1.1212, Val: 1.1214\n",
      "Epoch: 269, Loss: 1.2458, Train: 1.1249, Val: 1.1251\n",
      "Epoch: 270, Loss: 1.2377, Train: 1.1316, Val: 1.1322\n",
      "Epoch: 271, Loss: 1.2393, Train: 1.1340, Val: 1.1348\n",
      "Epoch: 272, Loss: 1.2363, Train: 1.1287, Val: 1.1294\n",
      "Epoch: 273, Loss: 1.2374, Train: 1.1627, Val: 1.1635\n",
      "Epoch: 274, Loss: 1.2307, Train: 1.1985, Val: 1.1995\n",
      "Epoch: 275, Loss: 1.2293, Train: 1.1749, Val: 1.1756\n",
      "Epoch: 276, Loss: 1.2302, Train: 1.1500, Val: 1.1505\n",
      "Epoch: 277, Loss: 1.2355, Train: 1.1800, Val: 1.1810\n",
      "Epoch: 278, Loss: 1.2344, Train: 1.1502, Val: 1.1512\n",
      "Epoch: 279, Loss: 1.2313, Train: 1.1125, Val: 1.1132\n",
      "Epoch: 280, Loss: 1.2298, Train: 1.1117, Val: 1.1126\n",
      "Epoch: 281, Loss: 1.2312, Train: 1.1138, Val: 1.1144\n",
      "Epoch: 282, Loss: 1.2267, Train: 1.1133, Val: 1.1141\n",
      "Epoch: 283, Loss: 1.2262, Train: 1.1444, Val: 1.1458\n",
      "Epoch: 284, Loss: 1.2270, Train: 1.1753, Val: 1.1769\n",
      "Epoch: 285, Loss: 1.2275, Train: 1.1878, Val: 1.1896\n",
      "Epoch: 286, Loss: 1.2252, Train: 1.2348, Val: 1.2368\n",
      "Epoch: 287, Loss: 1.2233, Train: 1.2238, Val: 1.2259\n",
      "Epoch: 288, Loss: 1.2209, Train: 1.2099, Val: 1.2118\n",
      "Epoch: 289, Loss: 1.2187, Train: 1.1835, Val: 1.1854\n",
      "Epoch: 290, Loss: 1.2172, Train: 1.1421, Val: 1.1437\n",
      "Epoch: 291, Loss: 1.2144, Train: 1.1276, Val: 1.1292\n",
      "Epoch: 292, Loss: 1.2139, Train: 1.1184, Val: 1.1200\n",
      "Epoch: 293, Loss: 1.2118, Train: 1.1139, Val: 1.1154\n",
      "Epoch: 294, Loss: 1.2098, Train: 1.1102, Val: 1.1116\n",
      "Epoch: 295, Loss: 1.2082, Train: 1.1073, Val: 1.1087\n",
      "Epoch: 296, Loss: 1.2060, Train: 1.1061, Val: 1.1075\n",
      "Epoch: 297, Loss: 1.2026, Train: 1.1121, Val: 1.1135\n",
      "Epoch: 298, Loss: 1.2057, Train: 1.1041, Val: 1.1054\n",
      "Epoch: 299, Loss: 1.2101, Train: 1.1276, Val: 1.1284\n",
      "Epoch: 300, Loss: 1.2168, Train: 1.1049, Val: 1.1059\n",
      "Epoch: 301, Loss: 1.2114, Train: 1.1077, Val: 1.1087\n",
      "Epoch: 302, Loss: 1.2094, Train: 1.1063, Val: 1.1069\n",
      "Epoch: 303, Loss: 1.2111, Train: 1.1227, Val: 1.1241\n",
      "Epoch: 304, Loss: 1.2017, Train: 1.1636, Val: 1.1655\n",
      "Epoch: 305, Loss: 1.2671, Train: 1.2073, Val: 1.2094\n",
      "Epoch: 306, Loss: 1.2867, Train: 1.1851, Val: 1.1880\n",
      "Epoch: 307, Loss: 1.2330, Train: 1.1719, Val: 1.1744\n",
      "Epoch: 308, Loss: 1.2540, Train: 1.2251, Val: 1.2267\n",
      "Epoch: 309, Loss: 1.2278, Train: 1.2051, Val: 1.2063\n",
      "Epoch: 310, Loss: 1.2343, Train: 1.1131, Val: 1.1147\n",
      "Epoch: 311, Loss: 1.2256, Train: 1.1137, Val: 1.1154\n",
      "Epoch: 312, Loss: 1.2150, Train: 1.1501, Val: 1.1513\n",
      "Epoch: 313, Loss: 1.2583, Train: 1.1069, Val: 1.1083\n",
      "Epoch: 314, Loss: 1.2092, Train: 1.1038, Val: 1.1054\n",
      "Epoch: 315, Loss: 1.2230, Train: 1.1140, Val: 1.1151\n",
      "Epoch: 316, Loss: 1.2040, Train: 1.1774, Val: 1.1782\n",
      "Epoch: 317, Loss: 1.2178, Train: 1.1293, Val: 1.1308\n",
      "Epoch: 318, Loss: 1.2085, Train: 1.1153, Val: 1.1168\n",
      "Epoch: 319, Loss: 1.2179, Train: 1.1326, Val: 1.1337\n",
      "Epoch: 320, Loss: 1.2068, Train: 1.1349, Val: 1.1350\n",
      "Epoch: 321, Loss: 1.2096, Train: 1.1259, Val: 1.1255\n",
      "Epoch: 322, Loss: 1.2093, Train: 1.1395, Val: 1.1385\n",
      "Epoch: 323, Loss: 1.2065, Train: 1.1660, Val: 1.1644\n",
      "Epoch: 324, Loss: 1.2016, Train: 1.1654, Val: 1.1637\n",
      "Epoch: 325, Loss: 1.2016, Train: 1.2152, Val: 1.2128\n",
      "Epoch: 326, Loss: 1.2195, Train: 1.2689, Val: 1.2666\n",
      "Epoch: 327, Loss: 1.2026, Train: 1.1802, Val: 1.1787\n",
      "Epoch: 328, Loss: 1.2081, Train: 1.1764, Val: 1.1753\n",
      "Epoch: 329, Loss: 1.2052, Train: 1.2456, Val: 1.2445\n",
      "Epoch: 330, Loss: 1.2097, Train: 1.1858, Val: 1.1852\n",
      "Epoch: 331, Loss: 1.1992, Train: 1.1228, Val: 1.1227\n",
      "Epoch: 332, Loss: 1.2010, Train: 1.1309, Val: 1.1307\n",
      "Epoch: 333, Loss: 1.1892, Train: 1.1527, Val: 1.1523\n",
      "Epoch: 334, Loss: 1.1930, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 335, Loss: 1.1854, Train: 1.0938, Val: 1.0951\n",
      "Epoch: 336, Loss: 1.1950, Train: 1.0966, Val: 1.0978\n",
      "Epoch: 337, Loss: 1.1832, Train: 1.1072, Val: 1.1082\n",
      "Epoch: 338, Loss: 1.1881, Train: 1.1037, Val: 1.1046\n",
      "Epoch: 339, Loss: 1.1878, Train: 1.1065, Val: 1.1077\n",
      "Epoch: 340, Loss: 1.1840, Train: 1.1075, Val: 1.1091\n",
      "Epoch: 341, Loss: 1.1836, Train: 1.1040, Val: 1.1058\n",
      "Epoch: 342, Loss: 1.1817, Train: 1.1083, Val: 1.1099\n",
      "Epoch: 343, Loss: 1.1823, Train: 1.1119, Val: 1.1135\n",
      "Epoch: 344, Loss: 1.1851, Train: 1.0925, Val: 1.0948\n",
      "Epoch: 345, Loss: 1.1811, Train: 1.0918, Val: 1.0944\n",
      "Epoch: 346, Loss: 1.1846, Train: 1.0870, Val: 1.0894\n",
      "Epoch: 347, Loss: 1.1800, Train: 1.0897, Val: 1.0918\n",
      "Epoch: 348, Loss: 1.1826, Train: 1.0880, Val: 1.0905\n",
      "Epoch: 349, Loss: 1.1786, Train: 1.0911, Val: 1.0938\n",
      "Epoch: 350, Loss: 1.1803, Train: 1.0861, Val: 1.0885\n",
      "Epoch: 351, Loss: 1.1757, Train: 1.0949, Val: 1.0970\n",
      "Epoch: 352, Loss: 1.1769, Train: 1.0894, Val: 1.0919\n",
      "Epoch: 353, Loss: 1.1733, Train: 1.0902, Val: 1.0931\n",
      "Epoch: 354, Loss: 1.1747, Train: 1.0992, Val: 1.1021\n",
      "Epoch: 355, Loss: 1.1723, Train: 1.1038, Val: 1.1068\n",
      "Epoch: 356, Loss: 1.1734, Train: 1.0933, Val: 1.0965\n",
      "Epoch: 357, Loss: 1.1719, Train: 1.0907, Val: 1.0940\n",
      "Epoch: 358, Loss: 1.1722, Train: 1.0868, Val: 1.0898\n",
      "Epoch: 359, Loss: 1.1709, Train: 1.0872, Val: 1.0898\n",
      "Epoch: 360, Loss: 1.1706, Train: 1.0836, Val: 1.0864\n",
      "Epoch: 361, Loss: 1.1693, Train: 1.0840, Val: 1.0869\n",
      "Epoch: 362, Loss: 1.1690, Train: 1.0876, Val: 1.0903\n",
      "Epoch: 363, Loss: 1.1681, Train: 1.0855, Val: 1.0882\n",
      "Epoch: 364, Loss: 1.1679, Train: 1.0818, Val: 1.0848\n",
      "Epoch: 365, Loss: 1.1672, Train: 1.0817, Val: 1.0847\n",
      "Epoch: 366, Loss: 1.1667, Train: 1.0808, Val: 1.0836\n",
      "Epoch: 367, Loss: 1.1665, Train: 1.0818, Val: 1.0848\n",
      "Epoch: 368, Loss: 1.1659, Train: 1.0851, Val: 1.0884\n",
      "Epoch: 369, Loss: 1.1657, Train: 1.0817, Val: 1.0848\n",
      "Epoch: 370, Loss: 1.1649, Train: 1.0795, Val: 1.0825\n",
      "Epoch: 371, Loss: 1.1645, Train: 1.0797, Val: 1.0827\n",
      "Epoch: 372, Loss: 1.1639, Train: 1.0795, Val: 1.0826\n",
      "Epoch: 373, Loss: 1.1636, Train: 1.0796, Val: 1.0825\n",
      "Epoch: 374, Loss: 1.1631, Train: 1.0797, Val: 1.0827\n",
      "Epoch: 375, Loss: 1.1629, Train: 1.0789, Val: 1.0820\n",
      "Epoch: 376, Loss: 1.1624, Train: 1.0793, Val: 1.0824\n",
      "Epoch: 377, Loss: 1.1622, Train: 1.0786, Val: 1.0816\n",
      "Epoch: 378, Loss: 1.1617, Train: 1.0782, Val: 1.0813\n",
      "Epoch: 379, Loss: 1.1614, Train: 1.0786, Val: 1.0818\n",
      "Epoch: 380, Loss: 1.1609, Train: 1.0791, Val: 1.0825\n",
      "Epoch: 381, Loss: 1.1606, Train: 1.0789, Val: 1.0822\n",
      "Epoch: 382, Loss: 1.1603, Train: 1.0784, Val: 1.0818\n",
      "Epoch: 383, Loss: 1.1600, Train: 1.0787, Val: 1.0822\n",
      "Epoch: 384, Loss: 1.1597, Train: 1.0783, Val: 1.0817\n",
      "Epoch: 385, Loss: 1.1594, Train: 1.0773, Val: 1.0806\n",
      "Epoch: 386, Loss: 1.1591, Train: 1.0775, Val: 1.0809\n",
      "Epoch: 387, Loss: 1.1587, Train: 1.0781, Val: 1.0816\n",
      "Epoch: 388, Loss: 1.1585, Train: 1.0774, Val: 1.0809\n",
      "Epoch: 389, Loss: 1.1581, Train: 1.0769, Val: 1.0803\n",
      "Epoch: 390, Loss: 1.1579, Train: 1.0771, Val: 1.0806\n",
      "Epoch: 391, Loss: 1.1576, Train: 1.0772, Val: 1.0808\n",
      "Epoch: 392, Loss: 1.1574, Train: 1.0769, Val: 1.0805\n",
      "Epoch: 393, Loss: 1.1571, Train: 1.0771, Val: 1.0808\n",
      "Epoch: 394, Loss: 1.1568, Train: 1.0774, Val: 1.0811\n",
      "Epoch: 395, Loss: 1.1566, Train: 1.0767, Val: 1.0803\n",
      "Epoch: 396, Loss: 1.1563, Train: 1.0761, Val: 1.0797\n",
      "Epoch: 397, Loss: 1.1560, Train: 1.0762, Val: 1.0799\n",
      "Epoch: 398, Loss: 1.1558, Train: 1.0763, Val: 1.0801\n",
      "Epoch: 399, Loss: 1.1556, Train: 1.0762, Val: 1.0799\n",
      "Epoch: 400, Loss: 1.1553, Train: 1.0761, Val: 1.0799\n",
      "Epoch: 401, Loss: 1.1551, Train: 1.0766, Val: 1.0804\n",
      "Epoch: 402, Loss: 1.1549, Train: 1.0772, Val: 1.0811\n",
      "Epoch: 403, Loss: 1.1546, Train: 1.0769, Val: 1.0808\n",
      "Epoch: 404, Loss: 1.1544, Train: 1.0765, Val: 1.0804\n",
      "Epoch: 405, Loss: 1.1542, Train: 1.0764, Val: 1.0804\n",
      "Epoch: 406, Loss: 1.1540, Train: 1.0760, Val: 1.0800\n",
      "Epoch: 407, Loss: 1.1538, Train: 1.0755, Val: 1.0794\n",
      "Epoch: 408, Loss: 1.1536, Train: 1.0757, Val: 1.0796\n",
      "Epoch: 409, Loss: 1.1533, Train: 1.0759, Val: 1.0799\n",
      "Epoch: 410, Loss: 1.1531, Train: 1.0754, Val: 1.0794\n",
      "Epoch: 411, Loss: 1.1529, Train: 1.0755, Val: 1.0795\n",
      "Epoch: 412, Loss: 1.1527, Train: 1.0757, Val: 1.0797\n",
      "Epoch: 413, Loss: 1.1526, Train: 1.0755, Val: 1.0796\n",
      "Epoch: 414, Loss: 1.1524, Train: 1.0755, Val: 1.0796\n",
      "Epoch: 415, Loss: 1.1522, Train: 1.0758, Val: 1.0799\n",
      "Epoch: 416, Loss: 1.1520, Train: 1.0755, Val: 1.0797\n",
      "Epoch: 417, Loss: 1.1518, Train: 1.0754, Val: 1.0796\n",
      "Epoch: 418, Loss: 1.1516, Train: 1.0755, Val: 1.0797\n",
      "Epoch: 419, Loss: 1.1515, Train: 1.0753, Val: 1.0795\n",
      "Epoch: 420, Loss: 1.1513, Train: 1.0753, Val: 1.0796\n",
      "Epoch: 421, Loss: 1.1511, Train: 1.0757, Val: 1.0800\n",
      "Epoch: 422, Loss: 1.1509, Train: 1.0750, Val: 1.0792\n",
      "Epoch: 423, Loss: 1.1508, Train: 1.0762, Val: 1.0806\n",
      "Epoch: 424, Loss: 1.1507, Train: 1.0744, Val: 1.0785\n",
      "Epoch: 425, Loss: 1.1511, Train: 1.0767, Val: 1.0812\n",
      "Epoch: 426, Loss: 1.1504, Train: 1.0758, Val: 1.0801\n",
      "Epoch: 427, Loss: 1.1502, Train: 1.0762, Val: 1.0807\n",
      "Epoch: 428, Loss: 1.1500, Train: 1.0748, Val: 1.0791\n",
      "Epoch: 429, Loss: 1.1500, Train: 1.0758, Val: 1.0803\n",
      "Epoch: 430, Loss: 1.1500, Train: 1.0742, Val: 1.0781\n",
      "Epoch: 431, Loss: 1.1513, Train: 1.0751, Val: 1.0793\n",
      "Epoch: 432, Loss: 1.1498, Train: 1.0834, Val: 1.0885\n",
      "Epoch: 433, Loss: 1.1588, Train: 1.1343, Val: 1.1348\n",
      "Epoch: 434, Loss: 1.1646, Train: 1.2618, Val: 1.2603\n",
      "Epoch: 435, Loss: 1.1664, Train: 1.4094, Val: 1.4075\n",
      "Epoch: 436, Loss: 1.1661, Train: 1.5204, Val: 1.5195\n",
      "Epoch: 437, Loss: 1.1862, Train: 1.2809, Val: 1.2810\n",
      "Epoch: 438, Loss: 1.1850, Train: 1.2558, Val: 1.2549\n",
      "Epoch: 439, Loss: 1.1774, Train: 1.2446, Val: 1.2437\n",
      "Epoch: 440, Loss: 1.1803, Train: 1.1380, Val: 1.1378\n",
      "Epoch: 441, Loss: 1.1644, Train: 1.1126, Val: 1.1132\n",
      "Epoch: 442, Loss: 1.1733, Train: 1.1359, Val: 1.1362\n",
      "Epoch: 443, Loss: 1.1675, Train: 1.2005, Val: 1.2002\n",
      "Epoch: 444, Loss: 1.1663, Train: 1.1280, Val: 1.1291\n",
      "Epoch: 445, Loss: 1.1605, Train: 1.1513, Val: 1.1526\n",
      "Epoch: 446, Loss: 1.1604, Train: 1.2172, Val: 1.2177\n",
      "Epoch: 447, Loss: 1.1630, Train: 1.1139, Val: 1.1155\n",
      "Epoch: 448, Loss: 1.1577, Train: 1.0814, Val: 1.0849\n",
      "Epoch: 449, Loss: 1.1599, Train: 1.0830, Val: 1.0867\n",
      "Epoch: 450, Loss: 1.1555, Train: 1.0830, Val: 1.0865\n",
      "Epoch: 451, Loss: 1.1540, Train: 1.0939, Val: 1.0979\n",
      "Epoch: 452, Loss: 1.1542, Train: 1.1299, Val: 1.1342\n",
      "Epoch: 453, Loss: 1.1518, Train: 1.1833, Val: 1.1874\n",
      "Epoch: 454, Loss: 1.1545, Train: 1.1806, Val: 1.1850\n",
      "Epoch: 455, Loss: 1.1510, Train: 1.1556, Val: 1.1601\n",
      "Epoch: 456, Loss: 1.1524, Train: 1.1481, Val: 1.1526\n",
      "Epoch: 457, Loss: 1.1512, Train: 1.1563, Val: 1.1608\n",
      "Epoch: 458, Loss: 1.1507, Train: 1.1453, Val: 1.1498\n",
      "Epoch: 459, Loss: 1.1509, Train: 1.1163, Val: 1.1209\n",
      "Epoch: 460, Loss: 1.1499, Train: 1.1010, Val: 1.1056\n",
      "Epoch: 461, Loss: 1.1507, Train: 1.0950, Val: 1.0995\n",
      "Epoch: 462, Loss: 1.1498, Train: 1.0950, Val: 1.0995\n",
      "Epoch: 463, Loss: 1.1493, Train: 1.0970, Val: 1.1015\n",
      "Epoch: 464, Loss: 1.1493, Train: 1.0961, Val: 1.1007\n",
      "Epoch: 465, Loss: 1.1490, Train: 1.0918, Val: 1.0963\n",
      "Epoch: 466, Loss: 1.1492, Train: 1.0910, Val: 1.0956\n",
      "Epoch: 467, Loss: 1.1485, Train: 1.0929, Val: 1.0976\n",
      "Epoch: 468, Loss: 1.1486, Train: 1.0897, Val: 1.0944\n",
      "Epoch: 469, Loss: 1.1482, Train: 1.0848, Val: 1.0894\n",
      "Epoch: 470, Loss: 1.1479, Train: 1.0835, Val: 1.0882\n",
      "Epoch: 471, Loss: 1.1481, Train: 1.0854, Val: 1.0902\n",
      "Epoch: 472, Loss: 1.1475, Train: 1.0900, Val: 1.0950\n",
      "Epoch: 473, Loss: 1.1477, Train: 1.0915, Val: 1.0965\n",
      "Epoch: 474, Loss: 1.1474, Train: 1.0892, Val: 1.0942\n",
      "Epoch: 475, Loss: 1.1473, Train: 1.0852, Val: 1.0901\n",
      "Epoch: 476, Loss: 1.1471, Train: 1.0842, Val: 1.0890\n",
      "Epoch: 477, Loss: 1.1469, Train: 1.0839, Val: 1.0888\n",
      "Epoch: 478, Loss: 1.1470, Train: 1.0817, Val: 1.0865\n",
      "Epoch: 479, Loss: 1.1466, Train: 1.0803, Val: 1.0851\n",
      "Epoch: 480, Loss: 1.1466, Train: 1.0826, Val: 1.0875\n",
      "Epoch: 481, Loss: 1.1464, Train: 1.0863, Val: 1.0912\n",
      "Epoch: 482, Loss: 1.1463, Train: 1.0848, Val: 1.0896\n",
      "Epoch: 483, Loss: 1.1463, Train: 1.0814, Val: 1.0862\n",
      "Epoch: 484, Loss: 1.1461, Train: 1.0792, Val: 1.0840\n",
      "Epoch: 485, Loss: 1.1461, Train: 1.0798, Val: 1.0847\n",
      "Epoch: 486, Loss: 1.1459, Train: 1.0816, Val: 1.0866\n",
      "Epoch: 487, Loss: 1.1458, Train: 1.0820, Val: 1.0870\n",
      "Epoch: 488, Loss: 1.1457, Train: 1.0799, Val: 1.0849\n",
      "Epoch: 489, Loss: 1.1456, Train: 1.0798, Val: 1.0847\n",
      "Epoch: 490, Loss: 1.1455, Train: 1.0811, Val: 1.0861\n",
      "Epoch: 491, Loss: 1.1454, Train: 1.0802, Val: 1.0852\n",
      "Epoch: 492, Loss: 1.1453, Train: 1.0770, Val: 1.0819\n",
      "Epoch: 493, Loss: 1.1452, Train: 1.0751, Val: 1.0800\n",
      "Epoch: 494, Loss: 1.1451, Train: 1.0751, Val: 1.0800\n",
      "Epoch: 495, Loss: 1.1450, Train: 1.0763, Val: 1.0812\n",
      "Epoch: 496, Loss: 1.1449, Train: 1.0760, Val: 1.0810\n",
      "Epoch: 497, Loss: 1.1448, Train: 1.0744, Val: 1.0793\n",
      "Epoch: 498, Loss: 1.1448, Train: 1.0743, Val: 1.0792\n",
      "Epoch: 499, Loss: 1.1447, Train: 1.0743, Val: 1.0792\n",
      "Epoch: 500, Loss: 1.1446, Train: 1.0736, Val: 1.0785\n",
      "Test RMSE: 1.0995\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.453632       3.583673\n",
      "std      1727.484387     741.673176       0.325537       1.116938\n",
      "min         0.000000       0.000000       2.175252       1.000000\n",
      "25%      1500.000000     259.000000       3.315403       3.000000\n",
      "50%      3066.000000     693.000000       3.566545       4.000000\n",
      "75%      4472.000000    1292.000000       3.566545       4.000000\n",
      "max      6039.000000    3702.000000       4.782679       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1032.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80909243415533\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  2\n",
      "Kernel:  spd\n",
      "Model Name:  KPGINPlus\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 12.9780, Train: 2.8265, Val: 2.8301\n",
      "Epoch: 002, Loss: 10.6076, Train: 2.8009, Val: 2.8050\n",
      "Epoch: 003, Loss: 6.4332, Train: 2.6332, Val: 2.6359\n",
      "Epoch: 004, Loss: 3.6378, Train: 2.2994, Val: 2.2989\n",
      "Epoch: 005, Loss: 2.7627, Train: 2.0258, Val: 2.0248\n",
      "Epoch: 006, Loss: 1.9007, Train: 1.9185, Val: 1.9186\n",
      "Epoch: 007, Loss: 1.5044, Train: 1.8584, Val: 1.8590\n",
      "Epoch: 008, Loss: 1.6264, Train: 1.7415, Val: 1.7424\n",
      "Epoch: 009, Loss: 1.5873, Train: 1.6095, Val: 1.6105\n",
      "Epoch: 010, Loss: 1.4694, Train: 1.5085, Val: 1.5097\n",
      "Epoch: 011, Loss: 1.4810, Train: 1.4796, Val: 1.4810\n",
      "Epoch: 012, Loss: 1.5031, Train: 1.5363, Val: 1.5381\n",
      "Epoch: 013, Loss: 1.4166, Train: 1.5685, Val: 1.5706\n",
      "Epoch: 014, Loss: 1.3401, Train: 1.5416, Val: 1.5438\n",
      "Epoch: 015, Loss: 1.2742, Train: 1.5095, Val: 1.5118\n",
      "Epoch: 016, Loss: 1.2721, Train: 1.5097, Val: 1.5121\n",
      "Epoch: 017, Loss: 1.2844, Train: 1.5298, Val: 1.5324\n",
      "Epoch: 018, Loss: 1.2982, Train: 1.5328, Val: 1.5355\n",
      "Epoch: 019, Loss: 1.3080, Train: 1.4938, Val: 1.4964\n",
      "Epoch: 020, Loss: 1.2991, Train: 1.4341, Val: 1.4366\n",
      "Epoch: 021, Loss: 1.2851, Train: 1.3924, Val: 1.3948\n",
      "Epoch: 022, Loss: 1.2716, Train: 1.3796, Val: 1.3819\n",
      "Epoch: 023, Loss: 1.2606, Train: 1.3641, Val: 1.3663\n",
      "Epoch: 024, Loss: 1.2602, Train: 1.3386, Val: 1.3407\n",
      "Epoch: 025, Loss: 1.2619, Train: 1.3107, Val: 1.3127\n",
      "Epoch: 026, Loss: 1.2620, Train: 1.2921, Val: 1.2939\n",
      "Epoch: 027, Loss: 1.2601, Train: 1.2837, Val: 1.2855\n",
      "Epoch: 028, Loss: 1.2575, Train: 1.2756, Val: 1.2774\n",
      "Epoch: 029, Loss: 1.2562, Train: 1.2621, Val: 1.2638\n",
      "Epoch: 030, Loss: 1.2554, Train: 1.2480, Val: 1.2497\n",
      "Epoch: 031, Loss: 1.2550, Train: 1.2416, Val: 1.2433\n",
      "Epoch: 032, Loss: 1.2541, Train: 1.2410, Val: 1.2427\n",
      "Epoch: 033, Loss: 1.2527, Train: 1.2367, Val: 1.2384\n",
      "Epoch: 034, Loss: 1.2517, Train: 1.2270, Val: 1.2286\n",
      "Epoch: 035, Loss: 1.2512, Train: 1.2160, Val: 1.2176\n",
      "Epoch: 036, Loss: 1.2516, Train: 1.2103, Val: 1.2118\n",
      "Epoch: 037, Loss: 1.2521, Train: 1.2061, Val: 1.2076\n",
      "Epoch: 038, Loss: 1.2522, Train: 1.1972, Val: 1.1986\n",
      "Epoch: 039, Loss: 1.2516, Train: 1.1849, Val: 1.1863\n",
      "Epoch: 040, Loss: 1.2505, Train: 1.1755, Val: 1.1768\n",
      "Epoch: 041, Loss: 1.2494, Train: 1.1693, Val: 1.1706\n",
      "Epoch: 042, Loss: 1.2486, Train: 1.1627, Val: 1.1639\n",
      "Epoch: 043, Loss: 1.2484, Train: 1.1541, Val: 1.1552\n",
      "Epoch: 044, Loss: 1.2485, Train: 1.1471, Val: 1.1481\n",
      "Epoch: 045, Loss: 1.2487, Train: 1.1432, Val: 1.1442\n",
      "Epoch: 046, Loss: 1.2488, Train: 1.1404, Val: 1.1414\n",
      "Epoch: 047, Loss: 1.2489, Train: 1.1363, Val: 1.1371\n",
      "Epoch: 048, Loss: 1.2488, Train: 1.1320, Val: 1.1328\n",
      "Epoch: 049, Loss: 1.2487, Train: 1.1302, Val: 1.1309\n",
      "Epoch: 050, Loss: 1.2486, Train: 1.1297, Val: 1.1303\n",
      "Epoch: 051, Loss: 1.2483, Train: 1.1283, Val: 1.1289\n",
      "Epoch: 052, Loss: 1.2480, Train: 1.1263, Val: 1.1269\n",
      "Epoch: 053, Loss: 1.2477, Train: 1.1250, Val: 1.1256\n",
      "Epoch: 054, Loss: 1.2474, Train: 1.1247, Val: 1.1253\n",
      "Epoch: 055, Loss: 1.2473, Train: 1.1235, Val: 1.1242\n",
      "Epoch: 056, Loss: 1.2474, Train: 1.1221, Val: 1.1227\n",
      "Epoch: 057, Loss: 1.2475, Train: 1.1214, Val: 1.1220\n",
      "Epoch: 058, Loss: 1.2476, Train: 1.1210, Val: 1.1216\n",
      "Epoch: 059, Loss: 1.2476, Train: 1.1201, Val: 1.1207\n",
      "Epoch: 060, Loss: 1.2475, Train: 1.1191, Val: 1.1196\n",
      "Epoch: 061, Loss: 1.2474, Train: 1.1186, Val: 1.1191\n",
      "Epoch: 062, Loss: 1.2473, Train: 1.1183, Val: 1.1187\n",
      "Epoch: 063, Loss: 1.2473, Train: 1.1178, Val: 1.1182\n",
      "Epoch: 064, Loss: 1.2472, Train: 1.1172, Val: 1.1176\n",
      "Epoch: 065, Loss: 1.2471, Train: 1.1171, Val: 1.1174\n",
      "Epoch: 066, Loss: 1.2471, Train: 1.1170, Val: 1.1174\n",
      "Epoch: 067, Loss: 1.2471, Train: 1.1170, Val: 1.1174\n",
      "Epoch: 068, Loss: 1.2471, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 069, Loss: 1.2471, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 070, Loss: 1.2471, Train: 1.1170, Val: 1.1174\n",
      "Epoch: 071, Loss: 1.2470, Train: 1.1170, Val: 1.1173\n",
      "Epoch: 072, Loss: 1.2470, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 073, Loss: 1.2470, Train: 1.1171, Val: 1.1175\n",
      "Epoch: 074, Loss: 1.2469, Train: 1.1172, Val: 1.1175\n",
      "Epoch: 075, Loss: 1.2469, Train: 1.1172, Val: 1.1175\n",
      "Epoch: 076, Loss: 1.2469, Train: 1.1172, Val: 1.1176\n",
      "Epoch: 077, Loss: 1.2469, Train: 1.1173, Val: 1.1176\n",
      "Epoch: 078, Loss: 1.2469, Train: 1.1171, Val: 1.1175\n",
      "Epoch: 079, Loss: 1.2469, Train: 1.1170, Val: 1.1173\n",
      "Epoch: 080, Loss: 1.2468, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 081, Loss: 1.2468, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 082, Loss: 1.2468, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 083, Loss: 1.2468, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 084, Loss: 1.2468, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 085, Loss: 1.2468, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 086, Loss: 1.2468, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 087, Loss: 1.2467, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 088, Loss: 1.2467, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 089, Loss: 1.2467, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 090, Loss: 1.2467, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 091, Loss: 1.2467, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 092, Loss: 1.2467, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 093, Loss: 1.2466, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 094, Loss: 1.2466, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 095, Loss: 1.2466, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 096, Loss: 1.2466, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 097, Loss: 1.2466, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 098, Loss: 1.2466, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 099, Loss: 1.2466, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 100, Loss: 1.2466, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 101, Loss: 1.2466, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 102, Loss: 1.2466, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 103, Loss: 1.2465, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 104, Loss: 1.2465, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 105, Loss: 1.2465, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 106, Loss: 1.2465, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 107, Loss: 1.2465, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 108, Loss: 1.2465, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 109, Loss: 1.2465, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 110, Loss: 1.2465, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 111, Loss: 1.2465, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 112, Loss: 1.2465, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 113, Loss: 1.2465, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 114, Loss: 1.2464, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 115, Loss: 1.2464, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 116, Loss: 1.2464, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 117, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 118, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 119, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 120, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 121, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 122, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 123, Loss: 1.2464, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 124, Loss: 1.2464, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 125, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 126, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 127, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 128, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 129, Loss: 1.2463, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 130, Loss: 1.2463, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 131, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 132, Loss: 1.2463, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 133, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 134, Loss: 1.2463, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 135, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 136, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 137, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 138, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 139, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 140, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 141, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 142, Loss: 1.2462, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 143, Loss: 1.2462, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 144, Loss: 1.2462, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 145, Loss: 1.2461, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 146, Loss: 1.2461, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 147, Loss: 1.2461, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 148, Loss: 1.2461, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 149, Loss: 1.2461, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 150, Loss: 1.2461, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 151, Loss: 1.2460, Train: 1.1169, Val: 1.1172\n",
      "Epoch: 152, Loss: 1.2460, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 153, Loss: 1.2460, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 154, Loss: 1.2460, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 155, Loss: 1.2460, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 156, Loss: 1.2460, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 157, Loss: 1.2460, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 158, Loss: 1.2460, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 159, Loss: 1.2460, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 160, Loss: 1.2459, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 161, Loss: 1.2459, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 162, Loss: 1.2459, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 163, Loss: 1.2459, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 164, Loss: 1.2459, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 165, Loss: 1.2459, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 166, Loss: 1.2458, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 167, Loss: 1.2458, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 168, Loss: 1.2458, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 169, Loss: 1.2458, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 170, Loss: 1.2458, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 171, Loss: 1.2458, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 172, Loss: 1.2458, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 173, Loss: 1.2458, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 174, Loss: 1.2457, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 175, Loss: 1.2457, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 176, Loss: 1.2457, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 177, Loss: 1.2456, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 178, Loss: 1.2456, Train: 1.1164, Val: 1.1167\n",
      "Epoch: 179, Loss: 1.2456, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 180, Loss: 1.2456, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 181, Loss: 1.2456, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 182, Loss: 1.2455, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 183, Loss: 1.2455, Train: 1.1169, Val: 1.1174\n",
      "Epoch: 184, Loss: 1.2459, Train: 1.1170, Val: 1.1176\n",
      "Epoch: 185, Loss: 1.2458, Train: 1.1171, Val: 1.1177\n",
      "Epoch: 186, Loss: 1.2461, Train: 1.1180, Val: 1.1187\n",
      "Epoch: 187, Loss: 1.2462, Train: 1.1172, Val: 1.1179\n",
      "Epoch: 188, Loss: 1.2462, Train: 1.1174, Val: 1.1181\n",
      "Epoch: 189, Loss: 1.2460, Train: 1.1169, Val: 1.1175\n",
      "Epoch: 190, Loss: 1.2459, Train: 1.1168, Val: 1.1173\n",
      "Epoch: 191, Loss: 1.2458, Train: 1.1168, Val: 1.1173\n",
      "Epoch: 192, Loss: 1.2459, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 193, Loss: 1.2459, Train: 1.1166, Val: 1.1172\n",
      "Epoch: 194, Loss: 1.2459, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 195, Loss: 1.2458, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 196, Loss: 1.2457, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 197, Loss: 1.2456, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 198, Loss: 1.2456, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 199, Loss: 1.2456, Train: 1.1162, Val: 1.1166\n",
      "Epoch: 200, Loss: 1.2457, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 201, Loss: 1.2458, Train: 1.1162, Val: 1.1166\n",
      "Epoch: 202, Loss: 1.2460, Train: 1.1167, Val: 1.1172\n",
      "Epoch: 203, Loss: 1.2463, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 204, Loss: 1.2468, Train: 1.1176, Val: 1.1182\n",
      "Epoch: 205, Loss: 1.2472, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 206, Loss: 1.2472, Train: 1.1173, Val: 1.1179\n",
      "Epoch: 207, Loss: 1.2466, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 208, Loss: 1.2458, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 209, Loss: 1.2455, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 210, Loss: 1.2458, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 211, Loss: 1.2462, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 212, Loss: 1.2466, Train: 1.1171, Val: 1.1174\n",
      "Epoch: 213, Loss: 1.2468, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 214, Loss: 1.2467, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 215, Loss: 1.2461, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 216, Loss: 1.2456, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 217, Loss: 1.2454, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 218, Loss: 1.2457, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 219, Loss: 1.2460, Train: 1.1170, Val: 1.1173\n",
      "Epoch: 220, Loss: 1.2460, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 221, Loss: 1.2457, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 222, Loss: 1.2454, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 223, Loss: 1.2452, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 224, Loss: 1.2454, Train: 1.1180, Val: 1.1183\n",
      "Epoch: 225, Loss: 1.2458, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 226, Loss: 1.2463, Train: 1.1186, Val: 1.1189\n",
      "Epoch: 227, Loss: 1.2469, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 228, Loss: 1.2469, Train: 1.1179, Val: 1.1181\n",
      "Epoch: 229, Loss: 1.2462, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 230, Loss: 1.2454, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 231, Loss: 1.2450, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 232, Loss: 1.2449, Train: 1.1159, Val: 1.1164\n",
      "Epoch: 233, Loss: 1.2448, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 234, Loss: 1.2448, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 235, Loss: 1.2447, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 236, Loss: 1.2463, Train: 1.1194, Val: 1.1200\n",
      "Epoch: 237, Loss: 1.2488, Train: 1.1173, Val: 1.1176\n",
      "Epoch: 238, Loss: 1.2497, Train: 1.1200, Val: 1.1206\n",
      "Epoch: 239, Loss: 1.2486, Train: 1.1162, Val: 1.1166\n",
      "Epoch: 240, Loss: 1.2461, Train: 1.1167, Val: 1.1172\n",
      "Epoch: 241, Loss: 1.2452, Train: 1.1194, Val: 1.1199\n",
      "Epoch: 242, Loss: 1.2464, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 243, Loss: 1.2475, Train: 1.1210, Val: 1.1216\n",
      "Epoch: 244, Loss: 1.2472, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 245, Loss: 1.2457, Train: 1.1173, Val: 1.1178\n",
      "Epoch: 246, Loss: 1.2451, Train: 1.1195, Val: 1.1200\n",
      "Epoch: 247, Loss: 1.2457, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 248, Loss: 1.2464, Train: 1.1200, Val: 1.1206\n",
      "Epoch: 249, Loss: 1.2462, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 250, Loss: 1.2452, Train: 1.1169, Val: 1.1174\n",
      "Epoch: 251, Loss: 1.2447, Train: 1.1179, Val: 1.1184\n",
      "Epoch: 252, Loss: 1.2450, Train: 1.1159, Val: 1.1162\n",
      "Epoch: 253, Loss: 1.2454, Train: 1.1186, Val: 1.1191\n",
      "Epoch: 254, Loss: 1.2453, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 255, Loss: 1.2443, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 256, Loss: 1.2657, Train: 1.1950, Val: 1.1951\n",
      "Epoch: 257, Loss: 1.3607, Train: 1.2482, Val: 1.2501\n",
      "Epoch: 258, Loss: 1.4638, Train: 1.1285, Val: 1.1290\n",
      "Epoch: 259, Loss: 1.2555, Train: 1.1414, Val: 1.1413\n",
      "Epoch: 260, Loss: 1.3641, Train: 1.2390, Val: 1.2402\n",
      "Epoch: 261, Loss: 1.3605, Train: 1.1741, Val: 1.1748\n",
      "Epoch: 262, Loss: 1.2838, Train: 1.1592, Val: 1.1585\n",
      "Epoch: 263, Loss: 1.3797, Train: 1.1270, Val: 1.1270\n",
      "Epoch: 264, Loss: 1.2473, Train: 1.1858, Val: 1.1865\n",
      "Epoch: 265, Loss: 1.3373, Train: 1.1211, Val: 1.1212\n",
      "Epoch: 266, Loss: 1.2474, Train: 1.1557, Val: 1.1552\n",
      "Epoch: 267, Loss: 1.3260, Train: 1.1209, Val: 1.1213\n",
      "Epoch: 268, Loss: 1.2505, Train: 1.1476, Val: 1.1485\n",
      "Epoch: 269, Loss: 1.3036, Train: 1.1177, Val: 1.1180\n",
      "Epoch: 270, Loss: 1.2471, Train: 1.1471, Val: 1.1469\n",
      "Epoch: 271, Loss: 1.2966, Train: 1.1174, Val: 1.1178\n",
      "Epoch: 272, Loss: 1.2475, Train: 1.1329, Val: 1.1338\n",
      "Epoch: 273, Loss: 1.2840, Train: 1.1172, Val: 1.1178\n",
      "Epoch: 274, Loss: 1.2467, Train: 1.1388, Val: 1.1389\n",
      "Epoch: 275, Loss: 1.2784, Train: 1.1177, Val: 1.1181\n",
      "Epoch: 276, Loss: 1.2469, Train: 1.1263, Val: 1.1272\n",
      "Epoch: 277, Loss: 1.2693, Train: 1.1172, Val: 1.1178\n",
      "Epoch: 278, Loss: 1.2492, Train: 1.1284, Val: 1.1285\n",
      "Epoch: 279, Loss: 1.2626, Train: 1.1206, Val: 1.1208\n",
      "Epoch: 280, Loss: 1.2505, Train: 1.1199, Val: 1.1205\n",
      "Epoch: 281, Loss: 1.2565, Train: 1.1186, Val: 1.1192\n",
      "Epoch: 282, Loss: 1.2532, Train: 1.1212, Val: 1.1214\n",
      "Epoch: 283, Loss: 1.2512, Train: 1.1227, Val: 1.1228\n",
      "Epoch: 284, Loss: 1.2540, Train: 1.1171, Val: 1.1177\n",
      "Epoch: 285, Loss: 1.2483, Train: 1.1199, Val: 1.1205\n",
      "Epoch: 286, Loss: 1.2541, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 287, Loss: 1.2463, Train: 1.1212, Val: 1.1214\n",
      "Epoch: 288, Loss: 1.2535, Train: 1.1164, Val: 1.1167\n",
      "Epoch: 289, Loss: 1.2459, Train: 1.1191, Val: 1.1197\n",
      "Epoch: 290, Loss: 1.2518, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 291, Loss: 1.2462, Train: 1.1187, Val: 1.1189\n",
      "Epoch: 292, Loss: 1.2501, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 293, Loss: 1.2466, Train: 1.1182, Val: 1.1187\n",
      "Epoch: 294, Loss: 1.2484, Train: 1.1176, Val: 1.1181\n",
      "Epoch: 295, Loss: 1.2472, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 296, Loss: 1.2470, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 297, Loss: 1.2475, Train: 1.1174, Val: 1.1178\n",
      "Epoch: 298, Loss: 1.2461, Train: 1.1188, Val: 1.1193\n",
      "Epoch: 299, Loss: 1.2475, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 300, Loss: 1.2455, Train: 1.1163, Val: 1.1166\n",
      "Epoch: 301, Loss: 1.2472, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 302, Loss: 1.2453, Train: 1.1185, Val: 1.1190\n",
      "Epoch: 303, Loss: 1.2468, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 304, Loss: 1.2452, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 305, Loss: 1.2463, Train: 1.1161, Val: 1.1165\n",
      "Epoch: 306, Loss: 1.2451, Train: 1.1174, Val: 1.1179\n",
      "Epoch: 307, Loss: 1.2457, Train: 1.1168, Val: 1.1173\n",
      "Epoch: 308, Loss: 1.2451, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 309, Loss: 1.2455, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 310, Loss: 1.2451, Train: 1.1172, Val: 1.1177\n",
      "Epoch: 311, Loss: 1.2451, Train: 1.1170, Val: 1.1176\n",
      "Epoch: 312, Loss: 1.2450, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 313, Loss: 1.2447, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 314, Loss: 1.2447, Train: 1.1164, Val: 1.1169\n",
      "Epoch: 315, Loss: 1.2459, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 316, Loss: 1.2448, Train: 1.1176, Val: 1.1180\n",
      "Epoch: 317, Loss: 1.2454, Train: 1.1191, Val: 1.1196\n",
      "Epoch: 318, Loss: 1.2445, Train: 1.1250, Val: 1.1257\n",
      "Epoch: 319, Loss: 1.2447, Train: 1.1258, Val: 1.1265\n",
      "Epoch: 320, Loss: 1.2442, Train: 1.1223, Val: 1.1228\n",
      "Epoch: 321, Loss: 1.2443, Train: 1.1221, Val: 1.1227\n",
      "Epoch: 322, Loss: 1.2439, Train: 1.1228, Val: 1.1234\n",
      "Epoch: 323, Loss: 1.2440, Train: 1.1205, Val: 1.1210\n",
      "Epoch: 324, Loss: 1.2436, Train: 1.1178, Val: 1.1183\n",
      "Epoch: 325, Loss: 1.2436, Train: 1.1173, Val: 1.1179\n",
      "Epoch: 326, Loss: 1.2434, Train: 1.1179, Val: 1.1185\n",
      "Epoch: 327, Loss: 1.2432, Train: 1.1175, Val: 1.1181\n",
      "Epoch: 328, Loss: 1.2428, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 329, Loss: 1.2424, Train: 1.1169, Val: 1.1174\n",
      "Epoch: 330, Loss: 1.2421, Train: 1.1177, Val: 1.1181\n",
      "Epoch: 331, Loss: 1.2427, Train: 1.1186, Val: 1.1190\n",
      "Epoch: 332, Loss: 1.2417, Train: 1.1218, Val: 1.1224\n",
      "Epoch: 333, Loss: 1.2419, Train: 1.1277, Val: 1.1284\n",
      "Epoch: 334, Loss: 1.2420, Train: 1.1301, Val: 1.1308\n",
      "Epoch: 335, Loss: 1.2414, Train: 1.1311, Val: 1.1318\n",
      "Epoch: 336, Loss: 1.2402, Train: 1.1298, Val: 1.1303\n",
      "Epoch: 337, Loss: 1.2402, Train: 1.1270, Val: 1.1275\n",
      "Epoch: 338, Loss: 1.2396, Train: 1.1282, Val: 1.1288\n",
      "Epoch: 339, Loss: 1.2385, Train: 1.1344, Val: 1.1352\n",
      "Epoch: 340, Loss: 1.2405, Train: 1.1302, Val: 1.1307\n",
      "Epoch: 341, Loss: 1.2467, Train: 1.1328, Val: 1.1334\n",
      "Epoch: 342, Loss: 1.2409, Train: 1.1278, Val: 1.1281\n",
      "Epoch: 343, Loss: 1.2403, Train: 1.1263, Val: 1.1264\n",
      "Epoch: 344, Loss: 1.2405, Train: 1.1256, Val: 1.1259\n",
      "Epoch: 345, Loss: 1.2352, Train: 1.1276, Val: 1.1281\n",
      "Epoch: 346, Loss: 1.2386, Train: 1.1294, Val: 1.1299\n",
      "Epoch: 347, Loss: 1.2375, Train: 1.1275, Val: 1.1275\n",
      "Epoch: 348, Loss: 1.2308, Train: 1.1315, Val: 1.1312\n",
      "Epoch: 349, Loss: 1.2340, Train: 1.1241, Val: 1.1242\n",
      "Epoch: 350, Loss: 1.2304, Train: 1.1295, Val: 1.1303\n",
      "Epoch: 351, Loss: 1.2570, Train: 1.1219, Val: 1.1222\n",
      "Epoch: 352, Loss: 1.2259, Train: 1.1280, Val: 1.1280\n",
      "Epoch: 353, Loss: 1.2317, Train: 1.1460, Val: 1.1458\n",
      "Epoch: 354, Loss: 1.2519, Train: 1.1226, Val: 1.1232\n",
      "Epoch: 355, Loss: 1.2467, Train: 1.1182, Val: 1.1186\n",
      "Epoch: 356, Loss: 1.2410, Train: 1.1537, Val: 1.1535\n",
      "Epoch: 357, Loss: 1.2611, Train: 1.1261, Val: 1.1261\n",
      "Epoch: 358, Loss: 1.2377, Train: 1.1152, Val: 1.1157\n",
      "Epoch: 359, Loss: 1.2495, Train: 1.1179, Val: 1.1184\n",
      "Epoch: 360, Loss: 1.2390, Train: 1.1403, Val: 1.1404\n",
      "Epoch: 361, Loss: 1.2438, Train: 1.1329, Val: 1.1331\n",
      "Epoch: 362, Loss: 1.2368, Train: 1.1190, Val: 1.1196\n",
      "Epoch: 363, Loss: 1.2537, Train: 1.1158, Val: 1.1163\n",
      "Epoch: 364, Loss: 1.2429, Train: 1.1368, Val: 1.1369\n",
      "Epoch: 365, Loss: 1.2447, Train: 1.1505, Val: 1.1505\n",
      "Epoch: 366, Loss: 1.2458, Train: 1.1201, Val: 1.1205\n",
      "Epoch: 367, Loss: 1.2417, Train: 1.1168, Val: 1.1173\n",
      "Epoch: 368, Loss: 1.2463, Train: 1.1449, Val: 1.1450\n",
      "Epoch: 369, Loss: 1.2390, Train: 1.1490, Val: 1.1491\n",
      "Epoch: 370, Loss: 1.2438, Train: 1.1134, Val: 1.1140\n",
      "Epoch: 371, Loss: 1.2338, Train: 1.1099, Val: 1.1109\n",
      "Epoch: 372, Loss: 1.2390, Train: 1.1137, Val: 1.1147\n",
      "Epoch: 373, Loss: 1.2282, Train: 1.1232, Val: 1.1245\n",
      "Epoch: 374, Loss: 1.2313, Train: 1.1258, Val: 1.1273\n",
      "Epoch: 375, Loss: 1.2388, Train: 1.1134, Val: 1.1150\n",
      "Epoch: 376, Loss: 1.2368, Train: 1.1186, Val: 1.1198\n",
      "Epoch: 377, Loss: 1.2369, Train: 1.1265, Val: 1.1278\n",
      "Epoch: 378, Loss: 1.2386, Train: 1.1187, Val: 1.1205\n",
      "Epoch: 379, Loss: 1.2294, Train: 1.1309, Val: 1.1331\n",
      "Epoch: 380, Loss: 1.2295, Train: 1.1346, Val: 1.1367\n",
      "Epoch: 381, Loss: 1.2254, Train: 1.1287, Val: 1.1306\n",
      "Epoch: 382, Loss: 1.2291, Train: 1.1292, Val: 1.1310\n",
      "Epoch: 383, Loss: 1.2243, Train: 1.1271, Val: 1.1290\n",
      "Epoch: 384, Loss: 1.2224, Train: 1.1159, Val: 1.1179\n",
      "Epoch: 385, Loss: 1.2168, Train: 1.1157, Val: 1.1177\n",
      "Epoch: 386, Loss: 1.2188, Train: 1.1145, Val: 1.1165\n",
      "Epoch: 387, Loss: 1.2169, Train: 1.1080, Val: 1.1099\n",
      "Epoch: 388, Loss: 1.2119, Train: 1.1075, Val: 1.1088\n",
      "Epoch: 389, Loss: 1.2112, Train: 1.1109, Val: 1.1118\n",
      "Epoch: 390, Loss: 1.2363, Train: 1.1295, Val: 1.1298\n",
      "Epoch: 391, Loss: 1.2148, Train: 1.1252, Val: 1.1257\n",
      "Epoch: 392, Loss: 1.2116, Train: 1.1223, Val: 1.1228\n",
      "Epoch: 393, Loss: 1.2289, Train: 1.1425, Val: 1.1425\n",
      "Epoch: 394, Loss: 1.2181, Train: 1.1738, Val: 1.1734\n",
      "Epoch: 395, Loss: 1.2188, Train: 1.1655, Val: 1.1651\n",
      "Epoch: 396, Loss: 1.2175, Train: 1.1724, Val: 1.1721\n",
      "Epoch: 397, Loss: 1.2136, Train: 1.2001, Val: 1.1997\n",
      "Epoch: 398, Loss: 1.2150, Train: 1.2409, Val: 1.2404\n",
      "Epoch: 399, Loss: 1.2149, Train: 1.2325, Val: 1.2322\n",
      "Epoch: 400, Loss: 1.2052, Train: 1.1979, Val: 1.1979\n",
      "Epoch: 401, Loss: 1.2079, Train: 1.2191, Val: 1.2190\n",
      "Epoch: 402, Loss: 1.2027, Train: 1.2362, Val: 1.2360\n",
      "Epoch: 403, Loss: 1.2053, Train: 1.1891, Val: 1.1891\n",
      "Epoch: 404, Loss: 1.2020, Train: 1.1716, Val: 1.1718\n",
      "Epoch: 405, Loss: 1.2049, Train: 1.2117, Val: 1.2118\n",
      "Epoch: 406, Loss: 1.1988, Train: 1.2486, Val: 1.2485\n",
      "Epoch: 407, Loss: 1.1991, Train: 1.1994, Val: 1.1996\n",
      "Epoch: 408, Loss: 1.1956, Train: 1.1556, Val: 1.1561\n",
      "Epoch: 409, Loss: 1.1958, Train: 1.1406, Val: 1.1413\n",
      "Epoch: 410, Loss: 1.1935, Train: 1.1348, Val: 1.1355\n",
      "Epoch: 411, Loss: 1.1955, Train: 1.1297, Val: 1.1299\n",
      "Epoch: 412, Loss: 1.1952, Train: 1.1187, Val: 1.1191\n",
      "Epoch: 413, Loss: 1.1965, Train: 1.1147, Val: 1.1152\n",
      "Epoch: 414, Loss: 1.1934, Train: 1.1118, Val: 1.1125\n",
      "Epoch: 415, Loss: 1.1927, Train: 1.1065, Val: 1.1074\n",
      "Epoch: 416, Loss: 1.1900, Train: 1.1077, Val: 1.1086\n",
      "Epoch: 417, Loss: 1.1903, Train: 1.1118, Val: 1.1125\n",
      "Epoch: 418, Loss: 1.1874, Train: 1.1183, Val: 1.1187\n",
      "Epoch: 419, Loss: 1.1881, Train: 1.1252, Val: 1.1255\n",
      "Epoch: 420, Loss: 1.1886, Train: 1.1276, Val: 1.1278\n",
      "Epoch: 421, Loss: 1.1867, Train: 1.1328, Val: 1.1328\n",
      "Epoch: 422, Loss: 1.1862, Train: 1.1271, Val: 1.1273\n",
      "Epoch: 423, Loss: 1.1852, Train: 1.1117, Val: 1.1123\n",
      "Epoch: 424, Loss: 1.1849, Train: 1.1067, Val: 1.1073\n",
      "Epoch: 425, Loss: 1.1835, Train: 1.1076, Val: 1.1081\n",
      "Epoch: 426, Loss: 1.1820, Train: 1.1056, Val: 1.1062\n",
      "Epoch: 427, Loss: 1.1821, Train: 1.1040, Val: 1.1049\n",
      "Epoch: 428, Loss: 1.1813, Train: 1.1018, Val: 1.1029\n",
      "Epoch: 429, Loss: 1.1796, Train: 1.0965, Val: 1.0976\n",
      "Epoch: 430, Loss: 1.1785, Train: 1.0952, Val: 1.0964\n",
      "Epoch: 431, Loss: 1.1783, Train: 1.0975, Val: 1.0987\n",
      "Epoch: 432, Loss: 1.1776, Train: 1.0977, Val: 1.0987\n",
      "Epoch: 433, Loss: 1.1768, Train: 1.0966, Val: 1.0975\n",
      "Epoch: 434, Loss: 1.1767, Train: 1.0953, Val: 1.0962\n",
      "Epoch: 435, Loss: 1.1760, Train: 1.0934, Val: 1.0946\n",
      "Epoch: 436, Loss: 1.1753, Train: 1.0903, Val: 1.0918\n",
      "Epoch: 437, Loss: 1.1743, Train: 1.0877, Val: 1.0893\n",
      "Epoch: 438, Loss: 1.1741, Train: 1.0873, Val: 1.0890\n",
      "Epoch: 439, Loss: 1.1734, Train: 1.0886, Val: 1.0903\n",
      "Epoch: 440, Loss: 1.1730, Train: 1.0889, Val: 1.0903\n",
      "Epoch: 441, Loss: 1.1723, Train: 1.0888, Val: 1.0900\n",
      "Epoch: 442, Loss: 1.1722, Train: 1.0885, Val: 1.0897\n",
      "Epoch: 443, Loss: 1.1714, Train: 1.0880, Val: 1.0893\n",
      "Epoch: 444, Loss: 1.1711, Train: 1.0880, Val: 1.0892\n",
      "Epoch: 445, Loss: 1.1706, Train: 1.0887, Val: 1.0897\n",
      "Epoch: 446, Loss: 1.1703, Train: 1.0884, Val: 1.0895\n",
      "Epoch: 447, Loss: 1.1697, Train: 1.0880, Val: 1.0892\n",
      "Epoch: 448, Loss: 1.1695, Train: 1.0877, Val: 1.0890\n",
      "Epoch: 449, Loss: 1.1690, Train: 1.0869, Val: 1.0882\n",
      "Epoch: 450, Loss: 1.1687, Train: 1.0857, Val: 1.0872\n",
      "Epoch: 451, Loss: 1.1683, Train: 1.0849, Val: 1.0865\n",
      "Epoch: 452, Loss: 1.1681, Train: 1.0842, Val: 1.0858\n",
      "Epoch: 453, Loss: 1.1676, Train: 1.0838, Val: 1.0855\n",
      "Epoch: 454, Loss: 1.1673, Train: 1.0826, Val: 1.0844\n",
      "Epoch: 455, Loss: 1.1671, Train: 1.0815, Val: 1.0834\n",
      "Epoch: 456, Loss: 1.1668, Train: 1.0811, Val: 1.0831\n",
      "Epoch: 457, Loss: 1.1664, Train: 1.0810, Val: 1.0832\n",
      "Epoch: 458, Loss: 1.1661, Train: 1.0808, Val: 1.0830\n",
      "Epoch: 459, Loss: 1.1659, Train: 1.0810, Val: 1.0832\n",
      "Epoch: 460, Loss: 1.1656, Train: 1.0818, Val: 1.0840\n",
      "Epoch: 461, Loss: 1.1653, Train: 1.0818, Val: 1.0840\n",
      "Epoch: 462, Loss: 1.1650, Train: 1.0813, Val: 1.0836\n",
      "Epoch: 463, Loss: 1.1648, Train: 1.0827, Val: 1.0849\n",
      "Epoch: 464, Loss: 1.1645, Train: 1.0842, Val: 1.0863\n",
      "Epoch: 465, Loss: 1.1643, Train: 1.0840, Val: 1.0862\n",
      "Epoch: 466, Loss: 1.1640, Train: 1.0835, Val: 1.0858\n",
      "Epoch: 467, Loss: 1.1638, Train: 1.0832, Val: 1.0856\n",
      "Epoch: 468, Loss: 1.1636, Train: 1.0835, Val: 1.0859\n",
      "Epoch: 469, Loss: 1.1634, Train: 1.0825, Val: 1.0850\n",
      "Epoch: 470, Loss: 1.1631, Train: 1.0829, Val: 1.0854\n",
      "Epoch: 471, Loss: 1.1629, Train: 1.0839, Val: 1.0864\n",
      "Epoch: 472, Loss: 1.1627, Train: 1.0833, Val: 1.0859\n",
      "Epoch: 473, Loss: 1.1625, Train: 1.0838, Val: 1.0863\n",
      "Epoch: 474, Loss: 1.1625, Train: 1.0830, Val: 1.0856\n",
      "Epoch: 475, Loss: 1.1628, Train: 1.0859, Val: 1.0883\n",
      "Epoch: 476, Loss: 1.1626, Train: 1.0838, Val: 1.0863\n",
      "Epoch: 477, Loss: 1.1623, Train: 1.0859, Val: 1.0881\n",
      "Epoch: 478, Loss: 1.1616, Train: 1.0855, Val: 1.0877\n",
      "Epoch: 479, Loss: 1.1614, Train: 1.0842, Val: 1.0865\n",
      "Epoch: 480, Loss: 1.1618, Train: 1.0928, Val: 1.0944\n",
      "Epoch: 481, Loss: 1.1635, Train: 1.0832, Val: 1.0856\n",
      "Epoch: 482, Loss: 1.1636, Train: 1.0848, Val: 1.0880\n",
      "Epoch: 483, Loss: 1.1618, Train: 1.0864, Val: 1.0900\n",
      "Epoch: 484, Loss: 1.1619, Train: 1.0841, Val: 1.0877\n",
      "Epoch: 485, Loss: 1.1618, Train: 1.0819, Val: 1.0850\n",
      "Epoch: 486, Loss: 1.1610, Train: 1.0836, Val: 1.0863\n",
      "Epoch: 487, Loss: 1.1621, Train: 1.0805, Val: 1.0830\n",
      "Epoch: 488, Loss: 1.1621, Train: 1.0787, Val: 1.0816\n",
      "Epoch: 489, Loss: 1.1606, Train: 1.0785, Val: 1.0815\n",
      "Epoch: 490, Loss: 1.1616, Train: 1.0782, Val: 1.0811\n",
      "Epoch: 491, Loss: 1.1600, Train: 1.0792, Val: 1.0815\n",
      "Epoch: 492, Loss: 1.1611, Train: 1.0796, Val: 1.0819\n",
      "Epoch: 493, Loss: 1.1599, Train: 1.0777, Val: 1.0806\n",
      "Epoch: 494, Loss: 1.1603, Train: 1.0795, Val: 1.0817\n",
      "Epoch: 495, Loss: 1.1604, Train: 1.0810, Val: 1.0831\n",
      "Epoch: 496, Loss: 1.1598, Train: 1.0824, Val: 1.0844\n",
      "Epoch: 497, Loss: 1.1595, Train: 1.0817, Val: 1.0837\n",
      "Epoch: 498, Loss: 1.1602, Train: 1.0783, Val: 1.0808\n",
      "Epoch: 499, Loss: 1.1586, Train: 1.0778, Val: 1.0806\n",
      "Epoch: 500, Loss: 1.1598, Train: 1.0807, Val: 1.0829\n",
      "Test RMSE: 1.0894\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.653458       3.583673\n",
      "std      1727.484387     741.673176       0.257493       1.116938\n",
      "min         0.000000       0.000000       2.114229       1.000000\n",
      "25%      1500.000000     259.000000       3.602080       3.000000\n",
      "50%      3066.000000     693.000000       3.602080       4.000000\n",
      "75%      4472.000000    1292.000000       3.778620       4.000000\n",
      "max      6039.000000    3702.000000       4.737682       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1099.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80909243415533\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  3\n",
      "Kernel:  gd\n",
      "Model Name:  KPGINPlus\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 15.2259, Train: 3.2461, Val: 3.2492\n",
      "Epoch: 002, Loss: 12.1617, Train: 3.1714, Val: 3.1746\n",
      "Epoch: 003, Loss: 8.0664, Train: 3.0537, Val: 3.0566\n",
      "Epoch: 004, Loss: 3.8080, Train: 2.5896, Val: 2.5913\n",
      "Epoch: 005, Loss: 2.2186, Train: 2.1425, Val: 2.1433\n",
      "Epoch: 006, Loss: 3.7558, Train: 2.0620, Val: 2.0631\n",
      "Epoch: 007, Loss: 2.0568, Train: 2.1302, Val: 2.1313\n",
      "Epoch: 008, Loss: 1.4754, Train: 2.1902, Val: 2.1915\n",
      "Epoch: 009, Loss: 1.8447, Train: 2.1642, Val: 2.1656\n",
      "Epoch: 010, Loss: 1.9042, Train: 2.0136, Val: 2.0151\n",
      "Epoch: 011, Loss: 1.6013, Train: 1.7796, Val: 1.7810\n",
      "Epoch: 012, Loss: 1.3384, Train: 1.5514, Val: 1.5527\n",
      "Epoch: 013, Loss: 1.4214, Train: 1.4318, Val: 1.4329\n",
      "Epoch: 014, Loss: 1.5945, Train: 1.4313, Val: 1.4325\n",
      "Epoch: 015, Loss: 1.4931, Train: 1.5087, Val: 1.5102\n",
      "Epoch: 016, Loss: 1.3217, Train: 1.6153, Val: 1.6172\n",
      "Epoch: 017, Loss: 1.3157, Train: 1.6675, Val: 1.6696\n",
      "Epoch: 018, Loss: 1.3840, Train: 1.6376, Val: 1.6398\n",
      "Epoch: 019, Loss: 1.3928, Train: 1.5380, Val: 1.5402\n",
      "Epoch: 020, Loss: 1.3312, Train: 1.4125, Val: 1.4145\n",
      "Epoch: 021, Loss: 1.2754, Train: 1.3120, Val: 1.3140\n",
      "Epoch: 022, Loss: 1.2927, Train: 1.2594, Val: 1.2614\n",
      "Epoch: 023, Loss: 1.3422, Train: 1.2493, Val: 1.2513\n",
      "Epoch: 024, Loss: 1.3334, Train: 1.2769, Val: 1.2790\n",
      "Epoch: 025, Loss: 1.2779, Train: 1.3349, Val: 1.3370\n",
      "Epoch: 026, Loss: 1.2537, Train: 1.3921, Val: 1.3944\n",
      "Epoch: 027, Loss: 1.2792, Train: 1.4126, Val: 1.4150\n",
      "Epoch: 028, Loss: 1.3078, Train: 1.3818, Val: 1.3841\n",
      "Epoch: 029, Loss: 1.3021, Train: 1.3112, Val: 1.3134\n",
      "Epoch: 030, Loss: 1.2743, Train: 1.2344, Val: 1.2363\n",
      "Epoch: 031, Loss: 1.2591, Train: 1.1826, Val: 1.1844\n",
      "Epoch: 032, Loss: 1.2684, Train: 1.1638, Val: 1.1655\n",
      "Epoch: 033, Loss: 1.2778, Train: 1.1643, Val: 1.1660\n",
      "Epoch: 034, Loss: 1.2689, Train: 1.1737, Val: 1.1755\n",
      "Epoch: 035, Loss: 1.2568, Train: 1.1865, Val: 1.1883\n",
      "Epoch: 036, Loss: 1.2594, Train: 1.1937, Val: 1.1955\n",
      "Epoch: 037, Loss: 1.2688, Train: 1.1869, Val: 1.1887\n",
      "Epoch: 038, Loss: 1.2676, Train: 1.1670, Val: 1.1687\n",
      "Epoch: 039, Loss: 1.2556, Train: 1.1448, Val: 1.1463\n",
      "Epoch: 040, Loss: 1.2481, Train: 1.1302, Val: 1.1314\n",
      "Epoch: 041, Loss: 1.2535, Train: 1.1243, Val: 1.1253\n",
      "Epoch: 042, Loss: 1.2606, Train: 1.1236, Val: 1.1245\n",
      "Epoch: 043, Loss: 1.2579, Train: 1.1269, Val: 1.1279\n",
      "Epoch: 044, Loss: 1.2512, Train: 1.1329, Val: 1.1341\n",
      "Epoch: 045, Loss: 1.2508, Train: 1.1366, Val: 1.1378\n",
      "Epoch: 046, Loss: 1.2546, Train: 1.1339, Val: 1.1351\n",
      "Epoch: 047, Loss: 1.2545, Train: 1.1275, Val: 1.1286\n",
      "Epoch: 048, Loss: 1.2503, Train: 1.1234, Val: 1.1244\n",
      "Epoch: 049, Loss: 1.2487, Train: 1.1233, Val: 1.1242\n",
      "Epoch: 050, Loss: 1.2515, Train: 1.1234, Val: 1.1241\n",
      "Epoch: 051, Loss: 1.2529, Train: 1.1212, Val: 1.1219\n",
      "Epoch: 052, Loss: 1.2500, Train: 1.1195, Val: 1.1202\n",
      "Epoch: 053, Loss: 1.2475, Train: 1.1202, Val: 1.1210\n",
      "Epoch: 054, Loss: 1.2486, Train: 1.1214, Val: 1.1221\n",
      "Epoch: 055, Loss: 1.2503, Train: 1.1203, Val: 1.1210\n",
      "Epoch: 056, Loss: 1.2494, Train: 1.1181, Val: 1.1186\n",
      "Epoch: 057, Loss: 1.2475, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 058, Loss: 1.2477, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 059, Loss: 1.2487, Train: 1.1170, Val: 1.1174\n",
      "Epoch: 060, Loss: 1.2482, Train: 1.1172, Val: 1.1177\n",
      "Epoch: 061, Loss: 1.2470, Train: 1.1180, Val: 1.1186\n",
      "Epoch: 062, Loss: 1.2471, Train: 1.1186, Val: 1.1192\n",
      "Epoch: 063, Loss: 1.2479, Train: 1.1183, Val: 1.1188\n",
      "Epoch: 064, Loss: 1.2476, Train: 1.1173, Val: 1.1178\n",
      "Epoch: 065, Loss: 1.2466, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 066, Loss: 1.2464, Train: 1.1168, Val: 1.1170\n",
      "Epoch: 067, Loss: 1.2470, Train: 1.1167, Val: 1.1169\n",
      "Epoch: 068, Loss: 1.2468, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 069, Loss: 1.2463, Train: 1.1169, Val: 1.1172\n",
      "Epoch: 070, Loss: 1.2457, Train: 1.1172, Val: 1.1176\n",
      "Epoch: 071, Loss: 1.2456, Train: 1.1172, Val: 1.1176\n",
      "Epoch: 072, Loss: 1.2453, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 073, Loss: 1.2442, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 074, Loss: 1.2506, Train: 1.1213, Val: 1.1216\n",
      "Epoch: 075, Loss: 1.2477, Train: 1.1263, Val: 1.1267\n",
      "Epoch: 076, Loss: 1.2492, Train: 1.1234, Val: 1.1239\n",
      "Epoch: 077, Loss: 1.2484, Train: 1.1199, Val: 1.1205\n",
      "Epoch: 078, Loss: 1.2474, Train: 1.1238, Val: 1.1246\n",
      "Epoch: 079, Loss: 1.2481, Train: 1.1296, Val: 1.1305\n",
      "Epoch: 080, Loss: 1.2485, Train: 1.1288, Val: 1.1296\n",
      "Epoch: 081, Loss: 1.2473, Train: 1.1247, Val: 1.1253\n",
      "Epoch: 082, Loss: 1.2465, Train: 1.1220, Val: 1.1225\n",
      "Epoch: 083, Loss: 1.2472, Train: 1.1217, Val: 1.1222\n",
      "Epoch: 084, Loss: 1.2474, Train: 1.1233, Val: 1.1238\n",
      "Epoch: 085, Loss: 1.2467, Train: 1.1256, Val: 1.1262\n",
      "Epoch: 086, Loss: 1.2465, Train: 1.1266, Val: 1.1272\n",
      "Epoch: 087, Loss: 1.2469, Train: 1.1246, Val: 1.1252\n",
      "Epoch: 088, Loss: 1.2465, Train: 1.1210, Val: 1.1216\n",
      "Epoch: 089, Loss: 1.2459, Train: 1.1183, Val: 1.1188\n",
      "Epoch: 090, Loss: 1.2460, Train: 1.1174, Val: 1.1179\n",
      "Epoch: 091, Loss: 1.2462, Train: 1.1177, Val: 1.1182\n",
      "Epoch: 092, Loss: 1.2458, Train: 1.1191, Val: 1.1197\n",
      "Epoch: 093, Loss: 1.2454, Train: 1.1208, Val: 1.1214\n",
      "Epoch: 094, Loss: 1.2453, Train: 1.1213, Val: 1.1219\n",
      "Epoch: 095, Loss: 1.2451, Train: 1.1198, Val: 1.1204\n",
      "Epoch: 096, Loss: 1.2445, Train: 1.1186, Val: 1.1192\n",
      "Epoch: 097, Loss: 1.2442, Train: 1.1183, Val: 1.1189\n",
      "Epoch: 098, Loss: 1.2434, Train: 1.1190, Val: 1.1196\n",
      "Epoch: 099, Loss: 1.2466, Train: 1.1188, Val: 1.1194\n",
      "Epoch: 100, Loss: 1.2428, Train: 1.1195, Val: 1.1201\n",
      "Epoch: 101, Loss: 1.2419, Train: 1.1219, Val: 1.1225\n",
      "Epoch: 102, Loss: 1.2462, Train: 1.1231, Val: 1.1237\n",
      "Epoch: 103, Loss: 1.2461, Train: 1.1231, Val: 1.1237\n",
      "Epoch: 104, Loss: 1.2457, Train: 1.1237, Val: 1.1243\n",
      "Epoch: 105, Loss: 1.2447, Train: 1.1252, Val: 1.1259\n",
      "Epoch: 106, Loss: 1.2467, Train: 1.1232, Val: 1.1238\n",
      "Epoch: 107, Loss: 1.2455, Train: 1.1220, Val: 1.1226\n",
      "Epoch: 108, Loss: 1.2448, Train: 1.1226, Val: 1.1233\n",
      "Epoch: 109, Loss: 1.2547, Train: 1.1179, Val: 1.1183\n",
      "Epoch: 110, Loss: 1.2543, Train: 1.1182, Val: 1.1188\n",
      "Epoch: 111, Loss: 1.2513, Train: 1.1268, Val: 1.1276\n",
      "Epoch: 112, Loss: 1.2449, Train: 1.1522, Val: 1.1534\n",
      "Epoch: 113, Loss: 1.2450, Train: 1.1738, Val: 1.1750\n",
      "Epoch: 114, Loss: 1.2433, Train: 1.1644, Val: 1.1655\n",
      "Epoch: 115, Loss: 1.2401, Train: 1.1441, Val: 1.1451\n",
      "Epoch: 116, Loss: 1.2412, Train: 1.1323, Val: 1.1330\n",
      "Epoch: 117, Loss: 1.2400, Train: 1.1309, Val: 1.1315\n",
      "Epoch: 118, Loss: 1.2444, Train: 1.1360, Val: 1.1368\n",
      "Epoch: 119, Loss: 1.2434, Train: 1.1414, Val: 1.1423\n",
      "Epoch: 120, Loss: 1.2446, Train: 1.1375, Val: 1.1384\n",
      "Epoch: 121, Loss: 1.2454, Train: 1.1255, Val: 1.1262\n",
      "Epoch: 122, Loss: 1.2430, Train: 1.1176, Val: 1.1181\n",
      "Epoch: 123, Loss: 1.2427, Train: 1.1159, Val: 1.1163\n",
      "Epoch: 124, Loss: 1.2441, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 125, Loss: 1.2425, Train: 1.1232, Val: 1.1239\n",
      "Epoch: 126, Loss: 1.2391, Train: 1.1338, Val: 1.1347\n",
      "Epoch: 127, Loss: 1.2403, Train: 1.1352, Val: 1.1360\n",
      "Epoch: 128, Loss: 1.2449, Train: 1.1228, Val: 1.1234\n",
      "Epoch: 129, Loss: 1.2424, Train: 1.1154, Val: 1.1159\n",
      "Epoch: 130, Loss: 1.2442, Train: 1.1145, Val: 1.1149\n",
      "Epoch: 131, Loss: 1.2424, Train: 1.1143, Val: 1.1148\n",
      "Epoch: 132, Loss: 1.2345, Train: 1.1173, Val: 1.1181\n",
      "Epoch: 133, Loss: 1.2474, Train: 1.1140, Val: 1.1146\n",
      "Epoch: 134, Loss: 1.2409, Train: 1.1207, Val: 1.1213\n",
      "Epoch: 135, Loss: 1.2389, Train: 1.1244, Val: 1.1249\n",
      "Epoch: 136, Loss: 1.2413, Train: 1.1152, Val: 1.1159\n",
      "Epoch: 137, Loss: 1.2571, Train: 1.1324, Val: 1.1330\n",
      "Epoch: 138, Loss: 1.2367, Train: 1.1457, Val: 1.1463\n",
      "Epoch: 139, Loss: 1.2468, Train: 1.1200, Val: 1.1209\n",
      "Epoch: 140, Loss: 1.2359, Train: 1.1310, Val: 1.1326\n",
      "Epoch: 141, Loss: 1.2393, Train: 1.1413, Val: 1.1430\n",
      "Epoch: 142, Loss: 1.2391, Train: 1.1217, Val: 1.1232\n",
      "Epoch: 143, Loss: 1.2345, Train: 1.1188, Val: 1.1202\n",
      "Epoch: 144, Loss: 1.2362, Train: 1.1357, Val: 1.1373\n",
      "Epoch: 145, Loss: 1.2278, Train: 1.1576, Val: 1.1594\n",
      "Epoch: 146, Loss: 1.2323, Train: 1.1358, Val: 1.1373\n",
      "Epoch: 147, Loss: 1.2254, Train: 1.1213, Val: 1.1227\n",
      "Epoch: 148, Loss: 1.2263, Train: 1.1207, Val: 1.1222\n",
      "Epoch: 149, Loss: 1.2222, Train: 1.1291, Val: 1.1307\n",
      "Epoch: 150, Loss: 1.2250, Train: 1.1231, Val: 1.1247\n",
      "Epoch: 151, Loss: 1.2208, Train: 1.1116, Val: 1.1130\n",
      "Epoch: 152, Loss: 1.2205, Train: 1.1153, Val: 1.1167\n",
      "Epoch: 153, Loss: 1.2192, Train: 1.1408, Val: 1.1424\n",
      "Epoch: 154, Loss: 1.2177, Train: 1.1492, Val: 1.1508\n",
      "Epoch: 155, Loss: 1.2172, Train: 1.1350, Val: 1.1364\n",
      "Epoch: 156, Loss: 1.2144, Train: 1.1244, Val: 1.1258\n",
      "Epoch: 157, Loss: 1.2152, Train: 1.1266, Val: 1.1280\n",
      "Epoch: 158, Loss: 1.2135, Train: 1.1363, Val: 1.1377\n",
      "Epoch: 159, Loss: 1.2121, Train: 1.1404, Val: 1.1418\n",
      "Epoch: 160, Loss: 1.2117, Train: 1.1348, Val: 1.1362\n",
      "Epoch: 161, Loss: 1.2101, Train: 1.1294, Val: 1.1307\n",
      "Epoch: 162, Loss: 1.2098, Train: 1.1309, Val: 1.1322\n",
      "Epoch: 163, Loss: 1.2089, Train: 1.1347, Val: 1.1360\n",
      "Epoch: 164, Loss: 1.2074, Train: 1.1350, Val: 1.1362\n",
      "Epoch: 165, Loss: 1.2071, Train: 1.1300, Val: 1.1311\n",
      "Epoch: 166, Loss: 1.2058, Train: 1.1247, Val: 1.1257\n",
      "Epoch: 167, Loss: 1.2053, Train: 1.1244, Val: 1.1254\n",
      "Epoch: 168, Loss: 1.2046, Train: 1.1287, Val: 1.1298\n",
      "Epoch: 169, Loss: 1.2032, Train: 1.1315, Val: 1.1327\n",
      "Epoch: 170, Loss: 1.2028, Train: 1.1265, Val: 1.1277\n",
      "Epoch: 171, Loss: 1.2021, Train: 1.1145, Val: 1.1156\n",
      "Epoch: 172, Loss: 1.2012, Train: 1.1081, Val: 1.1091\n",
      "Epoch: 173, Loss: 1.2007, Train: 1.1080, Val: 1.1090\n",
      "Epoch: 174, Loss: 1.1997, Train: 1.1106, Val: 1.1116\n",
      "Epoch: 175, Loss: 1.1989, Train: 1.1104, Val: 1.1114\n",
      "Epoch: 176, Loss: 1.1985, Train: 1.1046, Val: 1.1055\n",
      "Epoch: 177, Loss: 1.1975, Train: 1.1013, Val: 1.1022\n",
      "Epoch: 178, Loss: 1.1969, Train: 1.1020, Val: 1.1029\n",
      "Epoch: 179, Loss: 1.1962, Train: 1.1032, Val: 1.1041\n",
      "Epoch: 180, Loss: 1.1952, Train: 1.1042, Val: 1.1051\n",
      "Epoch: 181, Loss: 1.1947, Train: 1.1033, Val: 1.1042\n",
      "Epoch: 182, Loss: 1.1938, Train: 1.1011, Val: 1.1021\n",
      "Epoch: 183, Loss: 1.1930, Train: 1.0988, Val: 1.0997\n",
      "Epoch: 184, Loss: 1.1924, Train: 1.0995, Val: 1.1004\n",
      "Epoch: 185, Loss: 1.1916, Train: 1.1010, Val: 1.1020\n",
      "Epoch: 186, Loss: 1.1910, Train: 1.0985, Val: 1.0995\n",
      "Epoch: 187, Loss: 1.1905, Train: 1.0996, Val: 1.1006\n",
      "Epoch: 188, Loss: 1.1897, Train: 1.0988, Val: 1.0998\n",
      "Epoch: 189, Loss: 1.1892, Train: 1.0991, Val: 1.1001\n",
      "Epoch: 190, Loss: 1.1886, Train: 1.1004, Val: 1.1014\n",
      "Epoch: 191, Loss: 1.1880, Train: 1.0963, Val: 1.0974\n",
      "Epoch: 192, Loss: 1.1878, Train: 1.1021, Val: 1.1031\n",
      "Epoch: 193, Loss: 1.1885, Train: 1.0970, Val: 1.0981\n",
      "Epoch: 194, Loss: 1.1958, Train: 1.1065, Val: 1.1074\n",
      "Epoch: 195, Loss: 1.1952, Train: 1.1213, Val: 1.1221\n",
      "Epoch: 196, Loss: 1.2031, Train: 1.1954, Val: 1.1962\n",
      "Epoch: 197, Loss: 1.2043, Train: 1.2103, Val: 1.2111\n",
      "Epoch: 198, Loss: 1.2001, Train: 1.1842, Val: 1.1855\n",
      "Epoch: 199, Loss: 1.1945, Train: 1.1666, Val: 1.1686\n",
      "Epoch: 200, Loss: 1.1908, Train: 1.1228, Val: 1.1249\n",
      "Epoch: 201, Loss: 1.2070, Train: 1.2614, Val: 1.2616\n",
      "Epoch: 202, Loss: 1.2121, Train: 1.2803, Val: 1.2800\n",
      "Epoch: 203, Loss: 1.2045, Train: 1.2996, Val: 1.3003\n",
      "Epoch: 204, Loss: 1.2022, Train: 1.2896, Val: 1.2910\n",
      "Epoch: 205, Loss: 1.2070, Train: 1.2197, Val: 1.2212\n",
      "Epoch: 206, Loss: 1.2173, Train: 1.2249, Val: 1.2262\n",
      "Epoch: 207, Loss: 1.1962, Train: 1.2388, Val: 1.2397\n",
      "Epoch: 208, Loss: 1.2233, Train: 1.1661, Val: 1.1674\n",
      "Epoch: 209, Loss: 1.2054, Train: 1.1492, Val: 1.1512\n",
      "Epoch: 210, Loss: 1.2067, Train: 1.1351, Val: 1.1371\n",
      "Epoch: 211, Loss: 1.1994, Train: 1.1326, Val: 1.1347\n",
      "Epoch: 212, Loss: 1.1960, Train: 1.1357, Val: 1.1378\n",
      "Epoch: 213, Loss: 1.2958, Train: 1.1694, Val: 1.1707\n",
      "Epoch: 214, Loss: 1.3340, Train: 1.1218, Val: 1.1228\n",
      "Epoch: 215, Loss: 1.2879, Train: 1.2677, Val: 1.2696\n",
      "Epoch: 216, Loss: 1.3156, Train: 1.2723, Val: 1.2742\n",
      "Epoch: 217, Loss: 1.3354, Train: 1.1281, Val: 1.1290\n",
      "Epoch: 218, Loss: 1.2425, Train: 1.1937, Val: 1.1939\n",
      "Epoch: 219, Loss: 1.2859, Train: 1.2263, Val: 1.2269\n",
      "Epoch: 220, Loss: 1.2734, Train: 1.1498, Val: 1.1509\n",
      "Epoch: 221, Loss: 1.2413, Train: 1.1341, Val: 1.1353\n",
      "Epoch: 222, Loss: 1.2842, Train: 1.1251, Val: 1.1260\n",
      "Epoch: 223, Loss: 1.2523, Train: 1.1381, Val: 1.1384\n",
      "Epoch: 224, Loss: 1.2293, Train: 1.1701, Val: 1.1697\n",
      "Epoch: 225, Loss: 1.2638, Train: 1.1430, Val: 1.1425\n",
      "Epoch: 226, Loss: 1.2373, Train: 1.1209, Val: 1.1208\n",
      "Epoch: 227, Loss: 1.2252, Train: 1.1272, Val: 1.1273\n",
      "Epoch: 228, Loss: 1.2389, Train: 1.1315, Val: 1.1316\n",
      "Epoch: 229, Loss: 1.2197, Train: 1.1441, Val: 1.1441\n",
      "Epoch: 230, Loss: 1.2190, Train: 1.1409, Val: 1.1410\n",
      "Epoch: 231, Loss: 1.2161, Train: 1.1132, Val: 1.1135\n",
      "Epoch: 232, Loss: 1.2030, Train: 1.1042, Val: 1.1048\n",
      "Epoch: 233, Loss: 1.2098, Train: 1.1032, Val: 1.1040\n",
      "Epoch: 234, Loss: 1.2091, Train: 1.1089, Val: 1.1098\n",
      "Epoch: 235, Loss: 1.1985, Train: 1.1350, Val: 1.1359\n",
      "Epoch: 236, Loss: 1.1961, Train: 1.1535, Val: 1.1546\n",
      "Epoch: 237, Loss: 1.1972, Train: 1.1436, Val: 1.1450\n",
      "Epoch: 238, Loss: 1.1931, Train: 1.1311, Val: 1.1328\n",
      "Epoch: 239, Loss: 1.1944, Train: 1.1300, Val: 1.1317\n",
      "Epoch: 240, Loss: 1.1934, Train: 1.1395, Val: 1.1409\n",
      "Epoch: 241, Loss: 1.1882, Train: 1.1496, Val: 1.1508\n",
      "Epoch: 242, Loss: 1.1893, Train: 1.1436, Val: 1.1447\n",
      "Epoch: 243, Loss: 1.1895, Train: 1.1265, Val: 1.1278\n",
      "Epoch: 244, Loss: 1.1875, Train: 1.1168, Val: 1.1183\n",
      "Epoch: 245, Loss: 1.1866, Train: 1.1116, Val: 1.1132\n",
      "Epoch: 246, Loss: 1.1845, Train: 1.1098, Val: 1.1115\n",
      "Epoch: 247, Loss: 1.1828, Train: 1.1114, Val: 1.1131\n",
      "Epoch: 248, Loss: 1.1829, Train: 1.1076, Val: 1.1093\n",
      "Epoch: 249, Loss: 1.1824, Train: 1.1003, Val: 1.1019\n",
      "Epoch: 250, Loss: 1.1806, Train: 1.0959, Val: 1.0974\n",
      "Epoch: 251, Loss: 1.1806, Train: 1.0948, Val: 1.0962\n",
      "Epoch: 252, Loss: 1.1800, Train: 1.0950, Val: 1.0963\n",
      "Epoch: 253, Loss: 1.1792, Train: 1.0952, Val: 1.0965\n",
      "Epoch: 254, Loss: 1.1805, Train: 1.0944, Val: 1.0958\n",
      "Epoch: 255, Loss: 1.1810, Train: 1.0927, Val: 1.0943\n",
      "Epoch: 256, Loss: 1.1781, Train: 1.0926, Val: 1.0943\n",
      "Epoch: 257, Loss: 1.1828, Train: 1.1018, Val: 1.1036\n",
      "Epoch: 258, Loss: 1.1849, Train: 1.1094, Val: 1.1113\n",
      "Epoch: 259, Loss: 1.1902, Train: 1.1075, Val: 1.1092\n",
      "Epoch: 260, Loss: 1.1885, Train: 1.1003, Val: 1.1019\n",
      "Epoch: 261, Loss: 1.1824, Train: 1.0967, Val: 1.0981\n",
      "Epoch: 262, Loss: 1.1887, Train: 1.0880, Val: 1.0900\n",
      "Epoch: 263, Loss: 1.1757, Train: 1.0904, Val: 1.0927\n",
      "Epoch: 264, Loss: 1.1754, Train: 1.0962, Val: 1.0987\n",
      "Epoch: 265, Loss: 1.1766, Train: 1.0998, Val: 1.1024\n",
      "Epoch: 266, Loss: 1.1778, Train: 1.1056, Val: 1.1082\n",
      "Epoch: 267, Loss: 1.1768, Train: 1.1010, Val: 1.1036\n",
      "Epoch: 268, Loss: 1.1756, Train: 1.0936, Val: 1.0963\n",
      "Epoch: 269, Loss: 1.1751, Train: 1.0915, Val: 1.0939\n",
      "Epoch: 270, Loss: 1.1744, Train: 1.0903, Val: 1.0926\n",
      "Epoch: 271, Loss: 1.1734, Train: 1.0872, Val: 1.0892\n",
      "Epoch: 272, Loss: 1.1726, Train: 1.0851, Val: 1.0868\n",
      "Epoch: 273, Loss: 1.1722, Train: 1.0843, Val: 1.0860\n",
      "Epoch: 274, Loss: 1.1728, Train: 1.0845, Val: 1.0864\n",
      "Epoch: 275, Loss: 1.1716, Train: 1.0879, Val: 1.0901\n",
      "Epoch: 276, Loss: 1.1708, Train: 1.0907, Val: 1.0932\n",
      "Epoch: 277, Loss: 1.1697, Train: 1.0928, Val: 1.0956\n",
      "Epoch: 278, Loss: 1.1701, Train: 1.0976, Val: 1.1004\n",
      "Epoch: 279, Loss: 1.1696, Train: 1.0973, Val: 1.1001\n",
      "Epoch: 280, Loss: 1.1693, Train: 1.0916, Val: 1.0943\n",
      "Epoch: 281, Loss: 1.1684, Train: 1.0859, Val: 1.0885\n",
      "Epoch: 282, Loss: 1.1683, Train: 1.0841, Val: 1.0866\n",
      "Epoch: 283, Loss: 1.1676, Train: 1.0823, Val: 1.0847\n",
      "Epoch: 284, Loss: 1.1673, Train: 1.0818, Val: 1.0840\n",
      "Epoch: 285, Loss: 1.1670, Train: 1.0830, Val: 1.0852\n",
      "Epoch: 286, Loss: 1.1665, Train: 1.0845, Val: 1.0869\n",
      "Epoch: 287, Loss: 1.1662, Train: 1.0843, Val: 1.0868\n",
      "Epoch: 288, Loss: 1.1657, Train: 1.0830, Val: 1.0858\n",
      "Epoch: 289, Loss: 1.1655, Train: 1.0830, Val: 1.0858\n",
      "Epoch: 290, Loss: 1.1652, Train: 1.0839, Val: 1.0867\n",
      "Epoch: 291, Loss: 1.1648, Train: 1.0829, Val: 1.0856\n",
      "Epoch: 292, Loss: 1.1643, Train: 1.0813, Val: 1.0839\n",
      "Epoch: 293, Loss: 1.1640, Train: 1.0813, Val: 1.0839\n",
      "Epoch: 294, Loss: 1.1636, Train: 1.0807, Val: 1.0833\n",
      "Epoch: 295, Loss: 1.1633, Train: 1.0804, Val: 1.0830\n",
      "Epoch: 296, Loss: 1.1630, Train: 1.0816, Val: 1.0843\n",
      "Epoch: 297, Loss: 1.1626, Train: 1.0813, Val: 1.0840\n",
      "Epoch: 298, Loss: 1.1622, Train: 1.0798, Val: 1.0825\n",
      "Epoch: 299, Loss: 1.1620, Train: 1.0793, Val: 1.0821\n",
      "Epoch: 300, Loss: 1.1618, Train: 1.0785, Val: 1.0811\n",
      "Epoch: 301, Loss: 1.1612, Train: 1.0789, Val: 1.0814\n",
      "Epoch: 302, Loss: 1.1610, Train: 1.0790, Val: 1.0815\n",
      "Epoch: 303, Loss: 1.1609, Train: 1.0802, Val: 1.0826\n",
      "Epoch: 304, Loss: 1.1604, Train: 1.0799, Val: 1.0823\n",
      "Epoch: 305, Loss: 1.1600, Train: 1.0787, Val: 1.0813\n",
      "Epoch: 306, Loss: 1.1598, Train: 1.0791, Val: 1.0818\n",
      "Epoch: 307, Loss: 1.1596, Train: 1.0783, Val: 1.0810\n",
      "Epoch: 308, Loss: 1.1594, Train: 1.0786, Val: 1.0813\n",
      "Epoch: 309, Loss: 1.1590, Train: 1.0780, Val: 1.0808\n",
      "Epoch: 310, Loss: 1.1587, Train: 1.0783, Val: 1.0810\n",
      "Epoch: 311, Loss: 1.1584, Train: 1.0781, Val: 1.0809\n",
      "Epoch: 312, Loss: 1.1583, Train: 1.0784, Val: 1.0812\n",
      "Epoch: 313, Loss: 1.1582, Train: 1.0785, Val: 1.0814\n",
      "Epoch: 314, Loss: 1.1591, Train: 1.0787, Val: 1.0815\n",
      "Epoch: 315, Loss: 1.1577, Train: 1.0779, Val: 1.0807\n",
      "Epoch: 316, Loss: 1.1573, Train: 1.0793, Val: 1.0820\n",
      "Epoch: 317, Loss: 1.1570, Train: 1.0779, Val: 1.0807\n",
      "Epoch: 318, Loss: 1.1569, Train: 1.0794, Val: 1.0821\n",
      "Epoch: 319, Loss: 1.1569, Train: 1.0769, Val: 1.0799\n",
      "Epoch: 320, Loss: 1.1586, Train: 1.0778, Val: 1.0806\n",
      "Epoch: 321, Loss: 1.1562, Train: 1.0764, Val: 1.0794\n",
      "Epoch: 322, Loss: 1.1556, Train: 1.0760, Val: 1.0791\n",
      "Epoch: 323, Loss: 1.1555, Train: 1.0762, Val: 1.0793\n",
      "Epoch: 324, Loss: 1.1557, Train: 1.0768, Val: 1.0800\n",
      "Epoch: 325, Loss: 1.1579, Train: 1.0761, Val: 1.0791\n",
      "Epoch: 326, Loss: 1.1552, Train: 1.0755, Val: 1.0786\n",
      "Epoch: 327, Loss: 1.1547, Train: 1.0752, Val: 1.0785\n",
      "Epoch: 328, Loss: 1.1556, Train: 1.0760, Val: 1.0792\n",
      "Epoch: 329, Loss: 1.1561, Train: 1.0824, Val: 1.0859\n",
      "Epoch: 330, Loss: 1.1631, Train: 1.0777, Val: 1.0809\n",
      "Epoch: 331, Loss: 1.1597, Train: 1.0875, Val: 1.0899\n",
      "Epoch: 332, Loss: 1.1673, Train: 1.1062, Val: 1.1096\n",
      "Epoch: 333, Loss: 1.1784, Train: 1.1363, Val: 1.1393\n",
      "Epoch: 334, Loss: 1.1897, Train: 1.1310, Val: 1.1341\n",
      "Epoch: 335, Loss: 1.1914, Train: 1.1164, Val: 1.1197\n",
      "Epoch: 336, Loss: 1.1905, Train: 1.1093, Val: 1.1122\n",
      "Epoch: 337, Loss: 1.1877, Train: 1.1104, Val: 1.1121\n",
      "Epoch: 338, Loss: 1.1926, Train: 1.1264, Val: 1.1275\n",
      "Epoch: 339, Loss: 1.1861, Train: 1.1043, Val: 1.1058\n",
      "Epoch: 340, Loss: 1.1839, Train: 1.1118, Val: 1.1146\n",
      "Epoch: 341, Loss: 1.1892, Train: 1.1311, Val: 1.1342\n",
      "Epoch: 342, Loss: 1.1905, Train: 1.1203, Val: 1.1235\n",
      "Epoch: 343, Loss: 1.1801, Train: 1.0961, Val: 1.0991\n",
      "Epoch: 344, Loss: 1.1755, Train: 1.0868, Val: 1.0894\n",
      "Epoch: 345, Loss: 1.1761, Train: 1.1063, Val: 1.1083\n",
      "Epoch: 346, Loss: 1.1738, Train: 1.1260, Val: 1.1282\n",
      "Epoch: 347, Loss: 1.1686, Train: 1.1344, Val: 1.1370\n",
      "Epoch: 348, Loss: 1.1683, Train: 1.1341, Val: 1.1372\n",
      "Epoch: 349, Loss: 1.1640, Train: 1.1379, Val: 1.1415\n",
      "Epoch: 350, Loss: 1.1635, Train: 1.1524, Val: 1.1564\n",
      "Epoch: 351, Loss: 1.1664, Train: 1.1664, Val: 1.1707\n",
      "Epoch: 352, Loss: 1.1642, Train: 1.1722, Val: 1.1765\n",
      "Epoch: 353, Loss: 1.1620, Train: 1.1630, Val: 1.1672\n",
      "Epoch: 354, Loss: 1.1625, Train: 1.1359, Val: 1.1398\n",
      "Epoch: 355, Loss: 1.1597, Train: 1.1096, Val: 1.1131\n",
      "Epoch: 356, Loss: 1.1588, Train: 1.0961, Val: 1.0992\n",
      "Epoch: 357, Loss: 1.1602, Train: 1.0901, Val: 1.0929\n",
      "Epoch: 358, Loss: 1.1581, Train: 1.0859, Val: 1.0886\n",
      "Epoch: 359, Loss: 1.1560, Train: 1.0829, Val: 1.0856\n",
      "Epoch: 360, Loss: 1.1569, Train: 1.0806, Val: 1.0834\n",
      "Epoch: 361, Loss: 1.1572, Train: 1.0785, Val: 1.0816\n",
      "Epoch: 362, Loss: 1.1556, Train: 1.0789, Val: 1.0821\n",
      "Epoch: 363, Loss: 1.1545, Train: 1.0812, Val: 1.0846\n",
      "Epoch: 364, Loss: 1.1549, Train: 1.0815, Val: 1.0850\n",
      "Epoch: 365, Loss: 1.1550, Train: 1.0797, Val: 1.0832\n",
      "Epoch: 366, Loss: 1.1545, Train: 1.0779, Val: 1.0814\n",
      "Epoch: 367, Loss: 1.1543, Train: 1.0779, Val: 1.0812\n",
      "Epoch: 368, Loss: 1.1538, Train: 1.0814, Val: 1.0844\n",
      "Epoch: 369, Loss: 1.1529, Train: 1.0876, Val: 1.0903\n",
      "Epoch: 370, Loss: 1.1525, Train: 1.0899, Val: 1.0925\n",
      "Epoch: 371, Loss: 1.1526, Train: 1.0874, Val: 1.0901\n",
      "Epoch: 372, Loss: 1.1525, Train: 1.0842, Val: 1.0871\n",
      "Epoch: 373, Loss: 1.1521, Train: 1.0828, Val: 1.0858\n",
      "Epoch: 374, Loss: 1.1516, Train: 1.0846, Val: 1.0875\n",
      "Epoch: 375, Loss: 1.1513, Train: 1.0872, Val: 1.0899\n",
      "Epoch: 376, Loss: 1.1513, Train: 1.0841, Val: 1.0871\n",
      "Epoch: 377, Loss: 1.1510, Train: 1.0787, Val: 1.0820\n",
      "Epoch: 378, Loss: 1.1507, Train: 1.0759, Val: 1.0795\n",
      "Epoch: 379, Loss: 1.1505, Train: 1.0751, Val: 1.0787\n",
      "Epoch: 380, Loss: 1.1502, Train: 1.0748, Val: 1.0784\n",
      "Epoch: 381, Loss: 1.1499, Train: 1.0743, Val: 1.0778\n",
      "Epoch: 382, Loss: 1.1500, Train: 1.0733, Val: 1.0769\n",
      "Epoch: 383, Loss: 1.1497, Train: 1.0733, Val: 1.0770\n",
      "Epoch: 384, Loss: 1.1496, Train: 1.0736, Val: 1.0774\n",
      "Epoch: 385, Loss: 1.1494, Train: 1.0730, Val: 1.0768\n",
      "Epoch: 386, Loss: 1.1492, Train: 1.0727, Val: 1.0765\n",
      "Epoch: 387, Loss: 1.1490, Train: 1.0729, Val: 1.0766\n",
      "Epoch: 388, Loss: 1.1489, Train: 1.0727, Val: 1.0764\n",
      "Epoch: 389, Loss: 1.1486, Train: 1.0728, Val: 1.0765\n",
      "Epoch: 390, Loss: 1.1485, Train: 1.0737, Val: 1.0772\n",
      "Epoch: 391, Loss: 1.1483, Train: 1.0757, Val: 1.0790\n",
      "Epoch: 392, Loss: 1.1481, Train: 1.0767, Val: 1.0799\n",
      "Epoch: 393, Loss: 1.1480, Train: 1.0754, Val: 1.0787\n",
      "Epoch: 394, Loss: 1.1478, Train: 1.0738, Val: 1.0773\n",
      "Epoch: 395, Loss: 1.1476, Train: 1.0735, Val: 1.0770\n",
      "Epoch: 396, Loss: 1.1475, Train: 1.0738, Val: 1.0773\n",
      "Epoch: 397, Loss: 1.1473, Train: 1.0734, Val: 1.0770\n",
      "Epoch: 398, Loss: 1.1472, Train: 1.0724, Val: 1.0761\n",
      "Epoch: 399, Loss: 1.1470, Train: 1.0720, Val: 1.0758\n",
      "Epoch: 400, Loss: 1.1469, Train: 1.0724, Val: 1.0761\n",
      "Epoch: 401, Loss: 1.1467, Train: 1.0727, Val: 1.0763\n",
      "Epoch: 402, Loss: 1.1466, Train: 1.0727, Val: 1.0763\n",
      "Epoch: 403, Loss: 1.1465, Train: 1.0721, Val: 1.0757\n",
      "Epoch: 404, Loss: 1.1463, Train: 1.0714, Val: 1.0752\n",
      "Epoch: 405, Loss: 1.1462, Train: 1.0716, Val: 1.0753\n",
      "Epoch: 406, Loss: 1.1461, Train: 1.0720, Val: 1.0757\n",
      "Epoch: 407, Loss: 1.1459, Train: 1.0721, Val: 1.0757\n",
      "Epoch: 408, Loss: 1.1458, Train: 1.0718, Val: 1.0755\n",
      "Epoch: 409, Loss: 1.1457, Train: 1.0716, Val: 1.0752\n",
      "Epoch: 410, Loss: 1.1456, Train: 1.0715, Val: 1.0752\n",
      "Epoch: 411, Loss: 1.1454, Train: 1.0715, Val: 1.0752\n",
      "Epoch: 412, Loss: 1.1453, Train: 1.0713, Val: 1.0751\n",
      "Epoch: 413, Loss: 1.1452, Train: 1.0711, Val: 1.0750\n",
      "Epoch: 414, Loss: 1.1451, Train: 1.0712, Val: 1.0750\n",
      "Epoch: 415, Loss: 1.1450, Train: 1.0713, Val: 1.0751\n",
      "Epoch: 416, Loss: 1.1448, Train: 1.0712, Val: 1.0751\n",
      "Epoch: 417, Loss: 1.1447, Train: 1.0711, Val: 1.0750\n",
      "Epoch: 418, Loss: 1.1446, Train: 1.0710, Val: 1.0750\n",
      "Epoch: 419, Loss: 1.1445, Train: 1.0711, Val: 1.0750\n",
      "Epoch: 420, Loss: 1.1444, Train: 1.0713, Val: 1.0752\n",
      "Epoch: 421, Loss: 1.1443, Train: 1.0713, Val: 1.0751\n",
      "Epoch: 422, Loss: 1.1442, Train: 1.0711, Val: 1.0750\n",
      "Epoch: 423, Loss: 1.1441, Train: 1.0711, Val: 1.0749\n",
      "Epoch: 424, Loss: 1.1440, Train: 1.0713, Val: 1.0752\n",
      "Epoch: 425, Loss: 1.1438, Train: 1.0714, Val: 1.0753\n",
      "Epoch: 426, Loss: 1.1437, Train: 1.0712, Val: 1.0752\n",
      "Epoch: 427, Loss: 1.1436, Train: 1.0713, Val: 1.0753\n",
      "Epoch: 428, Loss: 1.1435, Train: 1.0714, Val: 1.0754\n",
      "Epoch: 429, Loss: 1.1434, Train: 1.0715, Val: 1.0755\n",
      "Epoch: 430, Loss: 1.1433, Train: 1.0712, Val: 1.0753\n",
      "Epoch: 431, Loss: 1.1432, Train: 1.0708, Val: 1.0749\n",
      "Epoch: 432, Loss: 1.1431, Train: 1.0706, Val: 1.0747\n",
      "Epoch: 433, Loss: 1.1430, Train: 1.0707, Val: 1.0748\n",
      "Epoch: 434, Loss: 1.1429, Train: 1.0702, Val: 1.0744\n",
      "Epoch: 435, Loss: 1.1428, Train: 1.0697, Val: 1.0738\n",
      "Epoch: 436, Loss: 1.1427, Train: 1.0694, Val: 1.0736\n",
      "Epoch: 437, Loss: 1.1426, Train: 1.0693, Val: 1.0735\n",
      "Epoch: 438, Loss: 1.1425, Train: 1.0693, Val: 1.0735\n",
      "Epoch: 439, Loss: 1.1424, Train: 1.0692, Val: 1.0736\n",
      "Epoch: 440, Loss: 1.1423, Train: 1.0691, Val: 1.0735\n",
      "Epoch: 441, Loss: 1.1422, Train: 1.0691, Val: 1.0735\n",
      "Epoch: 442, Loss: 1.1422, Train: 1.0691, Val: 1.0735\n",
      "Epoch: 443, Loss: 1.1421, Train: 1.0691, Val: 1.0735\n",
      "Epoch: 444, Loss: 1.1420, Train: 1.0692, Val: 1.0736\n",
      "Epoch: 445, Loss: 1.1419, Train: 1.0694, Val: 1.0738\n",
      "Epoch: 446, Loss: 1.1418, Train: 1.0695, Val: 1.0740\n",
      "Epoch: 447, Loss: 1.1417, Train: 1.0695, Val: 1.0740\n",
      "Epoch: 448, Loss: 1.1416, Train: 1.0695, Val: 1.0740\n",
      "Epoch: 449, Loss: 1.1415, Train: 1.0694, Val: 1.0739\n",
      "Epoch: 450, Loss: 1.1414, Train: 1.0694, Val: 1.0740\n",
      "Epoch: 451, Loss: 1.1413, Train: 1.0695, Val: 1.0741\n",
      "Epoch: 452, Loss: 1.1412, Train: 1.0696, Val: 1.0741\n",
      "Epoch: 453, Loss: 1.1412, Train: 1.0696, Val: 1.0742\n",
      "Epoch: 454, Loss: 1.1411, Train: 1.0692, Val: 1.0738\n",
      "Epoch: 455, Loss: 1.1410, Train: 1.0690, Val: 1.0736\n",
      "Epoch: 456, Loss: 1.1409, Train: 1.0690, Val: 1.0736\n",
      "Epoch: 457, Loss: 1.1408, Train: 1.0689, Val: 1.0735\n",
      "Epoch: 458, Loss: 1.1407, Train: 1.0691, Val: 1.0737\n",
      "Epoch: 459, Loss: 1.1406, Train: 1.0692, Val: 1.0738\n",
      "Epoch: 460, Loss: 1.1406, Train: 1.0693, Val: 1.0739\n",
      "Epoch: 461, Loss: 1.1405, Train: 1.0696, Val: 1.0742\n",
      "Epoch: 462, Loss: 1.1404, Train: 1.0696, Val: 1.0742\n",
      "Epoch: 463, Loss: 1.1403, Train: 1.0696, Val: 1.0742\n",
      "Epoch: 464, Loss: 1.1402, Train: 1.0691, Val: 1.0737\n",
      "Epoch: 465, Loss: 1.1401, Train: 1.0686, Val: 1.0732\n",
      "Epoch: 466, Loss: 1.1401, Train: 1.0685, Val: 1.0732\n",
      "Epoch: 467, Loss: 1.1400, Train: 1.0682, Val: 1.0729\n",
      "Epoch: 468, Loss: 1.1399, Train: 1.0682, Val: 1.0729\n",
      "Epoch: 469, Loss: 1.1398, Train: 1.0682, Val: 1.0730\n",
      "Epoch: 470, Loss: 1.1397, Train: 1.0683, Val: 1.0730\n",
      "Epoch: 471, Loss: 1.1397, Train: 1.0681, Val: 1.0728\n",
      "Epoch: 472, Loss: 1.1396, Train: 1.0681, Val: 1.0728\n",
      "Epoch: 473, Loss: 1.1395, Train: 1.0677, Val: 1.0724\n",
      "Epoch: 474, Loss: 1.1394, Train: 1.0679, Val: 1.0726\n",
      "Epoch: 475, Loss: 1.1394, Train: 1.0677, Val: 1.0724\n",
      "Epoch: 476, Loss: 1.1393, Train: 1.0678, Val: 1.0725\n",
      "Epoch: 477, Loss: 1.1392, Train: 1.0676, Val: 1.0723\n",
      "Epoch: 478, Loss: 1.1391, Train: 1.0677, Val: 1.0723\n",
      "Epoch: 479, Loss: 1.1391, Train: 1.0676, Val: 1.0723\n",
      "Epoch: 480, Loss: 1.1390, Train: 1.0675, Val: 1.0722\n",
      "Epoch: 481, Loss: 1.1389, Train: 1.0676, Val: 1.0723\n",
      "Epoch: 482, Loss: 1.1388, Train: 1.0674, Val: 1.0721\n",
      "Epoch: 483, Loss: 1.1388, Train: 1.0678, Val: 1.0725\n",
      "Epoch: 484, Loss: 1.1387, Train: 1.0676, Val: 1.0722\n",
      "Epoch: 485, Loss: 1.1387, Train: 1.0684, Val: 1.0730\n",
      "Epoch: 486, Loss: 1.1386, Train: 1.0683, Val: 1.0728\n",
      "Epoch: 487, Loss: 1.1385, Train: 1.0691, Val: 1.0737\n",
      "Epoch: 488, Loss: 1.1384, Train: 1.0693, Val: 1.0738\n",
      "Epoch: 489, Loss: 1.1383, Train: 1.0694, Val: 1.0740\n",
      "Epoch: 490, Loss: 1.1383, Train: 1.0697, Val: 1.0743\n",
      "Epoch: 491, Loss: 1.1382, Train: 1.0695, Val: 1.0741\n",
      "Epoch: 492, Loss: 1.1381, Train: 1.0689, Val: 1.0735\n",
      "Epoch: 493, Loss: 1.1381, Train: 1.0681, Val: 1.0727\n",
      "Epoch: 494, Loss: 1.1380, Train: 1.0682, Val: 1.0730\n",
      "Epoch: 495, Loss: 1.1379, Train: 1.0674, Val: 1.0721\n",
      "Epoch: 496, Loss: 1.1379, Train: 1.0684, Val: 1.0733\n",
      "Epoch: 497, Loss: 1.1378, Train: 1.0674, Val: 1.0722\n",
      "Epoch: 498, Loss: 1.1378, Train: 1.0691, Val: 1.0741\n",
      "Epoch: 499, Loss: 1.1377, Train: 1.0671, Val: 1.0720\n",
      "Epoch: 500, Loss: 1.1377, Train: 1.0694, Val: 1.0744\n",
      "Test RMSE: 1.0915\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.560501       3.583673\n",
      "std      1727.484387     741.673176       0.335514       1.116938\n",
      "min         0.000000       0.000000       1.966760       1.000000\n",
      "25%      1500.000000     259.000000       3.499775       3.000000\n",
      "50%      3066.000000     693.000000       3.499775       4.000000\n",
      "75%      4472.000000    1292.000000       3.732215       4.000000\n",
      "max      6039.000000    3702.000000       5.000000       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1143.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80909243415533\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------\n",
      "K:  3\n",
      "Kernel:  spd\n",
      "Model Name:  KPGINPlus\n",
      "---------------------------------\n",
      "Epoch: 001, Loss: 14.3530, Train: 3.0335, Val: 3.0354\n",
      "Epoch: 002, Loss: 11.9526, Train: 3.0965, Val: 3.0995\n",
      "Epoch: 003, Loss: 8.6746, Train: 2.9184, Val: 2.9208\n",
      "Epoch: 004, Loss: 4.4931, Train: 2.5404, Val: 2.5415\n",
      "Epoch: 005, Loss: 2.6270, Train: 2.0656, Val: 2.0655\n",
      "Epoch: 006, Loss: 3.3829, Train: 1.9005, Val: 1.9007\n",
      "Epoch: 007, Loss: 2.1259, Train: 1.7551, Val: 1.7552\n",
      "Epoch: 008, Loss: 1.5614, Train: 1.6573, Val: 1.6574\n",
      "Epoch: 009, Loss: 1.5943, Train: 1.5709, Val: 1.5710\n",
      "Epoch: 010, Loss: 1.5735, Train: 1.5140, Val: 1.5138\n",
      "Epoch: 011, Loss: 1.4682, Train: 1.4825, Val: 1.4822\n",
      "Epoch: 012, Loss: 1.3692, Train: 1.4511, Val: 1.4506\n",
      "Epoch: 013, Loss: 1.3269, Train: 1.4132, Val: 1.4128\n",
      "Epoch: 014, Loss: 1.3427, Train: 1.3774, Val: 1.3773\n",
      "Epoch: 015, Loss: 1.3698, Train: 1.3521, Val: 1.3525\n",
      "Epoch: 016, Loss: 1.3655, Train: 1.3433, Val: 1.3441\n",
      "Epoch: 017, Loss: 1.3364, Train: 1.3427, Val: 1.3439\n",
      "Epoch: 018, Loss: 1.3107, Train: 1.3324, Val: 1.3340\n",
      "Epoch: 019, Loss: 1.2958, Train: 1.3107, Val: 1.3124\n",
      "Epoch: 020, Loss: 1.2885, Train: 1.2888, Val: 1.2907\n",
      "Epoch: 021, Loss: 1.2884, Train: 1.2767, Val: 1.2786\n",
      "Epoch: 022, Loss: 1.2890, Train: 1.2743, Val: 1.2762\n",
      "Epoch: 023, Loss: 1.2846, Train: 1.2733, Val: 1.2752\n",
      "Epoch: 024, Loss: 1.2792, Train: 1.2639, Val: 1.2658\n",
      "Epoch: 025, Loss: 1.2721, Train: 1.2448, Val: 1.2467\n",
      "Epoch: 026, Loss: 1.2637, Train: 1.2256, Val: 1.2274\n",
      "Epoch: 027, Loss: 1.2603, Train: 1.2165, Val: 1.2182\n",
      "Epoch: 028, Loss: 1.2613, Train: 1.2171, Val: 1.2187\n",
      "Epoch: 029, Loss: 1.2627, Train: 1.2234, Val: 1.2249\n",
      "Epoch: 030, Loss: 1.2625, Train: 1.2263, Val: 1.2278\n",
      "Epoch: 031, Loss: 1.2610, Train: 1.2173, Val: 1.2188\n",
      "Epoch: 032, Loss: 1.2567, Train: 1.2020, Val: 1.2035\n",
      "Epoch: 033, Loss: 1.2528, Train: 1.1920, Val: 1.1935\n",
      "Epoch: 034, Loss: 1.2511, Train: 1.1911, Val: 1.1927\n",
      "Epoch: 035, Loss: 1.2504, Train: 1.1929, Val: 1.1945\n",
      "Epoch: 036, Loss: 1.2516, Train: 1.1866, Val: 1.1882\n",
      "Epoch: 037, Loss: 1.2531, Train: 1.1709, Val: 1.1724\n",
      "Epoch: 038, Loss: 1.2533, Train: 1.1546, Val: 1.1559\n",
      "Epoch: 039, Loss: 1.2532, Train: 1.1447, Val: 1.1460\n",
      "Epoch: 040, Loss: 1.2521, Train: 1.1388, Val: 1.1400\n",
      "Epoch: 041, Loss: 1.2503, Train: 1.1334, Val: 1.1345\n",
      "Epoch: 042, Loss: 1.2490, Train: 1.1273, Val: 1.1282\n",
      "Epoch: 043, Loss: 1.2481, Train: 1.1227, Val: 1.1234\n",
      "Epoch: 044, Loss: 1.2482, Train: 1.1205, Val: 1.1212\n",
      "Epoch: 045, Loss: 1.2488, Train: 1.1197, Val: 1.1202\n",
      "Epoch: 046, Loss: 1.2494, Train: 1.1192, Val: 1.1198\n",
      "Epoch: 047, Loss: 1.2498, Train: 1.1188, Val: 1.1192\n",
      "Epoch: 048, Loss: 1.2495, Train: 1.1186, Val: 1.1190\n",
      "Epoch: 049, Loss: 1.2488, Train: 1.1187, Val: 1.1191\n",
      "Epoch: 050, Loss: 1.2481, Train: 1.1187, Val: 1.1191\n",
      "Epoch: 051, Loss: 1.2476, Train: 1.1188, Val: 1.1192\n",
      "Epoch: 052, Loss: 1.2476, Train: 1.1189, Val: 1.1194\n",
      "Epoch: 053, Loss: 1.2477, Train: 1.1190, Val: 1.1195\n",
      "Epoch: 054, Loss: 1.2480, Train: 1.1188, Val: 1.1193\n",
      "Epoch: 055, Loss: 1.2481, Train: 1.1185, Val: 1.1189\n",
      "Epoch: 056, Loss: 1.2481, Train: 1.1181, Val: 1.1185\n",
      "Epoch: 057, Loss: 1.2478, Train: 1.1176, Val: 1.1180\n",
      "Epoch: 058, Loss: 1.2475, Train: 1.1173, Val: 1.1176\n",
      "Epoch: 059, Loss: 1.2473, Train: 1.1170, Val: 1.1174\n",
      "Epoch: 060, Loss: 1.2471, Train: 1.1169, Val: 1.1172\n",
      "Epoch: 061, Loss: 1.2471, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 062, Loss: 1.2472, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 063, Loss: 1.2472, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 064, Loss: 1.2471, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 065, Loss: 1.2470, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 066, Loss: 1.2469, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 067, Loss: 1.2468, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 068, Loss: 1.2468, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 069, Loss: 1.2468, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 070, Loss: 1.2469, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 071, Loss: 1.2478, Train: 1.1171, Val: 1.1175\n",
      "Epoch: 072, Loss: 1.2468, Train: 1.1176, Val: 1.1180\n",
      "Epoch: 073, Loss: 1.2467, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 074, Loss: 1.2467, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 075, Loss: 1.2466, Train: 1.1180, Val: 1.1184\n",
      "Epoch: 076, Loss: 1.2466, Train: 1.1181, Val: 1.1186\n",
      "Epoch: 077, Loss: 1.2467, Train: 1.1171, Val: 1.1174\n",
      "Epoch: 078, Loss: 1.2465, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 079, Loss: 1.2466, Train: 1.1171, Val: 1.1175\n",
      "Epoch: 080, Loss: 1.2464, Train: 1.1169, Val: 1.1173\n",
      "Epoch: 081, Loss: 1.2463, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 082, Loss: 1.2463, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 083, Loss: 1.2463, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 084, Loss: 1.2463, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 085, Loss: 1.2462, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 086, Loss: 1.2462, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 087, Loss: 1.2461, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 088, Loss: 1.2462, Train: 1.1170, Val: 1.1175\n",
      "Epoch: 089, Loss: 1.2461, Train: 1.1172, Val: 1.1177\n",
      "Epoch: 090, Loss: 1.2461, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 091, Loss: 1.2460, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 092, Loss: 1.2459, Train: 1.1170, Val: 1.1174\n",
      "Epoch: 093, Loss: 1.2461, Train: 1.1163, Val: 1.1166\n",
      "Epoch: 094, Loss: 1.2463, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 095, Loss: 1.2459, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 096, Loss: 1.2462, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 097, Loss: 1.2461, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 098, Loss: 1.2460, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 099, Loss: 1.2461, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 100, Loss: 1.2459, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 101, Loss: 1.2461, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 102, Loss: 1.2459, Train: 1.1164, Val: 1.1167\n",
      "Epoch: 103, Loss: 1.2460, Train: 1.1169, Val: 1.1172\n",
      "Epoch: 104, Loss: 1.2459, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 105, Loss: 1.2458, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 106, Loss: 1.2459, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 107, Loss: 1.2458, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 108, Loss: 1.2457, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 109, Loss: 1.2458, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 110, Loss: 1.2456, Train: 1.1168, Val: 1.1171\n",
      "Epoch: 111, Loss: 1.2457, Train: 1.1166, Val: 1.1169\n",
      "Epoch: 112, Loss: 1.2456, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 113, Loss: 1.2456, Train: 1.1172, Val: 1.1176\n",
      "Epoch: 114, Loss: 1.2456, Train: 1.1169, Val: 1.1172\n",
      "Epoch: 115, Loss: 1.2454, Train: 1.1167, Val: 1.1171\n",
      "Epoch: 116, Loss: 1.2454, Train: 1.1171, Val: 1.1175\n",
      "Epoch: 117, Loss: 1.2452, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 118, Loss: 1.2464, Train: 1.1219, Val: 1.1221\n",
      "Epoch: 119, Loss: 1.2501, Train: 1.1170, Val: 1.1175\n",
      "Epoch: 120, Loss: 1.2461, Train: 1.1191, Val: 1.1198\n",
      "Epoch: 121, Loss: 1.2490, Train: 1.1177, Val: 1.1181\n",
      "Epoch: 122, Loss: 1.2463, Train: 1.1200, Val: 1.1203\n",
      "Epoch: 123, Loss: 1.2480, Train: 1.1171, Val: 1.1177\n",
      "Epoch: 124, Loss: 1.2464, Train: 1.1175, Val: 1.1180\n",
      "Epoch: 125, Loss: 1.2473, Train: 1.1183, Val: 1.1186\n",
      "Epoch: 126, Loss: 1.2465, Train: 1.1184, Val: 1.1187\n",
      "Epoch: 127, Loss: 1.2467, Train: 1.1172, Val: 1.1178\n",
      "Epoch: 128, Loss: 1.2466, Train: 1.1171, Val: 1.1176\n",
      "Epoch: 129, Loss: 1.2462, Train: 1.1176, Val: 1.1180\n",
      "Epoch: 130, Loss: 1.2466, Train: 1.1168, Val: 1.1172\n",
      "Epoch: 131, Loss: 1.2456, Train: 1.1174, Val: 1.1179\n",
      "Epoch: 132, Loss: 1.2470, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 133, Loss: 1.2457, Train: 1.1164, Val: 1.1168\n",
      "Epoch: 134, Loss: 1.2460, Train: 1.1185, Val: 1.1190\n",
      "Epoch: 135, Loss: 1.2459, Train: 1.1188, Val: 1.1193\n",
      "Epoch: 136, Loss: 1.2457, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 137, Loss: 1.2458, Train: 1.1174, Val: 1.1179\n",
      "Epoch: 138, Loss: 1.2455, Train: 1.1214, Val: 1.1220\n",
      "Epoch: 139, Loss: 1.2458, Train: 1.1196, Val: 1.1201\n",
      "Epoch: 140, Loss: 1.2453, Train: 1.1170, Val: 1.1175\n",
      "Epoch: 141, Loss: 1.2456, Train: 1.1182, Val: 1.1187\n",
      "Epoch: 142, Loss: 1.2451, Train: 1.1198, Val: 1.1204\n",
      "Epoch: 143, Loss: 1.2455, Train: 1.1170, Val: 1.1175\n",
      "Epoch: 144, Loss: 1.2451, Train: 1.1166, Val: 1.1171\n",
      "Epoch: 145, Loss: 1.2451, Train: 1.1185, Val: 1.1191\n",
      "Epoch: 146, Loss: 1.2452, Train: 1.1175, Val: 1.1180\n",
      "Epoch: 147, Loss: 1.2448, Train: 1.1165, Val: 1.1170\n",
      "Epoch: 148, Loss: 1.2450, Train: 1.1177, Val: 1.1182\n",
      "Epoch: 149, Loss: 1.2448, Train: 1.1184, Val: 1.1190\n",
      "Epoch: 150, Loss: 1.2449, Train: 1.1167, Val: 1.1172\n",
      "Epoch: 151, Loss: 1.2448, Train: 1.1167, Val: 1.1172\n",
      "Epoch: 152, Loss: 1.2446, Train: 1.1177, Val: 1.1183\n",
      "Epoch: 153, Loss: 1.2447, Train: 1.1166, Val: 1.1170\n",
      "Epoch: 154, Loss: 1.2445, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 155, Loss: 1.2445, Train: 1.1165, Val: 1.1169\n",
      "Epoch: 156, Loss: 1.2444, Train: 1.1162, Val: 1.1166\n",
      "Epoch: 157, Loss: 1.2443, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 158, Loss: 1.2443, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 159, Loss: 1.2441, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 160, Loss: 1.2440, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 161, Loss: 1.2439, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 162, Loss: 1.2437, Train: 1.1157, Val: 1.1161\n",
      "Epoch: 163, Loss: 1.2444, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 164, Loss: 1.2438, Train: 1.1164, Val: 1.1167\n",
      "Epoch: 165, Loss: 1.2438, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 166, Loss: 1.2438, Train: 1.1158, Val: 1.1162\n",
      "Epoch: 167, Loss: 1.2437, Train: 1.1160, Val: 1.1163\n",
      "Epoch: 168, Loss: 1.2440, Train: 1.1162, Val: 1.1167\n",
      "Epoch: 169, Loss: 1.2513, Train: 1.1336, Val: 1.1335\n",
      "Epoch: 170, Loss: 1.2764, Train: 1.1340, Val: 1.1348\n",
      "Epoch: 171, Loss: 1.2515, Train: 1.1523, Val: 1.1532\n",
      "Epoch: 172, Loss: 1.2640, Train: 1.1167, Val: 1.1170\n",
      "Epoch: 173, Loss: 1.2520, Train: 1.1172, Val: 1.1174\n",
      "Epoch: 174, Loss: 1.2565, Train: 1.1483, Val: 1.1492\n",
      "Epoch: 175, Loss: 1.2536, Train: 1.1483, Val: 1.1493\n",
      "Epoch: 176, Loss: 1.2530, Train: 1.1161, Val: 1.1164\n",
      "Epoch: 177, Loss: 1.2520, Train: 1.1163, Val: 1.1167\n",
      "Epoch: 178, Loss: 1.2496, Train: 1.1453, Val: 1.1462\n",
      "Epoch: 179, Loss: 1.2511, Train: 1.1374, Val: 1.1382\n",
      "Epoch: 180, Loss: 1.2482, Train: 1.1156, Val: 1.1160\n",
      "Epoch: 181, Loss: 1.2495, Train: 1.1160, Val: 1.1164\n",
      "Epoch: 182, Loss: 1.2463, Train: 1.1327, Val: 1.1335\n",
      "Epoch: 183, Loss: 1.2534, Train: 1.1197, Val: 1.1203\n",
      "Epoch: 184, Loss: 1.2435, Train: 1.1155, Val: 1.1159\n",
      "Epoch: 185, Loss: 1.2474, Train: 1.1205, Val: 1.1211\n",
      "Epoch: 186, Loss: 1.2439, Train: 1.1306, Val: 1.1314\n",
      "Epoch: 187, Loss: 1.2467, Train: 1.1217, Val: 1.1224\n",
      "Epoch: 188, Loss: 1.2438, Train: 1.1170, Val: 1.1176\n",
      "Epoch: 189, Loss: 1.2461, Train: 1.1232, Val: 1.1239\n",
      "Epoch: 190, Loss: 1.2436, Train: 1.1320, Val: 1.1329\n",
      "Epoch: 191, Loss: 1.2453, Train: 1.1234, Val: 1.1242\n",
      "Epoch: 192, Loss: 1.2433, Train: 1.1176, Val: 1.1182\n",
      "Epoch: 193, Loss: 1.2445, Train: 1.1212, Val: 1.1219\n",
      "Epoch: 194, Loss: 1.2425, Train: 1.1267, Val: 1.1275\n",
      "Epoch: 195, Loss: 1.2437, Train: 1.1198, Val: 1.1206\n",
      "Epoch: 196, Loss: 1.2419, Train: 1.1163, Val: 1.1169\n",
      "Epoch: 197, Loss: 1.2430, Train: 1.1200, Val: 1.1207\n",
      "Epoch: 198, Loss: 1.2442, Train: 1.1180, Val: 1.1187\n",
      "Epoch: 199, Loss: 1.2424, Train: 1.1229, Val: 1.1238\n",
      "Epoch: 200, Loss: 1.2422, Train: 1.1263, Val: 1.1271\n",
      "Epoch: 201, Loss: 1.2430, Train: 1.1209, Val: 1.1217\n",
      "Epoch: 202, Loss: 1.2422, Train: 1.1191, Val: 1.1198\n",
      "Epoch: 203, Loss: 1.2426, Train: 1.1247, Val: 1.1255\n",
      "Epoch: 204, Loss: 1.2418, Train: 1.1300, Val: 1.1309\n",
      "Epoch: 205, Loss: 1.2419, Train: 1.1266, Val: 1.1275\n",
      "Epoch: 206, Loss: 1.2421, Train: 1.1300, Val: 1.1309\n",
      "Epoch: 207, Loss: 1.2410, Train: 1.1256, Val: 1.1265\n",
      "Epoch: 208, Loss: 1.2402, Train: 1.1216, Val: 1.1225\n",
      "Epoch: 209, Loss: 1.2399, Train: 1.1245, Val: 1.1254\n",
      "Epoch: 210, Loss: 1.2395, Train: 1.1201, Val: 1.1210\n",
      "Epoch: 211, Loss: 1.2386, Train: 1.1158, Val: 1.1166\n",
      "Epoch: 212, Loss: 1.2382, Train: 1.1172, Val: 1.1181\n",
      "Epoch: 213, Loss: 1.2371, Train: 1.1190, Val: 1.1199\n",
      "Epoch: 214, Loss: 1.2358, Train: 1.1191, Val: 1.1199\n",
      "Epoch: 215, Loss: 1.2368, Train: 1.1152, Val: 1.1159\n",
      "Epoch: 216, Loss: 1.2353, Train: 1.1170, Val: 1.1177\n",
      "Epoch: 217, Loss: 1.2359, Train: 1.1145, Val: 1.1150\n",
      "Epoch: 218, Loss: 1.2383, Train: 1.1273, Val: 1.1281\n",
      "Epoch: 219, Loss: 1.2352, Train: 1.1399, Val: 1.1408\n",
      "Epoch: 220, Loss: 1.2362, Train: 1.1216, Val: 1.1222\n",
      "Epoch: 221, Loss: 1.2325, Train: 1.1202, Val: 1.1209\n",
      "Epoch: 222, Loss: 1.2367, Train: 1.1185, Val: 1.1188\n",
      "Epoch: 223, Loss: 1.2886, Train: 1.2075, Val: 1.2090\n",
      "Epoch: 224, Loss: 1.2715, Train: 1.1735, Val: 1.1748\n",
      "Epoch: 225, Loss: 1.2594, Train: 1.1299, Val: 1.1301\n",
      "Epoch: 226, Loss: 1.2653, Train: 1.1227, Val: 1.1229\n",
      "Epoch: 227, Loss: 1.2493, Train: 1.1389, Val: 1.1400\n",
      "Epoch: 228, Loss: 1.2585, Train: 1.1293, Val: 1.1302\n",
      "Epoch: 229, Loss: 1.2485, Train: 1.1263, Val: 1.1266\n",
      "Epoch: 230, Loss: 1.2463, Train: 1.1226, Val: 1.1230\n",
      "Epoch: 231, Loss: 1.2528, Train: 1.1558, Val: 1.1571\n",
      "Epoch: 232, Loss: 1.2493, Train: 1.1640, Val: 1.1653\n",
      "Epoch: 233, Loss: 1.2592, Train: 1.1160, Val: 1.1165\n",
      "Epoch: 234, Loss: 1.2444, Train: 1.1309, Val: 1.1312\n",
      "Epoch: 235, Loss: 1.2569, Train: 1.1210, Val: 1.1218\n",
      "Epoch: 236, Loss: 1.2379, Train: 1.1287, Val: 1.1297\n",
      "Epoch: 237, Loss: 1.2536, Train: 1.1300, Val: 1.1304\n",
      "Epoch: 238, Loss: 1.2338, Train: 1.1569, Val: 1.1573\n",
      "Epoch: 239, Loss: 1.2507, Train: 1.1149, Val: 1.1156\n",
      "Epoch: 240, Loss: 1.2303, Train: 1.1161, Val: 1.1170\n",
      "Epoch: 241, Loss: 1.2463, Train: 1.1149, Val: 1.1156\n",
      "Epoch: 242, Loss: 1.2281, Train: 1.1323, Val: 1.1329\n",
      "Epoch: 243, Loss: 1.2404, Train: 1.1139, Val: 1.1146\n",
      "Epoch: 244, Loss: 1.2293, Train: 1.1099, Val: 1.1107\n",
      "Epoch: 245, Loss: 1.2278, Train: 1.1090, Val: 1.1097\n",
      "Epoch: 246, Loss: 1.2268, Train: 1.1085, Val: 1.1092\n",
      "Epoch: 247, Loss: 1.2264, Train: 1.1079, Val: 1.1087\n",
      "Epoch: 248, Loss: 1.2256, Train: 1.1222, Val: 1.1232\n",
      "Epoch: 249, Loss: 1.2251, Train: 1.1308, Val: 1.1320\n",
      "Epoch: 250, Loss: 1.2229, Train: 1.1341, Val: 1.1353\n",
      "Epoch: 251, Loss: 1.2236, Train: 1.1482, Val: 1.1495\n",
      "Epoch: 252, Loss: 1.2225, Train: 1.1685, Val: 1.1700\n",
      "Epoch: 253, Loss: 1.2218, Train: 1.1665, Val: 1.1679\n",
      "Epoch: 254, Loss: 1.2211, Train: 1.1539, Val: 1.1552\n",
      "Epoch: 255, Loss: 1.2208, Train: 1.1541, Val: 1.1554\n",
      "Epoch: 256, Loss: 1.2203, Train: 1.1658, Val: 1.1672\n",
      "Epoch: 257, Loss: 1.2198, Train: 1.1684, Val: 1.1697\n",
      "Epoch: 258, Loss: 1.2194, Train: 1.1609, Val: 1.1623\n",
      "Epoch: 259, Loss: 1.2187, Train: 1.1604, Val: 1.1618\n",
      "Epoch: 260, Loss: 1.2184, Train: 1.1862, Val: 1.1878\n",
      "Epoch: 261, Loss: 1.2178, Train: 1.1907, Val: 1.1924\n",
      "Epoch: 262, Loss: 1.2176, Train: 1.1743, Val: 1.1759\n",
      "Epoch: 263, Loss: 1.2169, Train: 1.1603, Val: 1.1617\n",
      "Epoch: 264, Loss: 1.2166, Train: 1.1630, Val: 1.1645\n",
      "Epoch: 265, Loss: 1.2160, Train: 1.1504, Val: 1.1518\n",
      "Epoch: 266, Loss: 1.2156, Train: 1.1314, Val: 1.1327\n",
      "Epoch: 267, Loss: 1.2155, Train: 1.1326, Val: 1.1339\n",
      "Epoch: 268, Loss: 1.2151, Train: 1.1299, Val: 1.1311\n",
      "Epoch: 269, Loss: 1.2145, Train: 1.1276, Val: 1.1288\n",
      "Epoch: 270, Loss: 1.2139, Train: 1.1356, Val: 1.1369\n",
      "Epoch: 271, Loss: 1.2137, Train: 1.1466, Val: 1.1480\n",
      "Epoch: 272, Loss: 1.2131, Train: 1.1484, Val: 1.1498\n",
      "Epoch: 273, Loss: 1.2128, Train: 1.1319, Val: 1.1332\n",
      "Epoch: 274, Loss: 1.2125, Train: 1.1425, Val: 1.1438\n",
      "Epoch: 275, Loss: 1.2117, Train: 1.1404, Val: 1.1417\n",
      "Epoch: 276, Loss: 1.2111, Train: 1.1306, Val: 1.1318\n",
      "Epoch: 277, Loss: 1.2192, Train: 1.1075, Val: 1.1084\n",
      "Epoch: 278, Loss: 1.2148, Train: 1.1037, Val: 1.1042\n",
      "Epoch: 279, Loss: 1.2135, Train: 1.1043, Val: 1.1050\n",
      "Epoch: 280, Loss: 1.2143, Train: 1.1033, Val: 1.1038\n",
      "Epoch: 281, Loss: 1.2176, Train: 1.1111, Val: 1.1120\n",
      "Epoch: 282, Loss: 1.2136, Train: 1.1125, Val: 1.1135\n",
      "Epoch: 283, Loss: 1.2155, Train: 1.1043, Val: 1.1051\n",
      "Epoch: 284, Loss: 1.2142, Train: 1.1078, Val: 1.1087\n",
      "Epoch: 285, Loss: 1.2137, Train: 1.1055, Val: 1.1066\n",
      "Epoch: 286, Loss: 1.2115, Train: 1.1063, Val: 1.1074\n",
      "Epoch: 287, Loss: 1.2184, Train: 1.1182, Val: 1.1187\n",
      "Epoch: 288, Loss: 1.2162, Train: 1.1279, Val: 1.1276\n",
      "Epoch: 289, Loss: 1.2154, Train: 1.1148, Val: 1.1148\n",
      "Epoch: 290, Loss: 1.2148, Train: 1.1093, Val: 1.1097\n",
      "Epoch: 291, Loss: 1.2145, Train: 1.1057, Val: 1.1061\n",
      "Epoch: 292, Loss: 1.2124, Train: 1.1039, Val: 1.1045\n",
      "Epoch: 293, Loss: 1.2123, Train: 1.1026, Val: 1.1034\n",
      "Epoch: 294, Loss: 1.2132, Train: 1.1031, Val: 1.1038\n",
      "Epoch: 295, Loss: 1.2109, Train: 1.1088, Val: 1.1095\n",
      "Epoch: 296, Loss: 1.2102, Train: 1.1096, Val: 1.1102\n",
      "Epoch: 297, Loss: 1.2099, Train: 1.1063, Val: 1.1069\n",
      "Epoch: 298, Loss: 1.2099, Train: 1.1087, Val: 1.1095\n",
      "Epoch: 299, Loss: 1.2092, Train: 1.1234, Val: 1.1244\n",
      "Epoch: 300, Loss: 1.2086, Train: 1.1243, Val: 1.1254\n",
      "Epoch: 301, Loss: 1.2082, Train: 1.1237, Val: 1.1249\n",
      "Epoch: 302, Loss: 1.2079, Train: 1.1278, Val: 1.1290\n",
      "Epoch: 303, Loss: 1.2074, Train: 1.1115, Val: 1.1126\n",
      "Epoch: 304, Loss: 1.2065, Train: 1.1043, Val: 1.1051\n",
      "Epoch: 305, Loss: 1.2060, Train: 1.1057, Val: 1.1065\n",
      "Epoch: 306, Loss: 1.2057, Train: 1.1077, Val: 1.1087\n",
      "Epoch: 307, Loss: 1.2068, Train: 1.1063, Val: 1.1069\n",
      "Epoch: 308, Loss: 1.2067, Train: 1.0986, Val: 1.0995\n",
      "Epoch: 309, Loss: 1.2044, Train: 1.0997, Val: 1.1007\n",
      "Epoch: 310, Loss: 1.2057, Train: 1.1110, Val: 1.1116\n",
      "Epoch: 311, Loss: 1.2045, Train: 1.1064, Val: 1.1070\n",
      "Epoch: 312, Loss: 1.2022, Train: 1.0992, Val: 1.1002\n",
      "Epoch: 313, Loss: 1.2082, Train: 1.1411, Val: 1.1419\n",
      "Epoch: 314, Loss: 1.2059, Train: 1.1241, Val: 1.1252\n",
      "Epoch: 315, Loss: 1.2036, Train: 1.1008, Val: 1.1022\n",
      "Epoch: 316, Loss: 1.2066, Train: 1.1043, Val: 1.1057\n",
      "Epoch: 317, Loss: 1.2018, Train: 1.1089, Val: 1.1103\n",
      "Epoch: 318, Loss: 1.2050, Train: 1.1009, Val: 1.1023\n",
      "Epoch: 319, Loss: 1.2021, Train: 1.1038, Val: 1.1053\n",
      "Epoch: 320, Loss: 1.2032, Train: 1.1048, Val: 1.1061\n",
      "Epoch: 321, Loss: 1.2007, Train: 1.1137, Val: 1.1150\n",
      "Epoch: 322, Loss: 1.2013, Train: 1.1021, Val: 1.1035\n",
      "Epoch: 323, Loss: 1.1996, Train: 1.1010, Val: 1.1024\n",
      "Epoch: 324, Loss: 1.1995, Train: 1.1000, Val: 1.1013\n",
      "Epoch: 325, Loss: 1.1975, Train: 1.0998, Val: 1.1012\n",
      "Epoch: 326, Loss: 1.1966, Train: 1.1010, Val: 1.1024\n",
      "Epoch: 327, Loss: 1.1998, Train: 1.0996, Val: 1.1011\n",
      "Epoch: 328, Loss: 1.2017, Train: 1.0968, Val: 1.0980\n",
      "Epoch: 329, Loss: 1.2077, Train: 1.1234, Val: 1.1249\n",
      "Epoch: 330, Loss: 1.2054, Train: 1.1011, Val: 1.1024\n",
      "Epoch: 331, Loss: 1.1997, Train: 1.1093, Val: 1.1103\n",
      "Epoch: 332, Loss: 1.1998, Train: 1.1063, Val: 1.1074\n",
      "Epoch: 333, Loss: 1.2346, Train: 1.1485, Val: 1.1489\n",
      "Epoch: 334, Loss: 1.6181, Train: 1.6696, Val: 1.6697\n",
      "Epoch: 335, Loss: 1.4147, Train: 1.4696, Val: 1.4690\n",
      "Epoch: 336, Loss: 1.2637, Train: 1.3737, Val: 1.3731\n",
      "Epoch: 337, Loss: 1.3613, Train: 1.7088, Val: 1.7086\n",
      "Epoch: 338, Loss: 1.2278, Train: 1.7915, Val: 1.7897\n",
      "Epoch: 339, Loss: 1.3126, Train: 1.6997, Val: 1.6992\n",
      "Epoch: 340, Loss: 1.2536, Train: 1.2831, Val: 1.2821\n",
      "Epoch: 341, Loss: 1.3188, Train: 1.3092, Val: 1.3076\n",
      "Epoch: 342, Loss: 1.2794, Train: 1.5632, Val: 1.5610\n",
      "Epoch: 343, Loss: 1.2892, Train: 1.5417, Val: 1.5394\n",
      "Epoch: 344, Loss: 1.2956, Train: 1.2784, Val: 1.2766\n",
      "Epoch: 345, Loss: 1.2589, Train: 1.2078, Val: 1.2067\n",
      "Epoch: 346, Loss: 1.2877, Train: 1.3175, Val: 1.3166\n",
      "Epoch: 347, Loss: 1.2427, Train: 1.4843, Val: 1.4836\n",
      "Epoch: 348, Loss: 1.2821, Train: 1.3830, Val: 1.3825\n",
      "Epoch: 349, Loss: 1.2568, Train: 1.2166, Val: 1.2163\n",
      "Epoch: 350, Loss: 1.2639, Train: 1.1913, Val: 1.1909\n",
      "Epoch: 351, Loss: 1.2650, Train: 1.2604, Val: 1.2598\n",
      "Epoch: 352, Loss: 1.2461, Train: 1.3067, Val: 1.3058\n",
      "Epoch: 353, Loss: 1.2616, Train: 1.2312, Val: 1.2304\n",
      "Epoch: 354, Loss: 1.2550, Train: 1.1625, Val: 1.1621\n",
      "Epoch: 355, Loss: 1.2577, Train: 1.1620, Val: 1.1617\n",
      "Epoch: 356, Loss: 1.2531, Train: 1.2135, Val: 1.2130\n",
      "Epoch: 357, Loss: 1.2487, Train: 1.2317, Val: 1.2312\n",
      "Epoch: 358, Loss: 1.2561, Train: 1.1768, Val: 1.1764\n",
      "Epoch: 359, Loss: 1.2413, Train: 1.1490, Val: 1.1486\n",
      "Epoch: 360, Loss: 1.2485, Train: 1.1659, Val: 1.1652\n",
      "Epoch: 361, Loss: 1.2397, Train: 1.1994, Val: 1.1985\n",
      "Epoch: 362, Loss: 1.2526, Train: 1.1896, Val: 1.1887\n",
      "Epoch: 363, Loss: 1.2490, Train: 1.1355, Val: 1.1355\n",
      "Epoch: 364, Loss: 1.2416, Train: 1.1269, Val: 1.1277\n",
      "Epoch: 365, Loss: 1.2441, Train: 1.1341, Val: 1.1350\n",
      "Epoch: 366, Loss: 1.2373, Train: 1.1354, Val: 1.1362\n",
      "Epoch: 367, Loss: 1.2387, Train: 1.1337, Val: 1.1346\n",
      "Epoch: 368, Loss: 1.2376, Train: 1.1303, Val: 1.1312\n",
      "Epoch: 369, Loss: 1.2355, Train: 1.1277, Val: 1.1287\n",
      "Epoch: 370, Loss: 1.2310, Train: 1.1254, Val: 1.1260\n",
      "Epoch: 371, Loss: 1.2505, Train: 1.1183, Val: 1.1189\n",
      "Epoch: 372, Loss: 1.2443, Train: 1.1173, Val: 1.1178\n",
      "Epoch: 373, Loss: 1.2354, Train: 1.1163, Val: 1.1166\n",
      "Epoch: 374, Loss: 1.2388, Train: 1.1175, Val: 1.1173\n",
      "Epoch: 375, Loss: 1.2451, Train: 1.1257, Val: 1.1251\n",
      "Epoch: 376, Loss: 1.2420, Train: 1.1217, Val: 1.1213\n",
      "Epoch: 377, Loss: 1.2422, Train: 1.1190, Val: 1.1189\n",
      "Epoch: 378, Loss: 1.2464, Train: 1.1194, Val: 1.1194\n",
      "Epoch: 379, Loss: 1.2461, Train: 1.1221, Val: 1.1220\n",
      "Epoch: 380, Loss: 1.2472, Train: 1.1233, Val: 1.1231\n",
      "Epoch: 381, Loss: 1.2408, Train: 1.1277, Val: 1.1273\n",
      "Epoch: 382, Loss: 1.2452, Train: 1.1244, Val: 1.1240\n",
      "Epoch: 383, Loss: 1.2418, Train: 1.1223, Val: 1.1221\n",
      "Epoch: 384, Loss: 1.2486, Train: 1.1242, Val: 1.1240\n",
      "Epoch: 385, Loss: 1.2451, Train: 1.1254, Val: 1.1252\n",
      "Epoch: 386, Loss: 1.2459, Train: 1.1231, Val: 1.1232\n",
      "Epoch: 387, Loss: 1.2457, Train: 1.1195, Val: 1.1200\n",
      "Epoch: 388, Loss: 1.2433, Train: 1.1199, Val: 1.1205\n",
      "Epoch: 389, Loss: 1.2459, Train: 1.1180, Val: 1.1185\n",
      "Epoch: 390, Loss: 1.2437, Train: 1.1172, Val: 1.1176\n",
      "Epoch: 391, Loss: 1.2447, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 392, Loss: 1.2430, Train: 1.1170, Val: 1.1176\n",
      "Epoch: 393, Loss: 1.2432, Train: 1.1176, Val: 1.1182\n",
      "Epoch: 394, Loss: 1.2431, Train: 1.1163, Val: 1.1168\n",
      "Epoch: 395, Loss: 1.2424, Train: 1.1161, Val: 1.1166\n",
      "Epoch: 396, Loss: 1.2429, Train: 1.1183, Val: 1.1190\n",
      "Epoch: 397, Loss: 1.2420, Train: 1.1222, Val: 1.1230\n",
      "Epoch: 398, Loss: 1.2422, Train: 1.1229, Val: 1.1236\n",
      "Epoch: 399, Loss: 1.2414, Train: 1.1219, Val: 1.1225\n",
      "Epoch: 400, Loss: 1.2411, Train: 1.1240, Val: 1.1245\n",
      "Epoch: 401, Loss: 1.2409, Train: 1.1304, Val: 1.1310\n",
      "Epoch: 402, Loss: 1.2404, Train: 1.1370, Val: 1.1376\n",
      "Epoch: 403, Loss: 1.2404, Train: 1.1402, Val: 1.1409\n",
      "Epoch: 404, Loss: 1.2396, Train: 1.1439, Val: 1.1446\n",
      "Epoch: 405, Loss: 1.2392, Train: 1.1564, Val: 1.1572\n",
      "Epoch: 406, Loss: 1.2383, Train: 1.1787, Val: 1.1797\n",
      "Epoch: 407, Loss: 1.2373, Train: 1.2016, Val: 1.2026\n",
      "Epoch: 408, Loss: 1.2358, Train: 1.2255, Val: 1.2266\n",
      "Epoch: 409, Loss: 1.2317, Train: 1.2418, Val: 1.2430\n",
      "Epoch: 410, Loss: 1.2513, Train: 1.1739, Val: 1.1746\n",
      "Epoch: 411, Loss: 1.2463, Train: 1.1540, Val: 1.1547\n",
      "Epoch: 412, Loss: 1.2553, Train: 1.1902, Val: 1.1914\n",
      "Epoch: 413, Loss: 1.2380, Train: 1.2151, Val: 1.2165\n",
      "Epoch: 414, Loss: 1.2596, Train: 1.1680, Val: 1.1691\n",
      "Epoch: 415, Loss: 1.2374, Train: 1.1407, Val: 1.1414\n",
      "Epoch: 416, Loss: 1.2527, Train: 1.1596, Val: 1.1606\n",
      "Epoch: 417, Loss: 1.2377, Train: 1.2023, Val: 1.2037\n",
      "Epoch: 418, Loss: 1.2444, Train: 1.1997, Val: 1.2012\n",
      "Epoch: 419, Loss: 1.2385, Train: 1.1633, Val: 1.1647\n",
      "Epoch: 420, Loss: 1.2323, Train: 1.1511, Val: 1.1522\n",
      "Epoch: 421, Loss: 1.2395, Train: 1.1710, Val: 1.1720\n",
      "Epoch: 422, Loss: 1.2320, Train: 1.1893, Val: 1.1903\n",
      "Epoch: 423, Loss: 1.2357, Train: 1.1758, Val: 1.1769\n",
      "Epoch: 424, Loss: 1.2289, Train: 1.1528, Val: 1.1538\n",
      "Epoch: 425, Loss: 1.2296, Train: 1.1564, Val: 1.1572\n",
      "Epoch: 426, Loss: 1.2256, Train: 1.1846, Val: 1.1855\n",
      "Epoch: 427, Loss: 1.2204, Train: 1.1998, Val: 1.2010\n",
      "Epoch: 428, Loss: 1.2211, Train: 1.1849, Val: 1.1860\n",
      "Epoch: 429, Loss: 1.2134, Train: 1.1734, Val: 1.1744\n",
      "Epoch: 430, Loss: 1.2146, Train: 1.1936, Val: 1.1945\n",
      "Epoch: 431, Loss: 1.2206, Train: 1.2196, Val: 1.2207\n",
      "Epoch: 432, Loss: 1.2168, Train: 1.2195, Val: 1.2212\n",
      "Epoch: 433, Loss: 1.2368, Train: 1.1921, Val: 1.1926\n",
      "Epoch: 434, Loss: 1.2209, Train: 1.1501, Val: 1.1500\n",
      "Epoch: 435, Loss: 1.2207, Train: 1.1262, Val: 1.1258\n",
      "Epoch: 436, Loss: 1.2058, Train: 1.1213, Val: 1.1209\n",
      "Epoch: 437, Loss: 1.2123, Train: 1.1253, Val: 1.1248\n",
      "Epoch: 438, Loss: 1.2098, Train: 1.1282, Val: 1.1274\n",
      "Epoch: 439, Loss: 1.2247, Train: 1.1251, Val: 1.1244\n",
      "Epoch: 440, Loss: 1.2236, Train: 1.1170, Val: 1.1169\n",
      "Epoch: 441, Loss: 1.2292, Train: 1.1145, Val: 1.1146\n",
      "Epoch: 442, Loss: 1.2250, Train: 1.1194, Val: 1.1193\n",
      "Epoch: 443, Loss: 1.2268, Train: 1.1174, Val: 1.1175\n",
      "Epoch: 444, Loss: 1.2234, Train: 1.1138, Val: 1.1140\n",
      "Epoch: 445, Loss: 1.2178, Train: 1.1165, Val: 1.1168\n",
      "Epoch: 446, Loss: 1.2197, Train: 1.1249, Val: 1.1248\n",
      "Epoch: 447, Loss: 1.2148, Train: 1.1302, Val: 1.1297\n",
      "Epoch: 448, Loss: 1.2169, Train: 1.1231, Val: 1.1226\n",
      "Epoch: 449, Loss: 1.2186, Train: 1.1206, Val: 1.1203\n",
      "Epoch: 450, Loss: 1.2130, Train: 1.1205, Val: 1.1202\n",
      "Epoch: 451, Loss: 1.2117, Train: 1.1207, Val: 1.1206\n",
      "Epoch: 452, Loss: 1.2103, Train: 1.1159, Val: 1.1161\n",
      "Epoch: 453, Loss: 1.2147, Train: 1.1454, Val: 1.1459\n",
      "Epoch: 454, Loss: 1.2410, Train: 1.1278, Val: 1.1289\n",
      "Epoch: 455, Loss: 1.2081, Train: 1.1781, Val: 1.1795\n",
      "Epoch: 456, Loss: 1.2247, Train: 1.1482, Val: 1.1488\n",
      "Epoch: 457, Loss: 1.2212, Train: 1.1399, Val: 1.1397\n",
      "Epoch: 458, Loss: 1.2066, Train: 1.1429, Val: 1.1426\n",
      "Epoch: 459, Loss: 1.2112, Train: 1.1585, Val: 1.1591\n",
      "Epoch: 460, Loss: 1.2091, Train: 1.2178, Val: 1.2192\n",
      "Epoch: 461, Loss: 1.2133, Train: 1.2225, Val: 1.2239\n",
      "Epoch: 462, Loss: 1.2025, Train: 1.1579, Val: 1.1590\n",
      "Epoch: 463, Loss: 1.2058, Train: 1.1191, Val: 1.1204\n",
      "Epoch: 464, Loss: 1.1929, Train: 1.1624, Val: 1.1643\n",
      "Epoch: 465, Loss: 1.2113, Train: 1.1366, Val: 1.1382\n",
      "Epoch: 466, Loss: 1.1876, Train: 1.1483, Val: 1.1498\n",
      "Epoch: 467, Loss: 1.1944, Train: 1.1390, Val: 1.1405\n",
      "Epoch: 468, Loss: 1.1896, Train: 1.1244, Val: 1.1258\n",
      "Epoch: 469, Loss: 1.2111, Train: 1.1317, Val: 1.1325\n",
      "Epoch: 470, Loss: 1.1937, Train: 1.1688, Val: 1.1692\n",
      "Epoch: 471, Loss: 1.1998, Train: 1.1656, Val: 1.1661\n",
      "Epoch: 472, Loss: 1.1976, Train: 1.1442, Val: 1.1453\n",
      "Epoch: 473, Loss: 1.1914, Train: 1.1528, Val: 1.1541\n",
      "Epoch: 474, Loss: 1.1976, Train: 1.1299, Val: 1.1312\n",
      "Epoch: 475, Loss: 1.1993, Train: 1.1252, Val: 1.1260\n",
      "Epoch: 476, Loss: 1.1980, Train: 1.1153, Val: 1.1160\n",
      "Epoch: 477, Loss: 1.1976, Train: 1.1109, Val: 1.1120\n",
      "Epoch: 478, Loss: 1.1956, Train: 1.1152, Val: 1.1168\n",
      "Epoch: 479, Loss: 1.1946, Train: 1.1113, Val: 1.1131\n",
      "Epoch: 480, Loss: 1.1945, Train: 1.0996, Val: 1.1014\n",
      "Epoch: 481, Loss: 1.1881, Train: 1.1004, Val: 1.1019\n",
      "Epoch: 482, Loss: 1.1876, Train: 1.1031, Val: 1.1050\n",
      "Epoch: 483, Loss: 1.1811, Train: 1.1112, Val: 1.1135\n",
      "Epoch: 484, Loss: 1.1781, Train: 1.1216, Val: 1.1240\n",
      "Epoch: 485, Loss: 1.1869, Train: 1.1488, Val: 1.1510\n",
      "Epoch: 486, Loss: 1.1817, Train: 1.1692, Val: 1.1713\n",
      "Epoch: 487, Loss: 1.1835, Train: 1.1435, Val: 1.1459\n",
      "Epoch: 488, Loss: 1.1796, Train: 1.1412, Val: 1.1438\n",
      "Epoch: 489, Loss: 1.1782, Train: 1.1463, Val: 1.1492\n",
      "Epoch: 490, Loss: 1.1805, Train: 1.1645, Val: 1.1675\n",
      "Epoch: 491, Loss: 1.1758, Train: 1.1803, Val: 1.1833\n",
      "Epoch: 492, Loss: 1.1760, Train: 1.1432, Val: 1.1463\n",
      "Epoch: 493, Loss: 1.1731, Train: 1.1226, Val: 1.1257\n",
      "Epoch: 494, Loss: 1.1726, Train: 1.1405, Val: 1.1436\n",
      "Epoch: 495, Loss: 1.1737, Train: 1.1303, Val: 1.1335\n",
      "Epoch: 496, Loss: 1.1689, Train: 1.1364, Val: 1.1398\n",
      "Epoch: 497, Loss: 1.1707, Train: 1.1277, Val: 1.1311\n",
      "Epoch: 498, Loss: 1.1688, Train: 1.1062, Val: 1.1095\n",
      "Epoch: 499, Loss: 1.1683, Train: 1.0944, Val: 1.0976\n",
      "Epoch: 500, Loss: 1.1675, Train: 1.1098, Val: 1.1129\n",
      "Test RMSE: 1.1144\n",
      "              userId        movieId         rating         target\n",
      "count  100020.000000  100020.000000  100020.000000  100020.000000\n",
      "mean     3018.776515     877.536033       3.364030       3.583673\n",
      "std      1727.484387     741.673176       0.195963       1.116938\n",
      "min         0.000000       0.000000       2.382774       1.000000\n",
      "25%      1500.000000     259.000000       3.333824       3.000000\n",
      "50%      3066.000000     693.000000       3.359496       4.000000\n",
      "75%      4472.000000    1292.000000       3.416277       4.000000\n",
      "max      6039.000000    3702.000000       4.048375       5.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5961/5961 [00:05<00:00, 1123.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.80909243415533\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## KPGINplus\n",
    "args.model_name = \"KPGINPlus\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for k, kernel in variations:\n",
    "    args.K = k\n",
    "    args.kernel = kernel\n",
    "    model = Model(hidden_channels=32).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    def train(loss_type=\"mse\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        splitted_data = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        edge_types=[('user', 'rates', 'movie')],\n",
    "        rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    "    )(data)\n",
    "        pred = model(train_data[\"user\", \"rates\", \"movie\"], train_data['user', 'movie'].edge_label_index)\n",
    "        target = train_data['user', 'movie'].edge_label\n",
    "        if loss_type ==\"mse\":\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        elif loss_type == \"BPR\":\n",
    "            loss = F.mse_loss(pred, target)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return float(loss)\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test(data):\n",
    "        data = data.to(device)\n",
    "        model.eval()\n",
    "        pred = model(data[\"user\", \"rates\", \"movie\"],\n",
    "                     data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        return float(rmse)\n",
    "    \n",
    "    # ep = 500\n",
    "    # for i in [1]:\n",
    "    #     model = get_model__()\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        train_data = train_data.to(device)\n",
    "        loss = train()\n",
    "        train_rmse = test(train_data)\n",
    "        val_rmse = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "              f'Val: {val_rmse:.4f}')\n",
    "    with torch.no_grad():\n",
    "        test_data = test_data.to(device)\n",
    "        pred = model(test_data[\"user\", \"rates\", \"movie\"],\n",
    "                     test_data['user', 'movie'].edge_label_index)\n",
    "        \n",
    "        # pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
    "        #              test_data['user', 'movie'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=5)\n",
    "        target = test_data['user', 'movie'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        print(f'Test RMSE: {rmse:.4f}')\n",
    "\n",
    "    userId = test_data['user', 'movie'].edge_label_index[0].cpu().numpy()\n",
    "    movieId = test_data['user', 'movie'].edge_label_index[1].cpu().numpy()\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "    predicted_df = pd.DataFrame({'userId': userId, 'movieId': movieId, 'rating': pred, 'target': target})\n",
    "    \n",
    "    print(predicted_df.describe())\n",
    "    \n",
    "    print(hit_rate_top_k(k=10, predicted_df=predicted_df))\n",
    "\n",
    "    print(\"-------------------------------------------------\"*10)\n",
    "    print(\"-------------------------------------------------\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  1\n",
      "K:  1\n",
      "K:  2\n",
      "K:  2\n",
      "K:  3\n",
      "K:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat results/KPGINPlus.txt  | grep K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel:  gd\n",
      "Kernel:  spd\n",
      "Kernel:  gd\n",
      "Kernel:  spd\n",
      "Kernel:  gd\n",
      "Kernel:  spd\n"
     ]
    }
   ],
   "source": [
    "!cat results/KPGINPlus.txt | grep Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.866282\n",
      "2.046153\n",
      "2.175252\n",
      "2.114229\n",
      "1.966760\n",
      "2.382774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat results/KPGINPlus.txt  | grep min | awk '{print $4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.000000\n",
      "5.000000\n",
      "4.782679\n",
      "4.737682\n",
      "5.000000\n",
      "4.048375\n"
     ]
    }
   ],
   "source": [
    "!cat results/KPGINPlus.txt  | grep max | awk '{print $4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402697\n",
      "0.258405\n",
      "0.325537\n",
      "0.257493\n",
      "0.335514\n",
      "0.195963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat results/KPGINPlus.txt  | grep std | awk '{print $4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.254896\n",
      "3.551714\n",
      "3.453632\n",
      "3.653458\n",
      "3.560501\n",
      "3.364030\n"
     ]
    }
   ],
   "source": [
    "!cat results/KPGINPlus.txt | grep mean | awk '{print $4}'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00292370b95a4ca39c4261f7756674c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b821135bfeb64fc290ac622b73a2c81d",
       "IPY_MODEL_050c709880634afb8cfbbf9c6e6d4c08",
       "IPY_MODEL_347eeacd2c8548098dd0c84cc2a2b55d"
      ],
      "layout": "IPY_MODEL_ec4242dd4fd54be1b49ff6ced32df6b1"
     }
    },
    "006cbd458d9e4d2da68ff964cb424e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01e3d0f8640d49e3a9de2bb1824440e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9b28bb3b91f460492f7a2a6c78b2f39",
      "max": 1175,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ead3fe6696c54179bd74ea65f382fb41",
      "value": 1175
     }
    },
    "02bdda5953704642afc0ff3accff2c83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "050c709880634afb8cfbbf9c6e6d4c08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_875bc0f0b47c4d5fa3628bba5cddfd1a",
      "max": 305,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e2840d415724a4aa30f1f3b149dbb0a",
      "value": 305
     }
    },
    "063b57709d1a4b7e9e4f4d2ecdaee3c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "073ac27236834df2b1cf9c216abc79e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71a401e0b3284652a96fe0e36f2b97b1",
      "max": 90888945,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82af6ec6f7324f02bfeccc4ecab72b9b",
      "value": 90888945
     }
    },
    "0a16d05cfdd34f6a83511335e14a97a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a715bd4c9474ff099928e14f7075f42",
      "placeholder": "​",
      "style": "IPY_MODEL_6db786e7157743008b1d1024d71ed2cb",
      "value": "Downloading (…)e9125/tokenizer.json: 100%"
     }
    },
    "0b53fdf486b643ee9836adfb1e4890ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0d2aa8937ed24c3b82c6d316ab4076e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ace8d1d93404476783871a2873b46b37",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b05ca66fce674dca820eede95d3711c8",
      "value": 116
     }
    },
    "0eb13344a1724e7aaf9afda54f15b493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f559778b18346adb3a259c734cdd724",
      "max": 39265,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ddb5bd908e1d46d187c23e3cacfae75a",
      "value": 39265
     }
    },
    "1021a4d43fde4a46baf2cb38913f9ab7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "129b98c69e934d4191957c0623e660fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13602bc74b2c4844b9796f110d91e23e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "171f361c595946338f255f217fb33b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2fd0618070ad4ca6bb44220ceaa75b70",
       "IPY_MODEL_19fad356297a4563bc6bd94850a23ef5",
       "IPY_MODEL_2912bb4e3f4b4ed2a86d6016faf0f8c7"
      ],
      "layout": "IPY_MODEL_e751597fa4904d4fa680581adb4f47dd"
     }
    },
    "189380c32b9e48159aad4ac1880604b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18c9438aa1e8468b991de01bd1b77df2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cb08e56b4ca48758adc09030c871c94",
      "placeholder": "​",
      "style": "IPY_MODEL_a32b0b6b3c834b068d769dab2558572f",
      "value": "Downloading (…)7e55de9125/README.md: 100%"
     }
    },
    "18f3008fcfa24cc6a8dff5d85b6b6d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "19fad356297a4563bc6bd94850a23ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59e67a685bf54f8d83fbb2270de37940",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c085ff4c203c474ba5e324d0e2f06e42",
      "value": 350
     }
    },
    "1c2bcaff060d408cb8c0138dab6afb87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1edb24d73c004692928b0b915c113be9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f67efbd2c474dc0aa7383dfb65ceee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "204c00dd2b634413821a77432a532c76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "207ad90aa1254ba8a5b05de42444da70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26fdaa7b72f44bd69b677665537e4617": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27236a64fb9b42c09d24f2ce350227f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28c1151d8ffe467ea40d8f9c9e0cd291": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2912bb4e3f4b4ed2a86d6016faf0f8c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b1437058b8240d39d144c899697d006",
      "placeholder": "​",
      "style": "IPY_MODEL_949c1facc1264cba9ca17f07747a3eeb",
      "value": " 350/350 [00:00&lt;00:00, 24.4kB/s]"
     }
    },
    "29c8e87ba7954e8e992650be6e5c7546": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c02c05c036c42d3bdfd803816295b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf088c2fc6474d30b706992e7e1fcf42",
      "placeholder": "​",
      "style": "IPY_MODEL_4dbef274b3ff464289bf8279ba2345bb",
      "value": "Downloading (…)55de9125/config.json: 100%"
     }
    },
    "2c31b30d157944bf8b38b666836fa9b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d90fe4943e3a468a8f33e5ab9d0a1e23",
       "IPY_MODEL_9a7b4718f2aa46e295d59a25e645d8ce",
       "IPY_MODEL_cfec6212f00e43f9b0e69c00f18881f6"
      ],
      "layout": "IPY_MODEL_72b5874ea4554f4ebf690421200b3856"
     }
    },
    "2fd0618070ad4ca6bb44220ceaa75b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26fdaa7b72f44bd69b677665537e4617",
      "placeholder": "​",
      "style": "IPY_MODEL_a8829d3d46fd4508b55b5227c3efcb4b",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "347eeacd2c8548098dd0c84cc2a2b55d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c57bdd883e4407aa0b896de3a41d886",
      "placeholder": "​",
      "style": "IPY_MODEL_92cb3b8ff3bc4de2ada06246ceb3c1a9",
      "value": " 305/305 [00:06&lt;00:00, 137.66it/s]"
     }
    },
    "35c6762b00824d8cada477984f34c9ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02bdda5953704642afc0ff3accff2c83",
      "placeholder": "​",
      "style": "IPY_MODEL_207ad90aa1254ba8a5b05de42444da70",
      "value": " 53.0/53.0 [00:00&lt;00:00, 2.24kB/s]"
     }
    },
    "365e6d10b1ca4841a45664163cf76da9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39220418c8a04408a906badd447c14ae",
      "placeholder": "​",
      "style": "IPY_MODEL_bfc71c7b86834b29a57c03b8fca989ac",
      "value": " 1.18k/1.18k [00:00&lt;00:00, 57.7kB/s]"
     }
    },
    "366968eb63484a20beb5f1ff865b03a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "386003509cec4f3b88bd7399a5984b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1edb24d73c004692928b0b915c113be9",
      "placeholder": "​",
      "style": "IPY_MODEL_46d374c237b04f82b0c64bdccae09071",
      "value": " 39.3k/39.3k [00:00&lt;00:00, 187kB/s]"
     }
    },
    "39220418c8a04408a906badd447c14ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a34ff034cfc4cac95d19583c234aa6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18c9438aa1e8468b991de01bd1b77df2",
       "IPY_MODEL_9c433ef967a3406daa193ce904e9d222",
       "IPY_MODEL_6c3d50d6ccaf4cb6b35fef21b5feb90c"
      ],
      "layout": "IPY_MODEL_509c11a30a984efd9340e3039c2ad7a0"
     }
    },
    "3b0b722f2aad4c4c97478e66a20bdc6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c57bdd883e4407aa0b896de3a41d886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "451c0e6ca80840598800340df57e036a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46d374c237b04f82b0c64bdccae09071": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4770667b6b2d4ee097c0693407fa77a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49a8a040041244768d803bf98d98c666": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a69524be52449129364390309db45ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a715bd4c9474ff099928e14f7075f42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b1437058b8240d39d144c899697d006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cca8f40500d4698b709f225b0e8ac21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99b60ebc5adf42489183ea64e5cf2707",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1408b78587f4fd4b6c043e118f293e3",
      "value": 112
     }
    },
    "4d3401af5fec4a97aa305c49a0e21ec8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dbef274b3ff464289bf8279ba2345bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "509c11a30a984efd9340e3039c2ad7a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5299dedb8eff4202ae51211b428fd6b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df8c3d8a3fc44d7e9ce7a6a86f625aef",
      "placeholder": "​",
      "style": "IPY_MODEL_8431f4111a9240758cdbda1c830576fa",
      "value": "Downloading (…)_Pooling/config.json: 100%"
     }
    },
    "5336152e581b41e5b98b18d723009ca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "53578221920149b7b0212a63b531e3ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5e61177652e4e63b59188bd336df81a",
       "IPY_MODEL_f84b23cb30aa4006b0cc8ef21d15878f",
       "IPY_MODEL_35c6762b00824d8cada477984f34c9ec"
      ],
      "layout": "IPY_MODEL_92bc9919deb84127b7d975e60c175520"
     }
    },
    "548ce1469b414a659f862cc352014afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8243c17d3da141ceb37a0d1006eed8e0",
      "placeholder": "​",
      "style": "IPY_MODEL_063b57709d1a4b7e9e4f4d2ecdaee3c1",
      "value": "Downloading (…)125/data_config.json: 100%"
     }
    },
    "5513af69ee814214ba0bc67f55ef5296": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56bff5bc4a544bb8b1fa58e6caea579e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59e67a685bf54f8d83fbb2270de37940": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63c20dbc9081488ba16996a9717ddc2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6930e5b9888d4d79819bb6477ff2a8b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1021a4d43fde4a46baf2cb38913f9ab7",
      "placeholder": "​",
      "style": "IPY_MODEL_75f13359b6744f3b92f70b1900de7e56",
      "value": "Downloading (…)e9125/.gitattributes: 100%"
     }
    },
    "69f29e15d31e441aa01c49d6890b4947": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c3d50d6ccaf4cb6b35fef21b5feb90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f659bd8028ab45e9b223b39668ad1d4f",
      "placeholder": "​",
      "style": "IPY_MODEL_3b0b722f2aad4c4c97478e66a20bdc6e",
      "value": " 10.6k/10.6k [00:00&lt;00:00, 439kB/s]"
     }
    },
    "6d09e8a5a0cf436cbd76c2f518460e03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6db786e7157743008b1d1024d71ed2cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6dfb4e44f64c4a66abd4aed045dfafed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70cb2596438447a3a43a8d7e398fb642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71a401e0b3284652a96fe0e36f2b97b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72b5874ea4554f4ebf690421200b3856": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72d00cf9bbfd43a3b22a61c21fb6b2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0005be084eb4c3caca712d0aa0087c9",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4cae87753e541e99efabb75d0745d16",
      "value": 190
     }
    },
    "75f13359b6744f3b92f70b1900de7e56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77ca0fca1bd74cefa86987954c9a872f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "789771b912f24db69b119c58f4399c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_edb0030de05d46d685cd81f98b9510c6",
       "IPY_MODEL_c5e60fd1188043008de959752a863636",
       "IPY_MODEL_cbe6891eb4014e7f853ec76d4e56f1f8"
      ],
      "layout": "IPY_MODEL_4a69524be52449129364390309db45ff"
     }
    },
    "7a364ff5a7b64a32a0d69ba5ca3131be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faf96d8953634c9db379b46521d2397c",
      "placeholder": "​",
      "style": "IPY_MODEL_6d09e8a5a0cf436cbd76c2f518460e03",
      "value": " 349/349 [00:00&lt;00:00, 21.1kB/s]"
     }
    },
    "7c0e8b8413474c1a9b9bae2a0a011d84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ddbc4308f2d4122b79eaa5a67c21b2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "806ba54b23944a6ea39c94ad33a409f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d1f505075cb14afabff64cf79fabb5b7",
       "IPY_MODEL_0d2aa8937ed24c3b82c6d316ab4076e4",
       "IPY_MODEL_c4c61a6123854429bfc27f3baf41eb11"
      ],
      "layout": "IPY_MODEL_c7fd5905b58048e99fdc4a8a00d0cc8c"
     }
    },
    "8243c17d3da141ceb37a0d1006eed8e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82af6ec6f7324f02bfeccc4ecab72b9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "831fe169202041e2b7a245c62f6d1e94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8431f4111a9240758cdbda1c830576fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8447a5acf01045d78c8d512e0c7d1494": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "875bc0f0b47c4d5fa3628bba5cddfd1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87b66ea157fb425fa864cf6749b2a404": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d017509927b40c0b8a9c6bf1e104306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a16d05cfdd34f6a83511335e14a97a8",
       "IPY_MODEL_a60d771ede134585962cf170983e86c4",
       "IPY_MODEL_a0ade0b19d494f7fb8f97dd52db8290c"
      ],
      "layout": "IPY_MODEL_d75cf0296b7b4e72b5044e0319fc5d52"
     }
    },
    "8e2840d415724a4aa30f1f3b149dbb0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8f559778b18346adb3a259c734cdd724": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90189e21f58e403ca32747603df1efdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_548ce1469b414a659f862cc352014afd",
       "IPY_MODEL_0eb13344a1724e7aaf9afda54f15b493",
       "IPY_MODEL_386003509cec4f3b88bd7399a5984b2f"
      ],
      "layout": "IPY_MODEL_56bff5bc4a544bb8b1fa58e6caea579e"
     }
    },
    "908142a3070f4066bf34483734194cfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92bc9919deb84127b7d975e60c175520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92cb3b8ff3bc4de2ada06246ceb3c1a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "949c1facc1264cba9ca17f07747a3eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95919e20022d4f41816771418dc18a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "972248ce87f14339adc6239d99940e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_451c0e6ca80840598800340df57e036a",
      "placeholder": "​",
      "style": "IPY_MODEL_831fe169202041e2b7a245c62f6d1e94",
      "value": " 112/112 [00:00&lt;00:00, 6.34kB/s]"
     }
    },
    "9845dfbf20be4cbab436e0847eb8d98f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b753e8d5d2a94ea9be46309729b96291",
      "placeholder": "​",
      "style": "IPY_MODEL_129b98c69e934d4191957c0623e660fc",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 262MB/s]"
     }
    },
    "989ec473f53041eeb36c0af9f7a42413": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99b60ebc5adf42489183ea64e5cf2707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a35e140b5674849bfc1e6244f1a4a5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a7b4718f2aa46e295d59a25e645d8ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c077b4f279ff488996e6b583691fdcdf",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_366968eb63484a20beb5f1ff865b03a3",
      "value": 231508
     }
    },
    "9b6554a15f3145b1b4d13e9d3dc8385b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c433ef967a3406daa193ce904e9d222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27236a64fb9b42c09d24f2ce350227f4",
      "max": 10610,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5336152e581b41e5b98b18d723009ca3",
      "value": 10610
     }
    },
    "9cb08e56b4ca48758adc09030c871c94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f6e054cb20743fb86caa48b953ea343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcfc841b92ac4d969e7da44f7b5baaaa",
       "IPY_MODEL_ad572bdf184a44a6b59257f2e76f1269",
       "IPY_MODEL_7a364ff5a7b64a32a0d69ba5ca3131be"
      ],
      "layout": "IPY_MODEL_ff370c53ac7246839419a26b0e748c9d"
     }
    },
    "a0005be084eb4c3caca712d0aa0087c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ade0b19d494f7fb8f97dd52db8290c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac0c050322354e1fb0be1bcb69129c89",
      "placeholder": "​",
      "style": "IPY_MODEL_70cb2596438447a3a43a8d7e398fb642",
      "value": " 466k/466k [00:00&lt;00:00, 1.08MB/s]"
     }
    },
    "a32b0b6b3c834b068d769dab2558572f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a60d771ede134585962cf170983e86c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c54efdd2a7864e128552552c299cbe79",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dfb4e44f64c4a66abd4aed045dfafed",
      "value": 466247
     }
    },
    "a7ceb44d4d254fba835e304c1b540268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69f29e15d31e441aa01c49d6890b4947",
      "placeholder": "​",
      "style": "IPY_MODEL_c5e97a52f5554d47a293f5d01dbe8eeb",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "a8829d3d46fd4508b55b5227c3efcb4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9b28bb3b91f460492f7a2a6c78b2f39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac0c050322354e1fb0be1bcb69129c89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ace8d1d93404476783871a2873b46b37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad572bdf184a44a6b59257f2e76f1269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c170a8414ab94b81be0463276372fd26",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77ca0fca1bd74cefa86987954c9a872f",
      "value": 349
     }
    },
    "b05ca66fce674dca820eede95d3711c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5e61177652e4e63b59188bd336df81a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d3401af5fec4a97aa305c49a0e21ec8",
      "placeholder": "​",
      "style": "IPY_MODEL_7ddbc4308f2d4122b79eaa5a67c21b2c",
      "value": "Downloading (…)nce_bert_config.json: 100%"
     }
    },
    "b753e8d5d2a94ea9be46309729b96291": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b821135bfeb64fc290ac622b73a2c81d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e256842b3677478b91db3797dfb6ec59",
      "placeholder": "​",
      "style": "IPY_MODEL_f19c818af90b4d5ab4c01304b934e20c",
      "value": "Batches: 100%"
     }
    },
    "b8c7a57f106b470296b69d4238722600": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd8c833513694f8fa66960a6ff3746c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c02c05c036c42d3bdfd803816295b44",
       "IPY_MODEL_ffc76606820e4fba8d11c50694737a57",
       "IPY_MODEL_e440565f96d840629dde503854727bed"
      ],
      "layout": "IPY_MODEL_63c20dbc9081488ba16996a9717ddc2b"
     }
    },
    "bfc71c7b86834b29a57c03b8fca989ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c077b4f279ff488996e6b583691fdcdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c085ff4c203c474ba5e324d0e2f06e42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c170a8414ab94b81be0463276372fd26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1fe5c0ecaa14980a194646f6f66cd0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4c61a6123854429bfc27f3baf41eb11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1fe5c0ecaa14980a194646f6f66cd0b",
      "placeholder": "​",
      "style": "IPY_MODEL_28c1151d8ffe467ea40d8f9c9e0cd291",
      "value": " 116/116 [00:00&lt;00:00, 4.61kB/s]"
     }
    },
    "c4cae87753e541e99efabb75d0745d16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c53cafcacad84eaa836e52e817d2fdd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c54efdd2a7864e128552552c299cbe79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5e60fd1188043008de959752a863636": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e862b242aa9a4ee7be6eeb6325f11d6d",
      "max": 13156,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_95919e20022d4f41816771418dc18a54",
      "value": 13156
     }
    },
    "c5e97a52f5554d47a293f5d01dbe8eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7fd5905b58048e99fdc4a8a00d0cc8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb50c9545e2f43c4af106b649e475f79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5299dedb8eff4202ae51211b428fd6b8",
       "IPY_MODEL_72d00cf9bbfd43a3b22a61c21fb6b2dc",
       "IPY_MODEL_cf1dc50fb1774808b8691091d90069a1"
      ],
      "layout": "IPY_MODEL_8447a5acf01045d78c8d512e0c7d1494"
     }
    },
    "cbe6891eb4014e7f853ec76d4e56f1f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_989ec473f53041eeb36c0af9f7a42413",
      "placeholder": "​",
      "style": "IPY_MODEL_d7bbe9f004794122adcacf4d5dc51698",
      "value": " 13.2k/13.2k [00:00&lt;00:00, 870kB/s]"
     }
    },
    "cf088c2fc6474d30b706992e7e1fcf42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1dc50fb1774808b8691091d90069a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f67efbd2c474dc0aa7383dfb65ceee6",
      "placeholder": "​",
      "style": "IPY_MODEL_ffa49835e86e42248c93da624238651c",
      "value": " 190/190 [00:00&lt;00:00, 10.4kB/s]"
     }
    },
    "cf4705662bad4992b3ec40d026df8427": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee15028796094fbe82ac590c75ce1152",
       "IPY_MODEL_4cca8f40500d4698b709f225b0e8ac21",
       "IPY_MODEL_972248ce87f14339adc6239d99940e76"
      ],
      "layout": "IPY_MODEL_87b66ea157fb425fa864cf6749b2a404"
     }
    },
    "cfec6212f00e43f9b0e69c00f18881f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_908142a3070f4066bf34483734194cfa",
      "placeholder": "​",
      "style": "IPY_MODEL_7c0e8b8413474c1a9b9bae2a0a011d84",
      "value": " 232k/232k [00:00&lt;00:00, 549kB/s]"
     }
    },
    "d1f505075cb14afabff64cf79fabb5b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5513af69ee814214ba0bc67f55ef5296",
      "placeholder": "​",
      "style": "IPY_MODEL_f032af5a5cc24cd090c645369dc0c49c",
      "value": "Downloading (…)ce_transformers.json: 100%"
     }
    },
    "d75cf0296b7b4e72b5044e0319fc5d52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7bbe9f004794122adcacf4d5dc51698": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d90fe4943e3a468a8f33e5ab9d0a1e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7f27b06fd274ccaa45292ae892d6aff",
      "placeholder": "​",
      "style": "IPY_MODEL_204c00dd2b634413821a77432a532c76",
      "value": "Downloading (…)7e55de9125/vocab.txt: 100%"
     }
    },
    "dcfc841b92ac4d969e7da44f7b5baaaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f095354c57854bef86cf37752cb21a0b",
      "placeholder": "​",
      "style": "IPY_MODEL_006cbd458d9e4d2da68ff964cb424e00",
      "value": "Downloading (…)5de9125/modules.json: 100%"
     }
    },
    "ddb5bd908e1d46d187c23e3cacfae75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df8c3d8a3fc44d7e9ce7a6a86f625aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e256842b3677478b91db3797dfb6ec59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e440565f96d840629dde503854727bed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c53cafcacad84eaa836e52e817d2fdd8",
      "placeholder": "​",
      "style": "IPY_MODEL_4770667b6b2d4ee097c0693407fa77a6",
      "value": " 612/612 [00:00&lt;00:00, 40.5kB/s]"
     }
    },
    "e751597fa4904d4fa680581adb4f47dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e862b242aa9a4ee7be6eeb6325f11d6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ead3fe6696c54179bd74ea65f382fb41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec4242dd4fd54be1b49ff6ced32df6b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed32d0d192404ba699e752f454549a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6930e5b9888d4d79819bb6477ff2a8b4",
       "IPY_MODEL_01e3d0f8640d49e3a9de2bb1824440e3",
       "IPY_MODEL_365e6d10b1ca4841a45664163cf76da9"
      ],
      "layout": "IPY_MODEL_9a35e140b5674849bfc1e6244f1a4a5e"
     }
    },
    "edb0030de05d46d685cd81f98b9510c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49a8a040041244768d803bf98d98c666",
      "placeholder": "​",
      "style": "IPY_MODEL_b8c7a57f106b470296b69d4238722600",
      "value": "Downloading (…)9125/train_script.py: 100%"
     }
    },
    "ee15028796094fbe82ac590c75ce1152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c2bcaff060d408cb8c0138dab6afb87",
      "placeholder": "​",
      "style": "IPY_MODEL_189380c32b9e48159aad4ac1880604b8",
      "value": "Downloading (…)cial_tokens_map.json: 100%"
     }
    },
    "f032af5a5cc24cd090c645369dc0c49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f095354c57854bef86cf37752cb21a0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1408b78587f4fd4b6c043e118f293e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f19c818af90b4d5ab4c01304b934e20c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4f65ca7dbb44e53abf4a8969eeaf5d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7ceb44d4d254fba835e304c1b540268",
       "IPY_MODEL_073ac27236834df2b1cf9c216abc79e5",
       "IPY_MODEL_9845dfbf20be4cbab436e0847eb8d98f"
      ],
      "layout": "IPY_MODEL_29c8e87ba7954e8e992650be6e5c7546"
     }
    },
    "f659bd8028ab45e9b223b39668ad1d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7f27b06fd274ccaa45292ae892d6aff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f84b23cb30aa4006b0cc8ef21d15878f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13602bc74b2c4844b9796f110d91e23e",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b53fdf486b643ee9836adfb1e4890ed",
      "value": 53
     }
    },
    "faf96d8953634c9db379b46521d2397c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff370c53ac7246839419a26b0e748c9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffa49835e86e42248c93da624238651c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ffc76606820e4fba8d11c50694737a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b6554a15f3145b1b4d13e9d3dc8385b",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_18f3008fcfa24cc6a8dff5d85b6b6d85",
      "value": 612
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
